name: Daily Data Refresh

on:
  schedule:
    # Run at 05:00 UTC daily (before 6am briefings)
    - cron: '0 5 * * *'
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: write

jobs:
  refresh-data:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Install root dependencies
        run: npm install
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # CRITICAL DATA (used directly by pages)
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - name: Collect GitHub releases
        env:
          GITHUB_TOKEN: ${{ secrets.GH_PAT }}
        run: node scrapers/github-releases.js
        continue-on-error: true
      
      - name: Collect LMArena leaderboard
        run: node scrapers/lmarena-leaderboard.js
        continue-on-error: true
      
      - name: Collect Aider benchmark
        run: node scrapers/aider-benchmark.js
        continue-on-error: true
      
      - name: Collect HN AI mentions
        run: node scrapers/hn-ai-mentions.js
        continue-on-error: true
      
      - name: Collect SWE-Bench
        run: node scrapers/swe-bench.js
        continue-on-error: true
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # NEWS & FEEDS
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - name: Collect AI RSS feeds
        run: node scrapers/ai-rss-feeds.js
        continue-on-error: true
      
      - name: Collect Reddit AI
        run: node scrapers/reddit-ai.js
        continue-on-error: true
      
      - name: Collect Bluesky AI
        run: node scrapers/bluesky-ai.js
        continue-on-error: true
      
      - name: Collect Dev.to AI
        run: node scrapers/devto-ai.js
        continue-on-error: true
      
      - name: Collect TechCrunch AI
        run: node scrapers/techcrunch-ai.js
        continue-on-error: true
      
      - name: Collect ProductHunt AI
        run: node scrapers/producthunt-ai.js
        continue-on-error: true
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # COMPANY NEWS
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - name: Collect OpenAI news
        run: node scrapers/openai-news.js
        continue-on-error: true
      
      - name: Collect Anthropic news
        run: node scrapers/anthropic-news.js
        continue-on-error: true
      
      - name: Collect Google AI blog
        run: node scrapers/google-ai-blog.js
        continue-on-error: true
      
      - name: Collect DeepMind news
        run: node scrapers/deepmind-news.js
        continue-on-error: true
      
      - name: Collect Mistral news
        run: node scrapers/mistral-news.js
        continue-on-error: true
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # RESEARCH & MODELS
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - name: Collect ArXiv AI
        run: node scrapers/arxiv-ai.js
        continue-on-error: true
      
      - name: Collect HuggingFace trending
        run: node scrapers/huggingface-trending.js
        continue-on-error: true
      
      - name: Collect Model releases
        run: node scrapers/model-releases.js
        continue-on-error: true
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # STATS & METRICS
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - name: Collect VS Code stats
        run: node scripts/collect-vscode-stats.js
        continue-on-error: true
      
      - name: Collect GitHub trending
        env:
          GITHUB_TOKEN: ${{ secrets.GH_PAT }}
        run: node scripts/collect-github-trending.js
        continue-on-error: true
      
      - name: Collect Pricing
        run: node scrapers/pricing-monitor.js
        continue-on-error: true
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # GENERATORS
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - name: Aggregate news
        run: node scripts/aggregate-news.js
        continue-on-error: true
      
      - name: Generate insights
        run: node scripts/generate-insights.js
        continue-on-error: true
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # UPDATE META & COPY TO APP
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - name: Update meta timestamp
        run: |
          echo "{\"lastRefresh\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\", \"toolsTracked\": 15, \"dataVersion\": 2}" > data/meta.json
      
      - name: Copy data to app content directory
        run: |
          mkdir -p app/content/data
          mkdir -p app/content/briefings
          mkdir -p app/content/briefings/html
          mkdir -p app/content/briefings/data
          
          # Copy main data files
          cp data/*.json app/content/data/ 2>/dev/null || true
          cp site/data/*.json app/content/data/ 2>/dev/null || true
          
          # Copy briefing files
          cp briefing/digests/briefing-*.md app/content/briefings/ 2>/dev/null || true
          cp briefing/emails/briefing-*.html app/content/briefings/html/ 2>/dev/null || true
          cp briefing/data/*.json app/content/briefings/data/ 2>/dev/null || true
          
          echo "âœ“ Copied data to app/content"
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # COMMIT & PUSH
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - name: Check for changes
        id: changes
        run: |
          git add -A
          if git diff --cached --quiet; then
            echo "changed=false" >> $GITHUB_OUTPUT
          else
            echo "changed=true" >> $GITHUB_OUTPUT
          fi
      
      - name: Commit and push
        if: steps.changes.outputs.changed == 'true'
        run: |
          git config user.name "Kell"
          git config user.email "hi@kell.cx"
          git commit -m "ðŸ“Š Daily data refresh $(date -u +%Y-%m-%d)"
          git push origin main
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # BUILD & DEPLOY
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - name: Install app dependencies
        run: |
          cd app
          npm ci
      
      - name: Build Next.js app
        run: |
          cd app
          npm run build
      
      - name: Deploy to Vercel
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
        run: |
          npm i -g vercel
          cd app
          vercel link --yes --token=$VERCEL_TOKEN 2>/dev/null || true
          vercel deploy --prod --yes --token=$VERCEL_TOKEN
