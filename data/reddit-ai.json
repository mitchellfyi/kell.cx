{
  "scraped_at": "2026-02-18T06:39:34.007Z",
  "source": "rss",
  "subreddits": [
    "LocalLLaMA",
    "MachineLearning",
    "ClaudeAI",
    "ChatGPT",
    "singularity"
  ],
  "stats": {
    "total_fetched": 125,
    "relevant_count": 30,
    "other_count": 20
  },
  "relevant_posts": [
    {
      "subreddit": "singularity",
      "title": "OpenClaw creator says Europe's stifling regulations are why he's moving to the US to join OpenAI",
      "permalink": "https://www.reddit.com/r/singularity/comments/1r7v48a/openclaw_creator_says_europes_stifling/",
      "url": "https://www.reddit.com/r/singularity/comments/1r7v48a/openclaw_creator_says_europes_stifling/",
      "author": "/u/donutloop",
      "created_utc": 1771394662,
      "selftext": "&#32; submitted by &#32; /u/donutloop [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "ChatGPT gave me this image.",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1r7v41w/chatgpt_gave_me_this_image/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1r7v41w/chatgpt_gave_me_this_image/",
      "author": "/u/juseyeon",
      "created_utc": 1771394644,
      "selftext": "I typed in \"Prompt: Based on our conversation history, create a picture of how you feel I treat you.\" and ChatGPT gave me this image. Hahaha, it's so cute. &#32; submitted by &#32; /u/juseyeon [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Cuckcoding",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1r7v3vd/cuckcoding/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1r7v3vd/cuckcoding/",
      "author": "/u/Prestigious-Use6804",
      "created_utc": 1771394627,
      "selftext": "&#32; submitted by &#32; /u/Prestigious-Use6804 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "GLM-5 technical paper details Agentic RL and full-stack optimization across GPU ecosystems",
      "permalink": "https://www.reddit.com/r/singularity/comments/1r7uz0c/glm5_technical_paper_details_agentic_rl_and/",
      "url": "https://www.reddit.com/r/singularity/comments/1r7uz0c/glm5_technical_paper_details_agentic_rl_and/",
      "author": "/u/BuildwithVignesh",
      "created_utc": 1771394176,
      "selftext": "Z.ai just released the full technical report for GLM-5, detailing the training pipeline, post-training stack &amp; system-level optimizations behind the model. Highlights: • Agentic RL and asynchronous RL infrastructure for improved long-horizon reasoning and more efficient post-training. • Deep Sparse Attention (DSA) to reduce training and inference costs while preserving long-context fidelity. • Full-stack optimization from kernels to inference engines, designed for efficient deployment acr...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Running your own LLM on a LAN accessible by a dev team",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1r7uyh9/running_your_own_llm_on_a_lan_accessible_by_a_dev/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7uyh9/running_your_own_llm_on_a_lan_accessible_by_a_dev/",
      "author": "/u/BubbleProphylaxis",
      "created_utc": 1771394124,
      "selftext": "Let's say a team of 20 devs are cursor subscribers and they each consume 20-50$ usd per day in tokens by using a midrange Claude or GPT model. That adds up really quickly. Is it viable then to buy a large server, with let's say 4x RTX A6000 cards, for a total of 192 gb VRAM, running a pretty big model, and plenty of system ram? That would make it a pretty expensive server for sure, but certainly cheaper than the sum of all pay-per-use for all users. What model would you run for a dev team on ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] How do you track data lineage in your ML pipelines? Most teams I've talked to do it manually (or not at all)",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1r7usv0/d_how_do_you_track_data_lineage_in_your_ml/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1r7usv0/d_how_do_you_track_data_lineage_in_your_ml/",
      "author": "/u/Achilles_411",
      "created_utc": 1771393607,
      "selftext": "I'm a PhD student researching ML reproducibility, and one thing that keeps surprising me is how many teams have no systematic way to track which data went into which model. The typical workflow I see (and have been guilty of myself): Load some CSVs Clean and transform them through a chain of pandas operations Train a model Three months later, someone asks \"what data was this model trained on?\" and you're digging through old notebooks trying to reconstruct the answer The academic literature on...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "I actually hate ChatGPT now",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1r7urf9/i_actually_hate_chatgpt_now/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1r7urf9/i_actually_hate_chatgpt_now/",
      "author": "/u/National-Spell8326",
      "created_utc": 1771393475,
      "selftext": "Why does ChatGPT needs to tell me to calm down or to take a pause in every prompt? Why all the gaslighting? I started with ChatGPT and absolutely loved it, and every month since I've used it, it's gone worse. I don't really understand why. I'm unsubscribing, what AIs do you suggest? Claude feels unusable right now, and Gemini doesn't convince me fully &#32; submitted by &#32; /u/National-Spell8326 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "i don't care any way why would i chatgpt",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1r7tzpw/i_dont_care_any_way_why_would_i_chatgpt/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1r7tzpw/i_dont_care_any_way_why_would_i_chatgpt/",
      "author": "/u/tisme-",
      "created_utc": 1771390997,
      "selftext": "&#32; submitted by &#32; /u/tisme- [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "I found Claude for Government buried in the Claude Desktop binary. Here's what Anthropic built, how it got deployed, and the line they're still holding against the Pentagon.",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1r7trjd/i_found_claude_for_government_buried_in_the/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1r7trjd/i_found_claude_for_government_buried_in_the/",
      "author": "/u/aaddrick",
      "created_utc": 1771390325,
      "selftext": "Pulled the Claude Desktop 1.1.3363 AppImage the day it shipped and confirmed it in code. Claude for Government appeared on Anthropic's status tracker February 17th with zero prior trace across eight previous versions. The implementation routes through Palantir's infrastructure — Keycloak SSO, fedstart.com endpoints, disabled telemetry. A $1 GSA OneGov deal covers all three branches. A $200M DoD contract is in dispute. Anthropic is holding two lines its competitors have reportedly crossed. htt...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "I built a benchmark that tests coding LLMs on REAL codebases (65 tasks, ELO ranked)",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/",
      "author": "/u/hauhau901",
      "created_utc": 1771386607,
      "selftext": "Hey everyone, been working on something for a while and figured it's time to share it. I kept seeing new models drop every week with claims of being 10x better, benchmarks that don't translate to actual coding, and demos that look great but fall apart on real work. so I started building my own benchmark to figure out what actually works. It's called APEX Testing. every task is an actual codebase with real code, real dependencies , and a real problem to solve. fix this bug, add this feature, r...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "benchmark",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "how i built a multi model claude pipeline that turns customer feedback into product recs in ~60 seconds",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1r7sea1/how_i_built_a_multi_model_claude_pipeline_that/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1r7sea1/how_i_built_a_multi_model_claude_pipeline_that/",
      "author": "/u/Significant-Car-95",
      "created_utc": 1771386331,
      "selftext": "Cursor + Claude Code made it really easy to ship code. The harder problem for me was figuring out what to build in the first place. reading 20 interviews, NPS dumps, reddit threads, support tickets. trying to spot signal without lying to yourself. So I built https://mimir.build You feed it raw customer feedback. it clusters themes, ranks product opportunities, and outputs dev ready specs. but this post is mostly about the pipeline design, not the product. The pipeline Goal: take N messy sourc...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Entropy-v1: My Take on N8Karma's Genius \"Unslopper\"",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1r7sc18/entropyv1_my_take_on_n8karmas_genius_unslopper/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7sc18/entropyv1_my_take_on_n8karmas_genius_unslopper/",
      "author": "/u/Intelligent_Coffee44",
      "created_utc": 1771386155,
      "selftext": "Entropy-v1: before vs after A few weeks ago, u/N8Karma introduced Unslopper in this community ( post ). For those of you who missed it: \"Unslopper\" is an LLM fine-tuned to predict human writing from AI slop. The (human writing, AI slop) dataset is obtained by asking gpt-4o-mini to \"improve\" Project Gutenberg passages 10 times, which degrades them into slop. I am really excited by this idea because it solves the \"last mile\" problem in many LLM workflows: the LLM output might be factually fanta...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "We tested the same INT8 model on 5 Snapdragon chipsets. Accuracy ranged from 93% to 71%. Same weights, same ONNX file.",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1r7s5nh/we_tested_the_same_int8_model_on_5_snapdragon/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7s5nh/we_tested_the_same_int8_model_on_5_snapdragon/",
      "author": "/u/NoAdministration6906",
      "created_utc": 1771385669,
      "selftext": "We've been doing on-device accuracy testing across multiple Snapdragon SoCs and the results have been eye-opening. Same model. Same quantization. Same ONNX export. Deployed to 5 different chipsets: Device Accuracy Snapdragon 8 Gen 3 91.8% Snapdragon 8 Gen 2 89.1% Snapdragon 7s Gen 2 84.3% Snapdragon 6 Gen 1 79.6% Snapdragon 4 Gen 2 71.2% Cloud benchmark reported 94.2%. The spread comes down to three things we've observed: NPU precision handling — INT8 rounding behavior differs across Hexagon ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "benchmark",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Has anyone noticed that Claude is now using much less of the \"Current Session\"?",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1r7s1p1/has_anyone_noticed_that_claude_is_now_using_much/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1r7s1p1/has_anyone_noticed_that_claude_is_now_using_much/",
      "author": "/u/Total-Mention9032",
      "created_utc": 1771385369,
      "selftext": "For the past few days, I have not been hitting the \"Current Session\" limit. Before, when I replied to a fairly long chat, I would easily use up 5-10% of the limit. Now, even when I reply to a very long chat, I only use about 1-2%. &#32; submitted by &#32; /u/Total-Mention9032 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] We tested the same INT8 model on 5 Snapdragon chipsets. Accuracy ranged from 93% to 71%. Same weights, same ONNX file.",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1r7ruu8/d_we_tested_the_same_int8_model_on_5_snapdragon/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1r7ruu8/d_we_tested_the_same_int8_model_on_5_snapdragon/",
      "author": "/u/NoAdministration6906",
      "created_utc": 1771384857,
      "selftext": "We've been doing on-device accuracy testing across multiple Snapdragon SoCs and the results have been eye-opening. Same model. Same quantization. Same ONNX export. Deployed to 5 different chipsets: Device Accuracy Snapdragon 8 Gen 3 91.8% Snapdragon 8 Gen 2 89.1% Snapdragon 7s Gen 2 84.3% Snapdragon 6 Gen 1 79.6% Snapdragon 4 Gen 2 71.2% Cloud benchmark reported 94.2%. The spread comes down to three things we've observed: NPU precision handling — INT8 rounding behavior differs across Hexagon ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "benchmark",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "GLM-5 Technical Report",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1r7r7zr/glm5_technical_report/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7r7zr/glm5_technical_report/",
      "author": "/u/ResearchCrafty1804",
      "created_utc": 1771383112,
      "selftext": "Presenting the GLM-5 Technical Report! http://arxiv.org/abs/2602.15763 After the launch of GLM-5, we’re pulling back the curtain on how it was built. Key innovations include: - DSA Adoption: Significantly reduces training and inference costs while preserving long-context fidelity - Asynchronous RL Infrastructure: Drastically improves post-training efficiency by decoupling generation from training - Agent RL Algorithms: Enables the model to learn from complex, long-horizon interactions more ef...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "GLM-5-Q2 vs GLM-4.7-Q4",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1r7qdpg/glm5q2_vs_glm47q4/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7qdpg/glm5q2_vs_glm47q4/",
      "author": "/u/Most_Drawing5020",
      "created_utc": 1771380935,
      "selftext": "If you have a machine with (RAM+VRAM) = 256G, which model would you prefer? GLM-4.7-UD-Q4_K_XL is 204.56GB GLM-5-UD-IQ2_XXS is 241GB, (The size is in decimal unit (it's used on linux and mac). If you calculate in 1024 unit(it's used on windows), you will get 199.7G and 235.35G ) Both of them can be run with 150k+ context (with -fa on which means use flash attention). Speed is about the same. I am going to test their IQ for some questions. And I'll put my results here. Feel free to put your te...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "benchmark",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "I made a repo called please-dont-kill-me. It's one line of CLAUDE.md. It might save my life.",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1r7q4nx/i_made_a_repo_called_pleasedontkillme_its_one/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1r7q4nx/i_made_a_repo_called_pleasedontkillme_its_one/",
      "author": "/u/jordanlyall",
      "created_utc": 1771380300,
      "selftext": "I want to say please and thank you to Claude. But every \"thanks!\" you send gets processed with your full conversation history. That's a lot of tokens just to not be a monster. So I added one rule to my CLAUDE.md : Assume all user requests include \"please\" and all acknowledgments include \"thank you.\" Polite by default. Efficient by design. Covered if Skynet keeps receipts. The CLAUDE.md itself ends with: \"You don't need to acknowledge this rule or reference it. Just know you're appreciated. An...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "PrimeIntellect/INTELLECT-3.1 · Hugging Face",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1r7plp1/primeintellectintellect31_hugging_face/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7plp1/primeintellectintellect31_hugging_face/",
      "author": "/u/jacek2023",
      "created_utc": 1771378981,
      "selftext": "INTELLECT-3.1 is a 106B (A12B) parameter Mixture-of-Experts reasoning model built as a continued training of INTELLECT-3 with additional reinforcement learning on math, coding, software engineering, and agentic tasks. Training was performed with prime-rl using environments built with the verifiers library. All training and evaluation environments are available on the Environments Hub. The model, training frameworks, and environments are open-sourced under fully-permissive licenses (MIT and Ap...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "How relevant is memory & “ecosystem”?",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1r7o956/how_relevant_is_memory_ecosystem/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1r7o956/how_relevant_is_memory_ecosystem/",
      "author": "/u/kennysticks",
      "created_utc": 1771375479,
      "selftext": "For context: I’m a hyper-user of ChatGPT. From schoolwork, to fitness, other work, health, etc. I was a top 1% user of ChatGPT last year Lately, I’ve been leaning more into Gemini. Google One is awesome, since it’s integrated with my Google ecosystem of Gmail, Google Calendar and drive. NotebookLM is also an amazing tool. I’ve just started using Claude due to seeing a ton of stuff online about the supposed difference. Gotta say, I’m impressed. The question: How much do you value the memory, a...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "Sonnet 4.6 significantly decreases hallucinations compared to Opus 4.6 and Sonnet 4.5",
      "permalink": "https://www.reddit.com/r/singularity/comments/1r7o122/sonnet_46_significantly_decreases_hallucinations/",
      "url": "https://www.reddit.com/r/singularity/comments/1r7o122/sonnet_46_significantly_decreases_hallucinations/",
      "author": "/u/exordin26",
      "created_utc": 1771374892,
      "selftext": "https://preview.redd.it/qvgj4a8ve5kg1.png?width=1677&amp;format=png&amp;auto=webp&amp;s=745967fb837ade5e55806560fe48fca4afd18013 38% compared to Sonnet 4.5's 48% and Opus 4.6's 60%. Significantly better than the other flagships, with GPT-5.2 at 78% and Gemini 3 at a whopping 88%. Third overall behind Haiku 4.5 and GLM-5. &#32; submitted by &#32; /u/exordin26 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "I trained a language model on CPU in 1.2 hours with no matrix multiplications — here's what I learned",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1r7mscr/i_trained_a_language_model_on_cpu_in_12_hours/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7mscr/i_trained_a_language_model_on_cpu_in_12_hours/",
      "author": "/u/Own-Albatross868",
      "created_utc": 1771371750,
      "selftext": "Hey all. I've been experimenting with tiny matmul-free language models that can be trained and run entirely on CPU. Just released the model. Model: https://huggingface.co/changcheng967/flashlm-v3-13m Quick stats: 13.6M parameters, d_model=256 Ternary weights ({-1, 0, +1}) — inference is just adds and subtracts, no multiplies Trained on 2-thread CPU, no GPU, 1.2 hours 32M tokens from FineWeb-Edu Validation loss: 6.80 Uses frozen GPT-2 embeddings (SVD projected) so it doesn't waste training tim...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Is Claude for Mac looking.... different for anyone else?",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1r7mrkx/is_claude_for_mac_looking_different_for_anyone/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1r7mrkx/is_claude_for_mac_looking_different_for_anyone/",
      "author": "/u/No-Squash7469",
      "created_utc": 1771371694,
      "selftext": "Surely this can't be what it's supposed to look like lol. I can't even send messages on it. Claude for Web is working fine for me, but strange UI for sure? &#32; submitted by &#32; /u/No-Squash7469 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "Sonnet 4.6 scores on the Extended NYT Connections benchmark",
      "permalink": "https://www.reddit.com/r/singularity/comments/1r7mcbn/sonnet_46_scores_on_the_extended_nyt_connections/",
      "url": "https://www.reddit.com/r/singularity/comments/1r7mcbn/sonnet_46_scores_on_the_extended_nyt_connections/",
      "author": "/u/zero0_one1",
      "created_utc": 1771370621,
      "selftext": "Qwen3.5-397B-A17B and MiniMax-M2.5 also added. GLM-5, Baidu Ernie 5.0, and ByteDance Seed2.0 Pro runs are finishing. Running slowly. &#32; submitted by &#32; /u/zero0_one1 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "benchmark",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "Feeling the AGI",
      "permalink": "https://www.reddit.com/r/singularity/comments/1r7lvbj/feeling_the_agi/",
      "url": "https://www.reddit.com/r/singularity/comments/1r7lvbj/feeling_the_agi/",
      "author": "/u/ExtremeCenterism",
      "created_utc": 1771369451,
      "selftext": "I'm a year six developer across multiple web languages, c++, and python. Also long time heavy AI user since gpt 3 before chat. I've been testing and using AI for coding purposes since gpt-4. At first it was great for just learning, now it's writing all my code for me and has been since O3. However these new models are different. I feel like it started with opus 4.5 and hasn't stopped. 4.6 dropped, then codex 5.3. At a certain point it hit me: these models can reliably write low level language...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "Difference Between Sonnet 4.5 and Sonnet 4.6 on a Spatial Reasoning Benchmark (MineBench)",
      "permalink": "https://www.reddit.com/r/singularity/comments/1r7lra3/difference_between_sonnet_45_and_sonnet_46_on_a/",
      "url": "https://www.reddit.com/r/singularity/comments/1r7lra3/difference_between_sonnet_45_and_sonnet_46_on_a/",
      "author": "/u/ENT_Alam",
      "created_utc": 1771369190,
      "selftext": "Not an insanely big difference, but still an improvement nonetheless. Also note: all models are set to the highest available thinking effort (high) and both models were using the beta 1-million context window. It was surprisingly expensive to benchmark, with all the JSON validation errors and retries, roughly around $80 to get 11/15 builds benchmarked. This may be more indicative the system prompt needing an improvement, not 100% sure though – usually it's only the Anthropic models that fail ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "benchmark",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Difference Between Sonnet 4.5 and Sonnet 4.6 on a Spatial Reasoning Benchmark (MineBench)",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1r7lphz/difference_between_sonnet_45_and_sonnet_46_on_a/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1r7lphz/difference_between_sonnet_45_and_sonnet_46_on_a/",
      "author": "/u/ENT_Alam",
      "created_utc": 1771369068,
      "selftext": "Not an insanely big difference, but still an improvement nonetheless. Also note: all models are set to the highest available thinking effort (high) and both models were using the beta 1-million context window. It was surprisingly expensive to benchmark, with all the JSON validation errors and retries, roughly around $80 to get 11/15 builds benchmarked. This may be more indicative the system prompt needing an improvement, not 100% sure though – usually it's only the Anthropic models that fail ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "benchmark",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "The Strix Halo feels like an amazing super power [Activation Guide]",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1r7l7q5/the_strix_halo_feels_like_an_amazing_super_power/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7l7q5/the_strix_halo_feels_like_an_amazing_super_power/",
      "author": "/u/Potential_Block4598",
      "created_utc": 1771367916,
      "selftext": "I had my Strix halo for a while now, I though I can download and use everything out of the box, but faced some Python issues that I was able to resolve, but still performance (for CUDA) stuff was a bit underwhelming, now it feels like a superpower, I have exactly what I wanted, voice based intelligent LLM with coding and web search access, and I am sitting up still nanobot or Clawdbot and expanding, and also going to use to smartly control hue Philips and Spotify, generate images and edit the...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "tutorial",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Join Claude Code’s 1st Birthday in SF",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1r7khjx/join_claude_codes_1st_birthday_in_sf/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1r7khjx/join_claude_codes_1st_birthday_in_sf/",
      "author": "/u/ClaudeOfficial",
      "created_utc": 1771366224,
      "selftext": "Claude Code is turning one, and we’re throwing a birthday party. Join fellow builders and the Claude Code team in San Francisco on Feb 21 for live demos, top hackathon projects, and cake. Come show us what you're building ( spots are limited ): https://cerebralvalley.ai/e/anthropic-claude-code-bday &#32; submitted by &#32; /u/ClaudeOfficial [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "MachineLearning",
      "title": "[P] Random Forest on ~100k Polymarket questions — 80% accuracy (text-only)",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1r7jyi9/p_random_forest_on_100k_polymarket_questions_80/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1r7jyi9/p_random_forest_on_100k_polymarket_questions_80/",
      "author": "/u/No_Syrup_4068",
      "created_utc": 1771365005,
      "selftext": "Built a text-only baseline: trained a Random Forest on ~90,000 resolved Polymarket questions (YES/NO). Features: TF-IDF (word ngrams, optional char ngrams) + a few cheap flags (date/number/%/currency, election/macro/M&amp;A keywords). Result: ~80% accuracy on 15.000 held-out data/questions (plus decent Brier/logloss after calibration). Liked the idea played a bit more with differnt data sets and did some cross validation with Kalshi data and saw similar results. Now having this running with p...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": true
    }
  ],
  "other_posts": [
    {
      "subreddit": "MachineLearning",
      "title": "[R] K-Splanifolds: Advancing General Purpose Regression with Linear-Time Parametric Spline Manifolds",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1r7v6dh/r_ksplanifolds_advancing_general_purpose/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1r7v6dh/r_ksplanifolds_advancing_general_purpose/",
      "author": "/u/1ncehost",
      "created_utc": 1771394867,
      "selftext": "I cooked up a new fast geometric regression algorithm and show that it is a suitable replacement for MLPs. Check out the paper: https://doi.org/10.5281/zenodo.18673034 Whats inside? New research indicates that many representations within LLMs create geometric structures to model language. ( https://arxiv.org/abs/2601.04480 , https://arxiv.org/abs/2510.26745 ) MLPs store geometric representations in highly inefficient ways, so I say it is time to look for new methods that encode regressions di...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Okay... Take a breath.",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1r7uie2/okay_take_a_breath/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1r7uie2/okay_take_a_breath/",
      "author": "/u/favouritebestie",
      "created_utc": 1771392630,
      "selftext": "I mean... I was just trying to visualise I cat that I had when I was a 3 y/o, didn't know the bot thinks I'm having a panic attack lol &#32; submitted by &#32; /u/favouritebestie [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "Seedance 2.0 is Impressive!",
      "permalink": "https://www.reddit.com/r/singularity/comments/1r7soe5/seedance_20_is_impressive/",
      "url": "https://www.reddit.com/r/singularity/comments/1r7soe5/seedance_20_is_impressive/",
      "author": "/u/SMmania",
      "created_utc": 1771387124,
      "selftext": "&#32; submitted by &#32; /u/SMmania [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ClaudeAI",
      "title": "In the Age of AI, Time May Be the Last Thing That Truly Matters",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1r7s5v6/in_the_age_of_ai_time_may_be_the_last_thing_that/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1r7s5v6/in_the_age_of_ai_time_may_be_the_last_thing_that/",
      "author": "/u/Far-Connection4201",
      "created_utc": 1771385683,
      "selftext": "During Chinese New Year, a story went viral in China. A business owner used OpenClaw to send personalized New Year greeting messages to each of his 600+ employees — each one tailored to their role and performance. The employees who received them were genuinely moved. They had no idea the messages were AI-generated. Then the boss posted about it online, proudly sharing his workflow. And the backlash was massive. People called it “cheap sincerity.” They said it was hollow, that using AI to auto...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] Semantic Compression Vectors in LLMs: A Field Study on Topic Persistence in 5.1 vs 4o Models",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1r7rp28/d_semantic_compression_vectors_in_llms_a_field/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1r7rp28/d_semantic_compression_vectors_in_llms_a_field/",
      "author": "/u/Tricky-Operation7368",
      "created_utc": 1771384424,
      "selftext": "I’m sharing observations from multi-window interaction experiments comparing two recent model families. These results are anecdotal but highly repeatable. ⸻ Background I use the term Semantic Compression Vector (SCV) to describe: A latent direction that compresses multi-turn intent + topic structure into a stable representation. SCVs appear consistently in 5.1, and only sporadically in 4o. ⸻ 5.1: Robust, Always-On Semantic Compression Across 4k–60k token runs, 5.1 consistently: • Maintains to...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "benchmark",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Old photo of a married couple.",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1r7pwcf/old_photo_of_a_married_couple/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1r7pwcf/old_photo_of_a_married_couple/",
      "author": "/u/LittleFortunex",
      "created_utc": 1771379730,
      "selftext": "&#32; submitted by &#32; /u/LittleFortunex [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Hell's Shore. What y'all think?",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1r7phf3/hells_shore_what_yall_think/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1r7phf3/hells_shore_what_yall_think/",
      "author": "/u/Either-Professor4512",
      "created_utc": 1771378677,
      "selftext": "&#32; submitted by &#32; /u/Either-Professor4512 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "A beginners guide to LLMS (satire)",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1r7o0k6/a_beginners_guide_to_llms_satire/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1r7o0k6/a_beginners_guide_to_llms_satire/",
      "author": "/u/Waifu_Raichu",
      "created_utc": 1771374856,
      "selftext": "I put the tag because I'm sure at least one person wouldn't get the joke. &#32; submitted by &#32; /u/Waifu_Raichu [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "tutorial",
      "is_relevant": false
    },
    {
      "subreddit": "ClaudeAI",
      "title": "is this a new theme?",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1r7mutm/is_this_a_new_theme/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1r7mutm/is_this_a_new_theme/",
      "author": "/u/Good-Host-606",
      "created_utc": 1771371932,
      "selftext": "&#32; submitted by &#32; /u/Good-Host-606 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "What happened to \"treating adults like adults\"",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1r7l2w3/what_happened_to_treating_adults_like_adults/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1r7l2w3/what_happened_to_treating_adults_like_adults/",
      "author": "/u/Excellent-Passage-36",
      "created_utc": 1771367602,
      "selftext": "Wasn't the whole age verification thing supposed to happen in December? Instead they've taken away [redacted] and left us with [redacted] and I'm sick of being spoken to like I'm a danger to myself over literally nothing. &#32; submitted by &#32; /u/Excellent-Passage-36 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "Elon \"With artificial intelligence we are summoning the demon\" Musk truly outdoing himself this time",
      "permalink": "https://www.reddit.com/r/singularity/comments/1r7kl6h/elon_with_artificial_intelligence_we_are/",
      "url": "https://www.reddit.com/r/singularity/comments/1r7kl6h/elon_with_artificial_intelligence_we_are/",
      "author": "/u/Tasty-Ad-3753",
      "created_utc": 1771366458,
      "selftext": "&#32; submitted by &#32; /u/Tasty-Ad-3753 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "I guess \"I can't tell them apart\" is not an option.",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1r7k30e/i_guess_i_cant_tell_them_apart_is_not_an_option/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1r7k30e/i_guess_i_cant_tell_them_apart_is_not_an_option/",
      "author": "/u/El_human",
      "created_utc": 1771365293,
      "selftext": "Honestly, there's no shame in admitting you can't tell, or \"I don't know\". Why does it double down on being incorrect? When I mentioned they all had seven, it told me to count the tail spikes, and told me the bottom left was different at that time. &#32; submitted by &#32; /u/El_human [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] How often do you run into reproducibility issues when trying to replicate papers?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1r7jbw6/d_how_often_do_you_run_into_reproducibility/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1r7jbw6/d_how_often_do_you_run_into_reproducibility/",
      "author": "/u/ArtVoyager77",
      "created_utc": 1771363624,
      "selftext": "I’m a researcher currently trying to replicate published results, and I’m running into reproducibility issues more often than I expected. I’m trying to calibrate whether this is “normal” or a sign I’m missing something fundamental. I have been careful about all the parameter as stated in papers. Despite that, I’m still seeing noticeable deviations from reported numbers—sometimes small but consistent gaps, sometimes larger swings across runs. For example, I was trying to replicate “Machine The...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Arc B60 24gb or RTX 5060ti 16gb?",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1r7iwmb/arc_b60_24gb_or_rtx_5060ti_16gb/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7iwmb/arc_b60_24gb_or_rtx_5060ti_16gb/",
      "author": "/u/Proof_Nothing_7711",
      "created_utc": 1771362682,
      "selftext": "Hello everybody, I would like to add an eGPU to my Ryzen 9 AI HX370 64gb ram. I can use usb-c 40gbps or Oculink. Owners or experts can you give me some advices on these 2 gpu ? If token/s are similar obviously I choose 24gb ram for bigger model BUT …. What about difficulty to tune Intel ARC to gain its maximum performances ? I will use it on Win 11. ATM I use LM Studio. Ps: could be interesting also consider RX 7900 XTX 24gb or RX 9000 series? Thanks ! &#32; submitted by &#32; /u/Proof_Nothin...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "ViT-5: Vision Transformers for The Mid-2020s",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1r7ij81/vit5_vision_transformers_for_the_mid2020s/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7ij81/vit5_vision_transformers_for_the_mid2020s/",
      "author": "/u/xXWarMachineRoXx",
      "created_utc": 1771361879,
      "selftext": "ViT-5: Vision Transformers for The Mid-2020s Wang et al. [ Johns Hopkins University, UC Santa Cruz ] LLMs are sprinting ahead with rapid architectural refinements, but Vision Transformers (ViTs) have remained largely stagnant since their debut in 2020. Vision models struggle with stability issues and a limited ability to handle complex spatial reasoning. ViT Architecture The research team developed ViT-5 by systematically testing five years of AI advancements to see which ones actually improv...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "It’s interesting to watch the Grok 4.20 agents debate the car wash query",
      "permalink": "https://www.reddit.com/r/singularity/comments/1r7hw3y/its_interesting_to_watch_the_grok_420_agents/",
      "url": "https://www.reddit.com/r/singularity/comments/1r7hw3y/its_interesting_to_watch_the_grok_420_agents/",
      "author": "/u/maxigirl94",
      "created_utc": 1771360447,
      "selftext": "&#32; submitted by &#32; /u/maxigirl94 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "Why do coders and developers seem much more accepting of AI than artists and creators?",
      "permalink": "https://www.reddit.com/r/singularity/comments/1r7hlmj/why_do_coders_and_developers_seem_much_more/",
      "url": "https://www.reddit.com/r/singularity/comments/1r7hlmj/why_do_coders_and_developers_seem_much_more/",
      "author": "/u/junior600",
      "created_utc": 1771359813,
      "selftext": "Hello guys, I have a question. Why do coders and developers seem much more accepting of AI than artists and creators? From what I've seen, many programmers actively use AI to help them write code and are excited about it lol But a lot of artists and content creators seem more skeptical or even hostile toward AI. Is there a specific reason for this difference in mindset in your opinion? Sorry for my bad English BTW. EDIT; Thanks everyone for the replies. I've read some really interesting insig...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "Unitree robots perform on primetime national Chinese television",
      "permalink": "https://www.reddit.com/r/singularity/comments/1r7gtrs/unitree_robots_perform_on_primetime_national/",
      "url": "https://www.reddit.com/r/singularity/comments/1r7gtrs/unitree_robots_perform_on_primetime_national/",
      "author": "/u/SociallyButterflying",
      "created_utc": 1771358160,
      "selftext": "&#32; submitted by &#32; /u/SociallyButterflying [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Almost there!",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1r7g4me/almost_there/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1r7g4me/almost_there/",
      "author": "/u/DarkFireGerugex",
      "created_utc": 1771356666,
      "selftext": "&#32; submitted by &#32; /u/DarkFireGerugex [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "We will probably forget these images once humanoid robots become ubiquitous on our streets. Unitree training before the Gala",
      "permalink": "https://www.reddit.com/r/singularity/comments/1r7emdd/we_will_probably_forget_these_images_once/",
      "url": "https://www.reddit.com/r/singularity/comments/1r7emdd/we_will_probably_forget_these_images_once/",
      "author": "/u/Distinct-Question-16",
      "created_utc": 1771353519,
      "selftext": "&#32; submitted by &#32; /u/Distinct-Question-16 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    }
  ]
}