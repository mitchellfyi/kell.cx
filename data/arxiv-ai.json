{
  "scraped_at": "2026-02-20T05:25:09.282Z",
  "categories_searched": [
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "stats": {
    "total_fetched": 200,
    "relevant_count": 50,
    "recent_count": 20,
    "by_category": {
      "agents": 21,
      "code-generation": 9,
      "benchmarks": 19,
      "reasoning": 1
    }
  },
  "relevant_papers": [
    {
      "id": "2602.17127v1",
      "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
      "summary": "As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions. This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked",
      "authors": [
        "Dusan Bosnjakovic"
      ],
      "published": "2026-02-19T06:56:01Z",
      "updated": "2026-02-19T06:56:01Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17127v1",
      "abs_url": "https://arxiv.org/abs/2602.17127v1",
      "relevance_score": 16,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning",
        "safety",
        "alignment"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17547v1",
      "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
      "summary": "This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between ",
      "authors": [
        "Yue Liu",
        "Zhiyuan Hu",
        "Flood Sung",
        "Jiaheng Zhang",
        "Bryan Hooi"
      ],
      "published": "2026-02-19T17:01:08Z",
      "updated": "2026-02-19T17:01:08Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17547v1",
      "abs_url": "https://arxiv.org/abs/2602.17547v1",
      "relevance_score": 14,
      "matched_keywords": [
        "llm",
        "claude",
        "agent",
        "agentic",
        "benchmark",
        "evaluation"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17544v1",
      "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability",
      "summary": "In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability ",
      "authors": [
        "Shashank Aggarwal",
        "Ram Vikas Mishra",
        "Amit Awekar"
      ],
      "published": "2026-02-19T16:59:11Z",
      "updated": "2026-02-19T16:59:11Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17544v1",
      "abs_url": "https://arxiv.org/abs/2602.17544v1",
      "relevance_score": 14,
      "matched_keywords": [
        "llm",
        "llama",
        "agent",
        "benchmark",
        "evaluation",
        "reasoning",
        "cot",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17222v1",
      "title": "Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight",
      "summary": "Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding ",
      "authors": [
        "Ben Yellin",
        "Ehud Ezra",
        "Mark Foreman",
        "Shula Grinapol"
      ],
      "published": "2026-02-19T10:13:17Z",
      "updated": "2026-02-19T10:13:17Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17222v1",
      "abs_url": "https://arxiv.org/abs/2602.17222v1",
      "relevance_score": 14,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "llama",
        "fine-tuning",
        "prompt",
        "evaluation",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17045v1",
      "title": "Large Language Models Persuade Without Planning Theory of Mind",
      "summary": "A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategically revealing information. Success depends on a persuader's sensitivity to a given target's knowledge states (what the target knows about the policies) and motivational states (how much the target values different outcomes). We varied whether these states were Revealed to persuaders or Hidden, in whic",
      "authors": [
        "Jared Moore",
        "Rasmus Overmark",
        "Ned Cooper",
        "Beba Cibralic",
        "Nick Haber",
        "Cameron R. Jones"
      ],
      "published": "2026-02-19T03:31:31Z",
      "updated": "2026-02-19T03:31:31Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17045v1",
      "abs_url": "https://arxiv.org/abs/2602.17045v1",
      "relevance_score": 13,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17037v1",
      "title": "Wink: Recovering from Misbehaviors in Coding Agents",
      "summary": "Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent traject",
      "authors": [
        "Rahul Nanda",
        "Chandra Maddila",
        "Smriti Jha",
        "Euna Mehnaz Khan",
        "Matteo Paltenghi",
        "Satish Chandra"
      ],
      "published": "2026-02-19T03:15:00Z",
      "updated": "2026-02-19T03:15:00Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC",
        "cs.PL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17037v1",
      "abs_url": "https://arxiv.org/abs/2602.17037v1",
      "relevance_score": 13,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "agentic",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17003v1",
      "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History",
      "summary": "Large language models have advanced web agents, yet current agents lack personalization capabilities. Since users rarely specify every detail of their intent, practical web agents must be able to interpret ambiguous queries by inferring user preferences and contexts. To address this challenge, we present Persona2Web, the first benchmark for evaluating personalized web agents on the real open web, built upon the clarify-to-personalize principle, which requires agents to resolve ambiguity based on user history rather than relying on explicit instructions. Persona2Web consists of: (1) user histories that reveal preferences implicitly over long time spans, (2) ambiguous queries that require agents to infer implicit user preferences, and (3) a reasoning-aware evaluation framework that enables f",
      "authors": [
        "Serin Kim",
        "Sangam Lee",
        "Dongha Lee"
      ],
      "published": "2026-02-19T01:54:26Z",
      "updated": "2026-02-19T01:54:26Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17003v1",
      "abs_url": "https://arxiv.org/abs/2602.17003v1",
      "relevance_score": 13,
      "matched_keywords": [
        "large language model",
        "language model",
        "agent",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17096v1",
      "title": "Agentic Wireless Communication for 6G: Intent-Aware and Continuously Evolving Physical-Layer Intelligence",
      "summary": "As 6G wireless systems evolve, growing functional complexity and diverse service demands are driving a shift from rule-based control to intent-driven autonomous intelligence. User requirements are no longer captured by a single metric (e.g., throughput or reliability), but by multi-dimensional objectives such as latency sensitivity, energy preference, computational constraints, and service-level requirements. These objectives may also change over time due to environmental dynamics and user-network interactions. Therefore, accurate understanding of both the communication environment and user intent is critical for autonomous and sustainably evolving 6G communications. Large language models (LLMs), with strong contextual understanding and cross-modal reasoning, provide a promising foundation",
      "authors": [
        "Zhaoyang Li",
        "Xingzhi Jin",
        "Junyu Pan",
        "Qianqian Yang",
        "Zhiguo Shi"
      ],
      "published": "2026-02-19T05:36:27Z",
      "updated": "2026-02-19T05:36:27Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17096v1",
      "abs_url": "https://arxiv.org/abs/2602.17096v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "agentic",
        "reasoning",
        "reasoning",
        "multimodal"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17072v1",
      "title": "BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios",
      "summary": "Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. M",
      "authors": [
        "Yunseung Lee",
        "Subin Kim",
        "Youngjun Kwak",
        "Jaegul Choo"
      ],
      "published": "2026-02-19T04:27:47Z",
      "updated": "2026-02-19T04:27:47Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17072v1",
      "abs_url": "https://arxiv.org/abs/2602.17072v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "fine-tuning",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17022v1",
      "title": "ReIn: Conversational Error Recovery with Reasoning Inception",
      "summary": "Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial ",
      "authors": [
        "Takyoung Kim",
        "Jinseok Nam",
        "Chandrayee Basu",
        "Xing Fan",
        "Chengyuan Ma",
        "Heng Ji",
        "Gokhan Tur",
        "Dilek Hakkani-Tür"
      ],
      "published": "2026-02-19T02:37:29Z",
      "updated": "2026-02-19T02:37:29Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17022v1",
      "abs_url": "https://arxiv.org/abs/2602.17022v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "fine-tuning",
        "prompt",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17645v1",
      "title": "Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting",
      "summary": "Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA)",
      "authors": [
        "Xiaohan Zhao",
        "Zhaoyi Li",
        "Yaxin Luo",
        "Jiacheng Cui",
        "Zhiqiang Shen"
      ],
      "published": "2026-02-19T18:54:32Z",
      "updated": "2026-02-19T18:54:32Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17645v1",
      "abs_url": "https://arxiv.org/abs/2602.17645v1",
      "relevance_score": 11,
      "matched_keywords": [
        "language model",
        "gpt",
        "claude",
        "gemini",
        "rag",
        "multimodal",
        "alignment"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17443v1",
      "title": "AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue",
      "summary": "Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured \"20 Questions\" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driv",
      "authors": [
        "Adib Sakhawat",
        "Fardeen Sadab",
        "Rakin Shahriar"
      ],
      "published": "2026-02-19T15:09:12Z",
      "updated": "2026-02-19T15:09:12Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17443v1",
      "abs_url": "https://arxiv.org/abs/2602.17443v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17345v1",
      "title": "What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?",
      "summary": "Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from ",
      "authors": [
        "Boyang Ma",
        "Hechuan Guo",
        "Peizhuo Lv",
        "Minghui Xu",
        "Xuelong Dai",
        "YeChao Zhang",
        "Yijun Yang",
        "Yue Zhang"
      ],
      "published": "2026-02-19T13:29:00Z",
      "updated": "2026-02-19T13:29:00Z",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17345v1",
      "abs_url": "https://arxiv.org/abs/2602.17345v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "reasoning",
        "reasoning",
        "safety"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17308v1",
      "title": "MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions",
      "summary": "Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning",
      "authors": [
        "Hui Min Wong",
        "Philip Heesen",
        "Pascal Janetzky",
        "Martin Bendszus",
        "Stefan Feuerriegel"
      ],
      "published": "2026-02-19T12:19:12Z",
      "updated": "2026-02-19T12:19:12Z",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17308v1",
      "abs_url": "https://arxiv.org/abs/2602.17308v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "agentic",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17221v1",
      "title": "From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences",
      "summary": "Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a \"methodological experiment,\" this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology. This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI divis",
      "authors": [
        "Yi-Chih Huang"
      ],
      "published": "2026-02-19T10:12:08Z",
      "updated": "2026-02-19T10:12:08Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17221v1",
      "abs_url": "https://arxiv.org/abs/2602.17221v1",
      "relevance_score": 11,
      "matched_keywords": [
        "claude",
        "agent",
        "agentic",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17183v1",
      "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
      "summary": "Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavio",
      "authors": [
        "Kishan Maharaj",
        "Nandakishore Menon",
        "Ashita Saxena",
        "Srikanth Tamilselvam"
      ],
      "published": "2026-02-19T09:05:03Z",
      "updated": "2026-02-19T09:05:03Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17183v1",
      "abs_url": "https://arxiv.org/abs/2602.17183v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17623v1",
      "title": "Unmasking the Factual-Conceptual Gap in Persian Language Models",
      "summary": "While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to d",
      "authors": [
        "Alireza Sakhaeirad",
        "Ali Ma'manpoosh",
        "Arshia Hemmat"
      ],
      "published": "2026-02-19T18:42:46Z",
      "updated": "2026-02-19T18:42:46Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17623v1",
      "abs_url": "https://arxiv.org/abs/2602.17623v1",
      "relevance_score": 10,
      "matched_keywords": [
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17616v1",
      "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
      "summary": "Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\\textbf{V}$ariance $\\te",
      "authors": [
        "Luke Huang",
        "Zhuoyang Zhang",
        "Qinghao Hu",
        "Shang Yang",
        "Song Han"
      ],
      "published": "2026-02-19T18:40:51Z",
      "updated": "2026-02-19T18:40:51Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17616v1",
      "abs_url": "https://arxiv.org/abs/2602.17616v1",
      "relevance_score": 10,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17475v1",
      "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian",
      "summary": "Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether \"small\" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tun",
      "authors": [
        "Pietro Ferrazzi",
        "Mattia Franzin",
        "Alberto Lavelli",
        "Bernardo Magnini"
      ],
      "published": "2026-02-19T15:38:46Z",
      "updated": "2026-02-19T15:38:46Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17475v1",
      "abs_url": "https://arxiv.org/abs/2602.17475v1",
      "relevance_score": 10,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "llama",
        "fine-tuning",
        "prompt",
        "few-shot",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.16958v1",
      "title": "Automating Agent Hijacking via Structural Template Injection",
      "summary": "Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce rol",
      "authors": [
        "Xinhao Deng",
        "Jiaqing Wu",
        "Miao Chen",
        "Yue Xiao",
        "Ke Xu",
        "Qi Li"
      ],
      "published": "2026-02-18T23:52:14Z",
      "updated": "2026-02-18T23:52:14Z",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.16958v1",
      "abs_url": "https://arxiv.org/abs/2602.16958v1",
      "relevance_score": 10,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "gemini",
        "agent",
        "agentic",
        "prompt"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17641v1",
      "title": "FAMOSE: A ReAct Approach to Automated Feature Discovery",
      "summary": "Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on ",
      "authors": [
        "Keith Burghardt",
        "Jienan Liu",
        "Sadman Sakib",
        "Yuning Hao",
        "Bo Li"
      ],
      "published": "2026-02-19T18:53:15Z",
      "updated": "2026-02-19T18:53:15Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17641v1",
      "abs_url": "https://arxiv.org/abs/2602.17641v1",
      "relevance_score": 9,
      "matched_keywords": [
        "llm",
        "agent",
        "agentic",
        "prompt",
        "few-shot",
        "evaluation",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17497v1",
      "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
      "summary": "Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large language models (LLMs) to transform sparse rewards into dense training signals (i.e., the advantage function) through retrospective in-context learning (RICL). We further propose an online learning framework, RICOL, which iteratively refines the policy based on the credit assignment results from RICL. We e",
      "authors": [
        "Wen-Tse Chen",
        "Jiayu Chen",
        "Fahim Tajwar",
        "Hao Zhu",
        "Xintong Duan",
        "Ruslan Salakhutdinov",
        "Jeff Schneider"
      ],
      "published": "2026-02-19T16:13:28Z",
      "updated": "2026-02-19T16:13:28Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17497v1",
      "abs_url": "https://arxiv.org/abs/2602.17497v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "in-context learning",
        "evaluation",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17483v1",
      "title": "What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data",
      "summary": "Large language models (LLMs), and conversational agents based on them, are exposed to personal data (PD) during pre-training and during user interactions. Prior work shows that PD can resurface, yet users lack insight into how strongly models associate specific information to their identity. We audit PD across eight LLMs (3 open-source; 5 API-based, including GPT-4o), introduce LMP2 (Language Model Privacy Probe), a human-centered, privacy-preserving audit tool refined through two formative studies (N=20), and run two studies with EU residents to capture (i) intuitions about LLM-generated PD (N1=155) and (ii) reactions to tool output (N2=303). We show empirically that models confidently generate multiple PD categories for well-known individuals. For everyday users, GPT-4o generates 11 feat",
      "authors": [
        "Dimitri Staufer",
        "Kirsten Morehouse"
      ],
      "published": "2026-02-19T15:53:29Z",
      "updated": "2026-02-19T15:53:29Z",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17483v1",
      "abs_url": "https://arxiv.org/abs/2602.17483v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "agent"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17452v1",
      "title": "Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge",
      "summary": "We present Jolt Atlas, a zero-knowledge machine learning (zkML) framework that extends the Jolt proving system to model inference. Unlike zkVMs (zero-knowledge virtual machines), which emulate CPU instruction execution, Jolt Atlas adapts Jolt's lookup-centric approach and applies it directly to ONNX tensor operations. The ONNX computational model eliminates the need for CPU registers and simplifies memory consistency verification. In addition, ONNX is an open-source, portable format, which makes it easy to share and deploy models across different frameworks, hardware platforms, and runtime environments without requiring framework-specific conversions. Our lookup arguments, which use sumcheck protocol, are well-suited for non-linear functions -- key building blocks in modern ML. We apply op",
      "authors": [
        "Wyatt Benno",
        "Alberto Centelles",
        "Antoine Douchet",
        "Khalil Gibran"
      ],
      "published": "2026-02-19T15:17:18Z",
      "updated": "2026-02-19T15:17:18Z",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17452v1",
      "abs_url": "https://arxiv.org/abs/2602.17452v1",
      "relevance_score": 9,
      "matched_keywords": [
        "language model",
        "agent",
        "agentic",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17185v1",
      "title": "The Bots of Persuasion: Examining How Conversational Agents' Linguistic Expressions of Personality Affect User Perceptions and Decisions",
      "summary": "Large Language Model-powered conversational agents (CAs) are increasingly capable of projecting sophisticated personalities through language, but how these projections affect users is unclear. We thus examine how CA personalities expressed linguistically affect user decisions and perceptions in the context of charitable giving. In a crowdsourced study, 360 participants interacted with one of eight CAs, each projecting a personality composed of three linguistic aspects: attitude (optimistic/pessimistic), authority (authoritative/submissive), and reasoning (emotional/rational). While the CA's composite personality did not affect participants' decisions, it did affect their perceptions and emotional responses. Particularly, participants interacting with pessimistic CAs felt lower emotional st",
      "authors": [
        "Uğur Genç",
        "Heng Gu",
        "Chadha Degachi",
        "Evangelos Niforatos",
        "Senthil Chandrasegaran",
        "Himanshu Verma"
      ],
      "published": "2026-02-19T09:10:41Z",
      "updated": "2026-02-19T09:10:41Z",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17185v1",
      "abs_url": "https://arxiv.org/abs/2602.17185v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "language model",
        "agent",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17092v1",
      "title": "A Locality Radius Framework for Understanding Relational Inductive Bias in Database Learning",
      "summary": "Foreign key discovery and related schema-level prediction tasks are often modeled using graph neural networks (GNNs), implicitly assuming that relational inductive bias improves performance. However, it remains unclear when multi-hop structural reasoning is actually necessary. In this work, we introduce locality radius, a formal measure of the minimum structural neighborhood required to determine a prediction in relational schemas. We hypothesize that model performance depends critically on alignment between task locality radius and architectural aggregation depth. We conduct a controlled empirical study across foreign key prediction, join cost estimation, blast radius regression, cascade impact classification, and additional graph-derived schema tasks. Our evaluation includes multi-seed e",
      "authors": [
        "Aadi Joshi",
        "Kavya Bhand"
      ],
      "published": "2026-02-19T05:31:03Z",
      "updated": "2026-02-19T05:31:03Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17092v1",
      "abs_url": "https://arxiv.org/abs/2602.17092v1",
      "relevance_score": 9,
      "matched_keywords": [
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning",
        "alignment"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17054v1",
      "title": "ALPS: A Diagnostic Challenge Set for Arabic Linguistic &amp; Pragmatic Reasoning",
      "summary": "While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic &amp; Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single",
      "authors": [
        "Hussein S. Al-Olimat",
        "Ahmad Alshareef"
      ],
      "published": "2026-02-19T03:51:37Z",
      "updated": "2026-02-19T03:51:37Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17054v1",
      "abs_url": "https://arxiv.org/abs/2602.17054v1",
      "relevance_score": 9,
      "matched_keywords": [
        "gemini",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17608v1",
      "title": "Towards Anytime-Valid Statistical Watermarking",
      "summary": "The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detect",
      "authors": [
        "Baihe Huang",
        "Eric Xu",
        "Kannan Ramchandran",
        "Jiantao Jiao",
        "Michael I. Jordan"
      ],
      "published": "2026-02-19T18:32:26Z",
      "updated": "2026-02-19T18:32:26Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17608v1",
      "abs_url": "https://arxiv.org/abs/2602.17608v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "evaluation",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17560v1",
      "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
      "summary": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \\textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a ste",
      "authors": [
        "Hongjue Zhao",
        "Haosen Sun",
        "Jiangtao Kong",
        "Xiaochang Li",
        "Qineng Wang",
        "Liwei Jiang",
        "Qi Zhu",
        "Tarek Abdelzaher",
        "Yejin Choi",
        "Manling Li",
        "Huajie Shao"
      ],
      "published": "2026-02-19T17:13:44Z",
      "updated": "2026-02-19T17:13:44Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17560v1",
      "abs_url": "https://arxiv.org/abs/2602.17560v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "prompt",
        "benchmark",
        "alignment"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17550v1",
      "title": "MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning",
      "summary": "Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. M",
      "authors": [
        "Xiaoliang Fu",
        "Jiaye Lin",
        "Yangyi Fang",
        "Binbin Zheng",
        "Chaowen Hu",
        "Zekai Shao",
        "Cong Qin",
        "Lu Pan",
        "Ke Zeng",
        "Xunliang Cai"
      ],
      "published": "2026-02-19T17:05:20Z",
      "updated": "2026-02-19T17:05:20Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17550v1",
      "abs_url": "https://arxiv.org/abs/2602.17550v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17529v1",
      "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation",
      "summary": "Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to groun",
      "authors": [
        "Dun Yuan",
        "Hao Zhou",
        "Xue Liu",
        "Hao Chen",
        "Yan Xin",
        "Jianzhong",
        "Zhang"
      ],
      "published": "2026-02-19T16:40:17Z",
      "updated": "2026-02-19T16:40:17Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17529v1",
      "abs_url": "https://arxiv.org/abs/2602.17529v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "rag",
        "hallucination"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17316v1",
      "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation",
      "summary": "The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial",
      "authors": [
        "Bogdan Kostić",
        "Conor Fallon",
        "Julian Risch",
        "Alexander Löser"
      ],
      "published": "2026-02-19T12:24:42Z",
      "updated": "2026-02-19T12:24:42Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17316v1",
      "abs_url": "https://arxiv.org/abs/2602.17316v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "prompt",
        "benchmark",
        "evaluation"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17288v1",
      "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
      "summary": "While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight ",
      "authors": [
        "Anuj Gupta"
      ],
      "published": "2026-02-19T11:47:30Z",
      "updated": "2026-02-19T11:47:30Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17288v1",
      "abs_url": "https://arxiv.org/abs/2602.17288v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "language model",
        "transformer",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "reasoning"
    },
    {
      "id": "2602.17283v1",
      "title": "Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective",
      "summary": "While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global conse",
      "authors": [
        "Yukun Chen",
        "Xinyu Zhang",
        "Jialong Tang",
        "Yu Wan",
        "Baosong Yang",
        "Yiming Li",
        "Zhan Qin",
        "Kui Ren"
      ],
      "published": "2026-02-19T11:41:34Z",
      "updated": "2026-02-19T11:41:34Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17283v1",
      "abs_url": "https://arxiv.org/abs/2602.17283v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "evaluation",
        "safety"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17262v1",
      "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study",
      "summary": "Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and respo",
      "authors": [
        "Kensuke Okada",
        "Yui Furukawa",
        "Kyosuke Bunji"
      ],
      "published": "2026-02-19T11:07:24Z",
      "updated": "2026-02-19T11:07:24Z",
      "categories": [
        "cs.CL",
        "stat.ME"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17262v1",
      "abs_url": "https://arxiv.org/abs/2602.17262v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "evaluation",
        "safety"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17095v1",
      "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
      "summary": "Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLo",
      "authors": [
        "Chuiyang Meng",
        "Ming Tang",
        "Vincent W. S. Wong"
      ],
      "published": "2026-02-19T05:35:23Z",
      "updated": "2026-02-19T05:35:23Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17095v1",
      "abs_url": "https://arxiv.org/abs/2602.17095v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "fine-tuning",
        "benchmark",
        "alignment"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17053v1",
      "title": "RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models",
      "summary": "Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failur",
      "authors": [
        "Yunseok Han",
        "Yejoon Lee",
        "Jaeyoung Do"
      ],
      "published": "2026-02-19T03:49:37Z",
      "updated": "2026-02-19T03:49:37Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17053v1",
      "abs_url": "https://arxiv.org/abs/2602.17053v1",
      "relevance_score": 8,
      "matched_keywords": [
        "fine-tuning",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17025v1",
      "title": "WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning",
      "summary": "Group Relative Policy Optimization (GRPO) is effective for training language models on complex reasoning. However, since the objective is defined relative to a group of sampled trajectories, extended deliberation can create more chances to realize relative gains, leading to inefficient reasoning and overthinking, and complicating the trade-off between correctness and rollout efficiency. Controlling this behavior is difficult in practice, considering (i) Length penalties are hard to calibrate because longer rollouts may reflect harder problems that require longer reasoning, penalizing tokens risks truncating useful reasoning along with redundant continuation; and (ii) supervision that directly indicates when to continue or stop is typically unavailable beyond final answer correctness. We pr",
      "authors": [
        "Gagan Mundada",
        "Zihan Huang",
        "Rohan Surana",
        "Sheldon Yu",
        "Jennifer Yuntong Zhang",
        "Xintong Li",
        "Tong Yu",
        "Lina Yao",
        "Jingbo Shang",
        "Julian McAuley",
        "Junda Wu"
      ],
      "published": "2026-02-19T02:43:35Z",
      "updated": "2026-02-19T02:43:35Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17025v1",
      "abs_url": "https://arxiv.org/abs/2602.17025v1",
      "relevance_score": 8,
      "matched_keywords": [
        "language model",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17655v1",
      "title": "What Language is This? Ask Your Tokenizer",
      "summary": "Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental ",
      "authors": [
        "Clara Meister",
        "Ahmetcan Yavuz",
        "Pietro Lesci",
        "Tiago Pimentel"
      ],
      "published": "2026-02-19T18:58:39Z",
      "updated": "2026-02-19T18:58:39Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17655v1",
      "abs_url": "https://arxiv.org/abs/2602.17655v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "language model",
        "benchmark",
        "evaluation",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17594v1",
      "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
      "summary": "Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \\textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans f",
      "authors": [
        "Lance Ying",
        "Ryan Truong",
        "Prafull Sharma",
        "Kaiya Ivy Zhao",
        "Nathan Cloos",
        "Kelsey R. Allen",
        "Thomas L. Griffiths",
        "Katherine M. Collins",
        "José Hernández-Orallo",
        "Phillip Isola",
        "Samuel J. Gershman",
        "Joshua B. Tenenbaum"
      ],
      "published": "2026-02-19T18:17:25Z",
      "updated": "2026-02-19T18:17:25Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17594v1",
      "abs_url": "https://arxiv.org/abs/2602.17594v1",
      "relevance_score": 7,
      "matched_keywords": [
        "llm",
        "language model",
        "benchmark",
        "evaluation",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17542v1",
      "title": "Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems",
      "summary": "Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism ",
      "authors": [
        "Zhangqi Duan",
        "Arnav Kankaria",
        "Dhruv Kartik",
        "Andrew Lan"
      ],
      "published": "2026-02-19T16:58:34Z",
      "updated": "2026-02-19T16:58:34Z",
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17542v1",
      "abs_url": "https://arxiv.org/abs/2602.17542v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "evaluation",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17445v1",
      "title": "ABCD: All Biases Come Disguised",
      "summary": "Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between diffe",
      "authors": [
        "Mateusz Nowak",
        "Xavier Cadet",
        "Peter Chin"
      ],
      "published": "2026-02-19T15:12:33Z",
      "updated": "2026-02-19T15:12:33Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17445v1",
      "abs_url": "https://arxiv.org/abs/2602.17445v1",
      "relevance_score": 7,
      "matched_keywords": [
        "llm",
        "prompt",
        "few-shot",
        "benchmark",
        "evaluation"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17386v1",
      "title": "Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval",
      "summary": "Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are bo",
      "authors": [
        "Adrià Molina",
        "Oriol Ramos Terrades",
        "Josep Lladós"
      ],
      "published": "2026-02-19T14:10:55Z",
      "updated": "2026-02-19T14:10:55Z",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17386v1",
      "abs_url": "https://arxiv.org/abs/2602.17386v1",
      "relevance_score": 7,
      "matched_keywords": [
        "code generation",
        "reasoning",
        "reasoning"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17366v1",
      "title": "RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering",
      "summary": "Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We eval",
      "authors": [
        "Yiming Zhang",
        "Siyue Zhang",
        "Junbo Zhao",
        "Chen Zhao"
      ],
      "published": "2026-02-19T13:49:39Z",
      "updated": "2026-02-19T13:49:39Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17366v1",
      "abs_url": "https://arxiv.org/abs/2602.17366v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17245v1",
      "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web",
      "summary": "The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \\textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and com",
      "authors": [
        "Linxi Jiang",
        "Rui Xi",
        "Zhijie Liu",
        "Shuo Chen",
        "Zhiqiang Lin",
        "Suman Nath"
      ],
      "published": "2026-02-19T10:50:52Z",
      "updated": "2026-02-19T10:50:52Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17245v1",
      "abs_url": "https://arxiv.org/abs/2602.17245v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "agentic"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17242v1",
      "title": "TAPO-Structured Description Logic for Information Behavior: Procedural and Oracle-Based Extensions",
      "summary": "We introduce \\emph{TAPO-Structured Description Logic} (TAPO--DL), a formal extension of classical description logic designed to model \\emph{information behavior} as a structured, dynamic process. TAPO--DL extends the standard T--Box/A--Box architecture with two additional layers: a \\emph{Procedural Box} (P--Box), which supports concept-driven, imperative-style programs such as conditional and iterative actions, and an \\emph{Oracle Box} (O--Box), which formalizes controlled interaction with external information sources. While the terminological and assertional components capture static conceptual and factual knowledge, the procedural and oracle-based components enable the explicit representation of information-generating actions and external validation. We provide a unified semantic framewo",
      "authors": [
        "Takao Inoué"
      ],
      "published": "2026-02-19T10:43:21Z",
      "updated": "2026-02-19T10:43:21Z",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17242v1",
      "abs_url": "https://arxiv.org/abs/2602.17242v1",
      "relevance_score": 7,
      "matched_keywords": [
        "agent",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17234v1",
      "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting",
      "summary": "To evaluate whether LLMs can accurately predict future events, we need the ability to \\textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \\emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \\textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \\textbf{Shapley}-weighted \\textbf{D}ecision-\\textbf{C}ritical \\textbf{L}eakage \\textbf{R}ate (\\textbf{Shapl",
      "authors": [
        "Zeyu Zhang",
        "Ryan Chen",
        "Bradly C. Stadie"
      ],
      "published": "2026-02-19T10:28:00Z",
      "updated": "2026-02-19T10:28:00Z",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17234v1",
      "abs_url": "https://arxiv.org/abs/2602.17234v1",
      "relevance_score": 7,
      "matched_keywords": [
        "llm",
        "prompt",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17106v1",
      "title": "Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction",
      "summary": "Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating &amp; Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLM",
      "authors": [
        "Xiaoran Cai",
        "Wang Yang",
        "Xiyu Ren",
        "Chekun Law",
        "Rohit Sharma",
        "Peng Qi"
      ],
      "published": "2026-02-19T06:04:28Z",
      "updated": "2026-02-19T06:04:28Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17106v1",
      "abs_url": "https://arxiv.org/abs/2602.17106v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "evaluation"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17084v1",
      "title": "How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses",
      "summary": "The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We ob",
      "authors": [
        "Kan Watanabe",
        "Rikuto Tsuchida",
        "Takahiro Monno",
        "Bin Huang",
        "Kazuma Yamasaki",
        "Youmei Fan",
        "Kazumasa Shimari",
        "Kenichi Matsumoto"
      ],
      "published": "2026-02-19T05:06:31Z",
      "updated": "2026-02-19T05:06:31Z",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17084v1",
      "abs_url": "https://arxiv.org/abs/2602.17084v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "language model",
        "agent"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17062v1",
      "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning",
      "summary": "Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performan",
      "authors": [
        "Yonghyeon Jo",
        "Sunwoo Lee",
        "Seungyul Han"
      ],
      "published": "2026-02-19T04:07:55Z",
      "updated": "2026-02-19T04:07:55Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17062v1",
      "abs_url": "https://arxiv.org/abs/2602.17062v1",
      "relevance_score": 7,
      "matched_keywords": [
        "agent",
        "benchmark",
        "rag"
      ],
      "category": "agents"
    }
  ],
  "recent_papers": [
    {
      "id": "2602.17596v1",
      "title": "Asymptotic Smoothing of the Lipschitz Loss Landscape in Overparameterized One-Hidden-Layer ReLU Networks",
      "summary": "We study the topology of the loss landscape of one-hidden-layer ReLU networks under overparameterization. On the theory side, we (i) prove that for convex $L$-Lipschitz losses with an $\\ell_1$-regularized second layer, every pair of models at the same loss level can be connected by a continuous path within an arbitrarily small loss increase $ε$ (extending a known result for the quadratic loss); (ii) obtain an asymptotic upper bound on the energy gap $ε$ between local and global minima that vanishes as the width $m$ grows, implying that the landscape flattens and sublevel sets become connected in the limit. Empirically, on a synthetic Moons dataset and on the Wisconsin Breast Cancer dataset, we measure pairwise energy gaps via Dynamic String Sampling (DSS) and find that wider networks exhib",
      "authors": [
        "Saveliy Baturin"
      ],
      "published": "2026-02-19T18:20:21Z",
      "updated": "2026-02-19T18:20:21Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17596v1",
      "abs_url": "https://arxiv.org/abs/2602.17596v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.17587v1",
      "title": "Asymptotically Optimal Sequential Testing with Markovian Data",
      "summary": "We study one-sided and $α$-correct sequential hypothesis testing for data generated by an ergodic Markov chain. The null hypothesis is that the unknown transition matrix belongs to a prescribed set $P$ of stochastic matrices, and the alternative corresponds to a disjoint set $Q$. We establish a tight non-asymptotic instance-dependent lower bound on the expected stopping time of any valid sequential test under the alternative. Our novel analysis improves the existing lower bounds, which are either asymptotic or provably sub-optimal in this setting. Our lower bound incorporates both the stationary distribution and the transition structure induced by the unknown Markov chain. We further propose an optimal test whose expected stopping time matches this lower bound asymptotically as $α\\to 0$. W",
      "authors": [
        "Alhad Sethi",
        "Kavali Sofia Sagar",
        "Shubhada Agrawal",
        "Debabrota Basu",
        "P. N. Karthik"
      ],
      "published": "2026-02-19T18:11:02Z",
      "updated": "2026-02-19T18:11:02Z",
      "categories": [
        "math.ST",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17587v1",
      "abs_url": "https://arxiv.org/abs/2602.17587v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.17577v1",
      "title": "Simultaneous Blackwell Approachability and Applications to Multiclass Omniprediction",
      "summary": "Omniprediction is a learning problem that requires suboptimality bounds for each of a family of losses $\\mathcal{L}$ against a family of comparator predictors $\\mathcal{C}$. We initiate the study of omniprediction in a multiclass setting, where the comparator family $\\mathcal{C}$ may be infinite. Our main result is an extension of the recent binary omniprediction algorithm of [OKK25] to the multiclass setting, with sample complexity (in statistical settings) or regret horizon (in online settings) $\\approx \\varepsilon^{-(k+1)}$, for $\\varepsilon$-omniprediction in a $k$-class prediction problem. En route to proving this result, we design a framework of potential broader interest for solving Blackwell approachability problems where multiple sets must simultaneously be approached via coupled ",
      "authors": [
        "Lunjia Hu",
        "Kevin Tian",
        "Chutong Yang"
      ],
      "published": "2026-02-19T18:02:03Z",
      "updated": "2026-02-19T18:02:03Z",
      "categories": [
        "cs.DS",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17577v1",
      "abs_url": "https://arxiv.org/abs/2602.17577v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.17530v1",
      "title": "Provably Explaining Neural Additive Models",
      "summary": "Despite significant progress in post-hoc explanation methods for neural networks, many remain heuristic and lack provable guarantees. A key approach for obtaining explanations with provable guarantees is by identifying a cardinally-minimal subset of input features which by itself is provably sufficient to determine the prediction. However, for standard neural networks, this task is often computationally infeasible, as it demands a worst-case exponential number of verification queries in the number of input features, each of which is NP-hard. In this work, we show that for Neural Additive Models (NAMs), a recent and more interpretable neural network family, we can efficiently generate explanations with such guarantees. We present a new model-specific algorithm for NAMs that generates provab",
      "authors": [
        "Shahaf Bassan",
        "Yizhak Yisrael Elboher",
        "Tobias Ladner",
        "Volkan Şahin",
        "Jan Kretinsky",
        "Matthias Althoff",
        "Guy Katz"
      ],
      "published": "2026-02-19T16:42:29Z",
      "updated": "2026-02-19T16:42:29Z",
      "categories": [
        "cs.LG",
        "cs.CC",
        "cs.LO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17530v1",
      "abs_url": "https://arxiv.org/abs/2602.17530v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.17484v1",
      "title": "Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection",
      "summary": "Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace's verified mappings. Our method bridges pixel-level traceability wi",
      "authors": [
        "Yichen Lu",
        "Siwei Nie",
        "Minlong Lu",
        "Xudong Yang",
        "Xiaobo Zhang",
        "Peng Zhang"
      ],
      "published": "2026-02-19T15:54:55Z",
      "updated": "2026-02-19T15:54:55Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17484v1",
      "abs_url": "https://arxiv.org/abs/2602.17484v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.17423v1",
      "title": "Convergence Analysis of Two-Layer Neural Networks under Gaussian Input Masking",
      "summary": "We investigate the convergence guarantee of two-layer neural network training with Gaussian randomly masked inputs. This scenario corresponds to Gaussian dropout at the input level, or noisy input training common in sensor networks, privacy-preserving training, and federated learning, where each user may have access to partial or corrupted features. Using a Neural Tangent Kernel (NTK) analysis, we demonstrate that training a two-layer ReLU network with Gaussian randomly masked inputs achieves linear convergence up to an error region proportional to the mask's variance. A key technical contribution is resolving the randomness within the non-linear activation, a problem of independent interest.",
      "authors": [
        "Afroditi Kolomvaki",
        "Fangshuo Liao",
        "Evan Dramko",
        "Ziyun Guang",
        "Anastasios Kyrillidis"
      ],
      "published": "2026-02-19T14:55:10Z",
      "updated": "2026-02-19T14:55:10Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS",
        "math.OC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17423v1",
      "abs_url": "https://arxiv.org/abs/2602.17423v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.17397v1",
      "title": "A High-Level Survey of Optical Remote Sensing",
      "summary": "In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most",
      "authors": [
        "Panagiotis Koletsis",
        "Vasilis Efthymiou",
        "Maria Vakalopoulou",
        "Nikos Komodakis",
        "Anastasios Doulamis",
        "Georgios Th. Papadopoulos"
      ],
      "published": "2026-02-19T14:26:26Z",
      "updated": "2026-02-19T14:26:26Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17397v1",
      "abs_url": "https://arxiv.org/abs/2602.17397v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.17375v1",
      "title": "MDP Planning as Policy Inference",
      "summary": "We cast episodic Markov decision process (MDP) planning as Bayesian inference over _policies_. A policy is treated as the latent variable and is assigned an unnormalized probability of optimality that is monotone in its expected return, yielding a posterior distribution whose modes coincide with return-maximizing solutions while posterior dispersion represents uncertainty over optimal behavior. To approximate this posterior in discrete domains, we adapt variational sequential Monte Carlo (VSMC) to inference over deterministic policies under stochastic dynamics, introducing a sweep that enforces policy consistency across revisited states and couples transition randomness across particles to avoid confounding from simulator noise. Acting is performed by posterior predictive sampling, which i",
      "authors": [
        "David Tolpin"
      ],
      "published": "2026-02-19T13:56:31Z",
      "updated": "2026-02-19T13:56:31Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17375v1",
      "abs_url": "https://arxiv.org/abs/2602.17375v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.17350v1",
      "title": "Shortcut learning in geometric knot classification",
      "summary": "Classifying the topology of closed curves is a central problem in low dimensional topology with applications beyond mathematics spanning protein folding, polymer physics and even magnetohydrodynamics. The central problem is how to determine whether two embeddings of a closed arc are equivalent under ambient isotopy. Given the striking ability of neural networks to solve complex classification tasks, it is therefore natural to ask if the knot classification problem can be tackled using Machine Learning (ML). In this paper, we investigate generic shortcut methods employed by ML to solve the knot classification challenge and specifically discover hidden non-topological features in training data generated through Molecular Dynamics simulations of polygonal knots that are used by ML to arrive t",
      "authors": [
        "Djordje Mihajlovic",
        "Davide Michieletto"
      ],
      "published": "2026-02-19T13:36:19Z",
      "updated": "2026-02-19T13:36:19Z",
      "categories": [
        "cs.LG",
        "cond-mat.soft",
        "math.GT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17350v1",
      "abs_url": "https://arxiv.org/abs/2602.17350v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.17346v1",
      "title": "Partial Optimality in the Preordering Problem",
      "summary": "Preordering is a generalization of clustering and partial ordering with applications in bioinformatics and social network analysis. Given a finite set $V$ and a value $c_{ab} \\in \\mathbb{R}$ for every ordered pair $ab$ of elements of $V$, the preordering problem asks for a preorder $\\lesssim$ on $V$ that maximizes the sum of the values of those pairs $ab$ for which $a \\lesssim b$. Building on the state of the art in solving this NP-hard problem partially, we contribute new partial optimality conditions and efficient algorithms for deciding these conditions. In experiments with real and synthetic data, these new conditions increase, in particular, the fraction of pairs $ab$ for which it is decided efficiently that $a \\not\\lesssim b$ in an optimal preorder.",
      "authors": [
        "David Stein",
        "Jannik Irmai",
        "Bjoern Andres"
      ],
      "published": "2026-02-19T13:29:09Z",
      "updated": "2026-02-19T13:29:09Z",
      "categories": [
        "cs.DM",
        "cs.DS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17346v1",
      "abs_url": "https://arxiv.org/abs/2602.17346v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.17321v1",
      "title": "The Sound of Death: Deep Learning Reveals Vascular Damage from Carotid Ultrasound",
      "summary": "Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, yet early risk detection is often limited by available diagnostics. Carotid ultrasound, a non-invasive and widely accessible modality, encodes rich structural and hemodynamic information that is largely untapped. Here, we present a machine learning (ML) framework that extracts clinically meaningful representations of vascular damage (VD) from carotid ultrasound videos, using hypertension as a weak proxy label. The model learns robust features that are biologically plausible, interpretable, and strongly associated with established cardiovascular risk factors, comorbidities, and laboratory measures. High VD stratifies individuals for myocardial infarction, cardiac death, and all-cause mortality, matching or outpe",
      "authors": [
        "Christoph Balada",
        "Aida Romano-Martinez",
        "Payal Varshney",
        "Vincent ten Cate",
        "Katharina Geschke",
        "Jonas Tesarz",
        "Paul Claßen",
        "Alexander K. Schuster",
        "Dativa Tibyampansha",
        "Karl-Patrik Kresoja",
        "Philipp S. Wild",
        "Sheraz Ahmed",
        "Andreas Dengel"
      ],
      "published": "2026-02-19T12:37:48Z",
      "updated": "2026-02-19T12:37:48Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17321v1",
      "abs_url": "https://arxiv.org/abs/2602.17321v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.17314v1",
      "title": "Open Datasets in Learning Analytics: Trends, Challenges, and Best PRACTICE",
      "summary": "Open datasets play a crucial role in three research domains that intersect data science and education: learning analytics, educational data mining, and artificial intelligence in education. Researchers in these domains apply computational methods to analyze data from educational contexts, aiming to better understand and improve teaching and learning. Providing open datasets alongside research papers supports reproducibility, collaboration, and trust in research findings. It also provides individual benefits for authors, such as greater visibility, credibility, and citation potential. Despite these advantages, the availability of open datasets and the associated practices within the learning analytics research communities, especially at their flagship conference venues, remain unclear. We s",
      "authors": [
        "Valdemar Švábenský",
        "Brendan Flanagan",
        "Erwin Daniel López Zapata",
        "Atsushi Shimada"
      ],
      "published": "2026-02-19T12:23:25Z",
      "updated": "2026-02-19T12:23:25Z",
      "categories": [
        "cs.CY",
        "cs.DB",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17314v1",
      "abs_url": "https://arxiv.org/abs/2602.17314v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.17284v1",
      "title": "Efficient privacy loss accounting for subsampling and random allocation",
      "summary": "We consider the privacy amplification properties of a sampling scheme in which a user's data is used in $k$ steps chosen randomly and uniformly from a sequence (or set) of $t$ steps. This sampling scheme has been recently applied in the context of differentially private optimization (Chua et al., 2024a; Choquette-Choo et al., 2025) and communication-efficient high-dimensional private aggregation (Asi et al., 2025), where it was shown to have utility advantages over the standard Poisson sampling. Theoretical analyses of this sampling scheme (Feldman &amp; Shenfeld, 2025; Dong et al., 2025) lead to bounds that are close to those of Poisson sampling, yet still have two significant shortcomings. First, in many practical settings, the resulting privacy parameters are not tight due to the approx",
      "authors": [
        "Vitaly Feldman",
        "Moshe Shenfeld"
      ],
      "published": "2026-02-19T11:44:25Z",
      "updated": "2026-02-19T11:44:25Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17284v1",
      "abs_url": "https://arxiv.org/abs/2602.17284v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.17276v1",
      "title": "RLGT: A reinforcement learning framework for extremal graph theory",
      "summary": "Reinforcement learning (RL) is a subfield of machine learning that focuses on developing models that can autonomously learn optimal decision-making strategies over time. In a recent pioneering paper, Wagner demonstrated how the Deep Cross-Entropy RL method can be applied to tackle various problems from extremal graph theory by reformulating them as combinatorial optimization problems. Subsequently, many researchers became interested in refining and extending the framework introduced by Wagner, thereby creating various RL environments specialized for graph theory. Moreover, a number of problems from extremal graph theory were solved through the use of RL. In particular, several inequalities concerning the Laplacian spectral radius of graphs were refuted, new lower bounds were obtained for c",
      "authors": [
        "Ivan Damnjanović",
        "Uroš Milivojević",
        "Irena Đorđević",
        "Dragan Stevanović"
      ],
      "published": "2026-02-19T11:25:22Z",
      "updated": "2026-02-19T11:25:22Z",
      "categories": [
        "cs.LG",
        "math.CO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17276v1",
      "abs_url": "https://arxiv.org/abs/2602.17276v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.17270v1",
      "title": "Unified Latents (UL): How to train your latents",
      "summary": "We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.",
      "authors": [
        "Jonathan Heek",
        "Emiel Hoogeboom",
        "Thomas Mensink",
        "Tim Salimans"
      ],
      "published": "2026-02-19T11:18:12Z",
      "updated": "2026-02-19T11:18:12Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17270v1",
      "abs_url": "https://arxiv.org/abs/2602.17270v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.17263v1",
      "title": "Learning a Latent Pulse Shape Interface for Photoinjector Laser Systems",
      "summary": "Controlling the longitudinal laser pulse shape in photoinjectors of Free-Electron Lasers is a powerful lever for optimizing electron beam quality, but systematic exploration of the vast design space is limited by the cost of brute-force pulse propagation simulations. We present a generative modeling framework based on Wasserstein Autoencoders to learn a differentiable latent interface between pulse shaping and downstream beam dynamics. Our empirical findings show that the learned latent space is continuous and interpretable while maintaining high-fidelity reconstructions. Pulse families such as higher-order Gaussians trace coherent trajectories, while standardizing the temporal pulse lengths shows a latent organization correlated with pulse energy. Analysis via principal components and Gau",
      "authors": [
        "Alexander Klemps",
        "Denis Ilia",
        "Pradeep Kr. Banerjee",
        "Ye Chen",
        "Henrik Tünnermann",
        "Nihat Ay"
      ],
      "published": "2026-02-19T11:09:33Z",
      "updated": "2026-02-19T11:09:33Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17263v1",
      "abs_url": "https://arxiv.org/abs/2602.17263v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.17211v1",
      "title": "MGD: Moment Guided Diffusion for Maximum Entropy Generation",
      "summary": "Generating samples from limited information is a fundamental problem across scientific domains. Classical maximum entropy methods provide principled uncertainty quantification from moment constraints but require sampling via MCMC or Langevin dynamics, which typically exhibit exponential slowdown in high dimensions. In contrast, generative models based on diffusion and flow matching efficiently transport noise to data but offer limited theoretical guarantees and can overfit when data is scarce. We introduce Moment Guided Diffusion (MGD), which combines elements of both approaches. Building on the stochastic interpolant framework, MGD samples maximum entropy distributions by solving a stochastic differential equation that guides moments toward prescribed values in finite time, thereby avoidi",
      "authors": [
        "Etienne Lempereur",
        "Nathanaël Cuvelle--Magar",
        "Florentin Coeurdoux",
        "Stéphane Mallat",
        "Eric Vanden-Eijnden"
      ],
      "published": "2026-02-19T10:03:03Z",
      "updated": "2026-02-19T10:03:03Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17211v1",
      "abs_url": "https://arxiv.org/abs/2602.17211v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.17206v1",
      "title": "SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch",
      "summary": "We present softdtw-cuda-torch, an open-source PyTorch library for computing Soft Dynamic Time Warping (SoftDTW) on GPUs. Our implementation addresses three key limitations of existing GPU implementations of SoftDTW: a hard sequence-length cap of 1024, numerical instability in the backward pass for small smoothing parameters, and excessive GPU memory consumption from materializing pairwise distance tensors. We introduce (1) tiled anti-diagonal kernel execution that removes the sequence-length constraint, (2) a log-space back-ward pass that prevents floating-point overflow, and (3) a fused distance-computation mode that eliminates the O(BN M ) intermediate distance tensor, achieving up to 98% memory reduction compared to prior work. The library supports arbitrary sequence lengths, full PyTor",
      "authors": [
        "Ron Shapira Weber",
        "Oren Freifeld"
      ],
      "published": "2026-02-19T09:53:03Z",
      "updated": "2026-02-19T09:53:03Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17206v1",
      "abs_url": "https://arxiv.org/abs/2602.17206v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.17189v1",
      "title": "Texo: Formula Recognition within 20M Parameters",
      "summary": "In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.",
      "authors": [
        "Sicheng Mao"
      ],
      "published": "2026-02-19T09:14:32Z",
      "updated": "2026-02-19T09:14:32Z",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17189v1",
      "abs_url": "https://arxiv.org/abs/2602.17189v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.17145v1",
      "title": "Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning",
      "summary": "As the need for more accurate and powerful Convolutional Neural Networks (CNNs) increases, so too does the size, execution time, memory footprint, and power consumption. To overcome this, solutions such as pruning have been proposed with their own metrics and methodologies, or criteria, for how weights should be removed. These solutions do not share a common implementation and are difficult to implement and compare. In this work, we introduce Combine, a criterion- based pruning solution and demonstrate that it is fast and effective framework for iterative pruning, demonstrate that criterion have differing effects on different models, create a standard language for comparing criterion functions, and propose a few novel criterion functions. We show the capacity of these criterion functions a",
      "authors": [
        "Joseph Bingham",
        "Sam Helmich"
      ],
      "published": "2026-02-19T07:46:08Z",
      "updated": "2026-02-19T07:46:08Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17145v1",
      "abs_url": "https://arxiv.org/abs/2602.17145v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    }
  ]
}