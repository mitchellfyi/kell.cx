{
  "scraped_at": "2026-02-11T23:51:06.111Z",
  "categories_searched": [
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "stats": {
    "total_fetched": 200,
    "relevant_count": 50,
    "recent_count": 20,
    "by_category": {
      "agents": 17,
      "code-generation": 11,
      "benchmarks": 16,
      "safety": 2,
      "reasoning": 4
    }
  },
  "relevant_papers": [
    {
      "id": "2602.09447v1",
      "title": "SWE-AGI: Benchmarking Specification-Driven Software Construction with MoonBit in the Era of Autonomous Agents",
      "summary": "Although large language models (LLMs) have demonstrated impressive coding capabilities, their ability to autonomously build production-scale software from explicit specifications remains an open question. We introduce SWE-AGI, an open-source benchmark for evaluating end-to-end, specification-driven construction of software systems written in MoonBit. SWE-AGI tasks require LLM-based agents to implement parsers, interpreters, binary decoders, and SAT solvers strictly from authoritative standards and RFCs under a fixed API scaffold. Each task involves implementing 1,000-10,000 lines of core logic, corresponding to weeks or months of engineering effort for an experienced human developer. By leveraging the nascent MoonBit ecosystem, SWE-AGI minimizes data leakage, forcing agents to rely on long",
      "authors": [
        "Zhirui Zhang",
        "Hongbo Zhang",
        "Haoxiang Fei",
        "Zhiyuan Bao",
        "Yubin Chen",
        "Zhengyu Lei",
        "Ziyue Liu",
        "Yixuan Sun",
        "Mingkun Xiao",
        "Zihang Ye",
        "Yu Zhang",
        "Hongcheng Zhu",
        "Yuxiang Wen",
        "Heung-Yeung Shum"
      ],
      "published": "2026-02-10T06:31:47Z",
      "updated": "2026-02-10T06:31:47Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09447v1",
      "abs_url": "https://arxiv.org/abs/2602.09447v1",
      "relevance_score": 22,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "claude",
        "agent",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.10092v1",
      "title": "Quantum-Audit: Evaluating the Reasoning Limits of LLMs on Quantum Computing",
      "summary": "Language models have become practical tools for quantum computing education and research, from summarizing technical papers to explaining theoretical concepts and answering questions about recent developments in the field. While existing benchmarks evaluate quantum code generation and circuit design, their understanding of quantum computing concepts has not been systematically measured. Quantum-Audit addresses this gap with 2,700 questions covering core quantum computing topics. We evaluate 26 models from leading organizations. Our benchmark comprises 1,000 expert-written questions, 1,000 questions extracted from research papers using LLMs and validated by experts, plus an additional 700 questions including 350 open-ended questions and 350 questions with false premises to test whether mode",
      "authors": [
        "Mohamed Afane",
        "Kayla Laufer",
        "Wenqi Wei",
        "Ying Mao",
        "Junaid Farooq",
        "Ying Wang",
        "Juntao Chen"
      ],
      "published": "2026-02-10T18:56:04Z",
      "updated": "2026-02-10T18:56:04Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10092v1",
      "abs_url": "https://arxiv.org/abs/2602.10092v1",
      "relevance_score": 16,
      "matched_keywords": [
        "llm",
        "language model",
        "claude",
        "code generation",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.10063v1",
      "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes",
      "summary": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mind",
      "authors": [
        "Tianyi Jiang",
        "Arctanx An",
        "Hengyi Feng",
        "Naixin Zhai",
        "Haodong Li",
        "Xiaomin Yu",
        "Jiahui Liu",
        "Hanwen Du",
        "Shuo Zhang",
        "Zhi Yang",
        "Jie Huang",
        "Yuhua Li",
        "Yongxin Ni",
        "Huacan Wang",
        "Ronghao Chen"
      ],
      "published": "2026-02-10T18:31:47Z",
      "updated": "2026-02-10T18:31:47Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10063v1",
      "abs_url": "https://arxiv.org/abs/2602.10063v1",
      "relevance_score": 16,
      "matched_keywords": [
        "llm",
        "gemini",
        "agent",
        "agentic",
        "code generation",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09937v1",
      "title": "Why Do AI Agents Systematically Fail at Cloud Root Cause Analysis?",
      "summary": "Failures in large-scale cloud systems incur substantial financial losses, making automated Root Cause Analysis (RCA) essential for operational stability. Recent efforts leverage Large Language Model (LLM) agents to automate this task, yet existing systems exhibit low detection accuracy even with capable models, and current evaluation frameworks assess only final answer correctness without revealing why the agent's reasoning failed. This paper presents a process level failure analysis of LLM-based RCA agents. We execute the full OpenRCA benchmark across five LLM models, producing 1,675 agent runs, and classify observed failures into 12 pitfall types across intra-agent reasoning, inter-agent communication, and agent-environment interaction. Our analysis reveals that the most prevalent pitfal",
      "authors": [
        "Taeyoon Kim",
        "Woohyeok Park",
        "Hoyeong Yun",
        "Kyungyong Lee"
      ],
      "published": "2026-02-10T16:14:05Z",
      "updated": "2026-02-10T16:14:05Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09937v1",
      "abs_url": "https://arxiv.org/abs/2602.09937v1",
      "relevance_score": 16,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "prompt",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09598v1",
      "title": "Learning from the Irrecoverable: Error-Localized Policy Optimization for Tool-Integrated LLM Reasoning",
      "summary": "Tool-integrated reasoning (TIR) enables LLM agents to solve tasks through planning, tool use, and iterative revision, but outcome-only reinforcement learning in this setting suffers from sparse, delayed rewards and weak step-level credit assignment. In long-horizon TIR trajectories, an early irrecoverable mistake can determine success or failure, making it crucial to localize the first irrecoverable step and leverage it for fine-grained credit assignment. We propose Error-Localized Policy Optimization (ELPO), which localizes the first irrecoverable step via binary-search rollout trees under a fixed rollout budget, converts the resulting tree into stable learning signals through hierarchical advantage attribution, and applies error-localized adaptive clipping to strengthen corrective update",
      "authors": [
        "Qiao Liang",
        "Yuke Zhu",
        "Chao Ge",
        "Lei Yang",
        "Ying Shen",
        "Bo Zheng",
        "Sheng Guo"
      ],
      "published": "2026-02-10T09:50:24Z",
      "updated": "2026-02-10T09:50:24Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09598v1",
      "abs_url": "https://arxiv.org/abs/2602.09598v1",
      "relevance_score": 16,
      "matched_keywords": [
        "llm",
        "agent",
        "agentic",
        "tool use",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09517v1",
      "title": "Knowledge Integration Decay in Search-Augmented Reasoning of Large Language Models",
      "summary": "Modern Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks by employing search-augmented reasoning to incorporate external knowledge into long chains of thought. However, we identify a critical yet underexplored bottleneck in this paradigm, termed Knowledge Integration Decay (KID). Specifically, we observe that as the length of reasoning generated before search grows, models increasingly fail to integrate retrieved evidence into subsequent reasoning steps, limiting performance even when relevant information is available. To address this, we propose Self-Anchored Knowledge Encoding (SAKE), a training-free inference-time strategy designed to stabilize knowledge utilization. By anchoring retrieved knowledge at both the beginning and end of the reasoning pro",
      "authors": [
        "Sangwon Yu",
        "Ik-hwan Kim",
        "Donghun Kang",
        "Bongkyu Hwang",
        "Junhwa Choi",
        "Suk-hoon Jung",
        "Seungki Hong",
        "Taehee Lee",
        "Sungroh Yoon"
      ],
      "published": "2026-02-10T08:20:26Z",
      "updated": "2026-02-10T08:20:26Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09517v1",
      "abs_url": "https://arxiv.org/abs/2602.09517v1",
      "relevance_score": 16,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "agentic",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09463v1",
      "title": "SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning",
      "summary": "Large Vision-Language Models (LVLMs) have demonstrated strong reasoning capabilities in geo-localization, yet they often struggle in real-world scenarios where visual cues are sparse, long-tailed, and highly ambiguous. Previous approaches, bound by internal knowledge, often fail to provide verifiable results, yielding confident but ungrounded predictions when faced with confounded evidence. To address these challenges, we propose SpotAgent, a framework that formalizes geo-localization into an agentic reasoning process that leverages expert-level reasoning to synergize visual interpretation with tool-assisted verification. SpotAgent actively explores and verifies visual cues by leveraging external tools (e.g., web search, maps) through a ReAct diagram. We introduce a 3-stage post-training p",
      "authors": [
        "Furong Jia",
        "Ling Dai",
        "Wenjin Deng",
        "Fan Zhang",
        "Chen Hu",
        "Daxin Jiang",
        "Yu Liu"
      ],
      "published": "2026-02-10T06:57:12Z",
      "updated": "2026-02-10T06:57:12Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09463v1",
      "abs_url": "https://arxiv.org/abs/2602.09463v1",
      "relevance_score": 16,
      "matched_keywords": [
        "language model",
        "agent",
        "agentic",
        "fine-tuning",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag",
        "alignment",
        "hallucination"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09443v1",
      "title": "P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads",
      "summary": "The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum ",
      "authors": [
        "Yun Luo",
        "Futing Wang",
        "Qianjia Cheng",
        "Fangchen Yu",
        "Haodi Lei",
        "Jianhao Yan",
        "Chenxi Li",
        "Jiacheng Chen",
        "Yufeng Zhao",
        "Haiyuan Wan",
        "Yuchen Zhang",
        "Shenghe Zheng",
        "Junchi Yao",
        "Qingyang Zhang",
        "Haonan He",
        "Wenxuan Zeng",
        "Li Sheng",
        "Chengxing Xie",
        "Yuxin Zuo",
        "Yizhuo Li",
        "Yulun Wu",
        "Rui Huang",
        "Dongzhan Zhou",
        "Kai Chen",
        "Yu Qiao",
        "Lei Bai",
        "Yu Cheng",
        "Ning Ding",
        "Bowen Zhou",
        "Peng Ye",
        "Ganqu Cui"
      ],
      "published": "2026-02-10T06:28:08Z",
      "updated": "2026-02-10T06:28:08Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09443v1",
      "abs_url": "https://arxiv.org/abs/2602.09443v1",
      "relevance_score": 16,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gemini",
        "agent",
        "agentic",
        "benchmark",
        "reasoning",
        "reasoning",
        "multimodal"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09945v1",
      "title": "Closing Reasoning Gaps in Clinical Agents with Differential Reasoning Learning",
      "summary": "Clinical decision support requires not only correct answers but also clinically valid reasoning. We propose Differential Reasoning Learning (DRL), a framework that improves clinical agents by learning from reasoning discrepancies. From reference reasoning rationales (e.g., physician-authored clinical rationale, clinical guidelines, or outputs from more capable models) and the agent's free-form chain-of-thought (CoT), DRL extracts reasoning graphs as directed acyclic graphs (DAGs) and performs a clinically weighted graph edit distance (GED)-based discrepancy analysis. An LLM-as-a-judge aligns semantically equivalent nodes and diagnoses discrepancies between graphs. These graph-level discrepancy diagnostics are converted into natural-language instructions and stored in a Differential Reasoni",
      "authors": [
        "Jinsong Liu",
        "Yuhang Jiang",
        "Ramayya Krishnan",
        "Rema Padman",
        "Yiye Zhang",
        "Jiang Bian"
      ],
      "published": "2026-02-10T16:29:32Z",
      "updated": "2026-02-10T16:29:32Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09945v1",
      "abs_url": "https://arxiv.org/abs/2602.09945v1",
      "relevance_score": 15,
      "matched_keywords": [
        "llm",
        "agent",
        "prompt",
        "benchmark",
        "evaluation",
        "reasoning",
        "cot",
        "reasoning",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09817v1",
      "title": "AnalyticsGPT: An LLM Workflow for Scientometric Question Answering",
      "summary": "This paper introduces AnalyticsGPT, an intuitive and efficient large language model (LLM)-powered workflow for scientometric question answering. This underrepresented downstream task addresses the subcategory of meta-scientific questions concerning the \"science of science.\" When compared to traditional scientific question answering based on papers, the task poses unique challenges in the planning phase. Namely, the need for named-entity recognition of academic entities within questions and multi-faceted data retrieval involving scientometric indices, e.g. impact factors. Beyond their exceptional capacity for treating traditional natural language processing tasks, LLMs have shown great potential in more complex applications, such as task decomposition and planning and reasoning. In this pap",
      "authors": [
        "Khang Ly",
        "Georgios Cheirmpos",
        "Adrian Raudaschl",
        "Christopher James",
        "Seyed Amin Tabatabaei"
      ],
      "published": "2026-02-10T14:23:55Z",
      "updated": "2026-02-10T14:23:55Z",
      "categories": [
        "cs.CL",
        "cs.DL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09817v1",
      "abs_url": "https://arxiv.org/abs/2602.09817v1",
      "relevance_score": 15,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "agent",
        "agentic",
        "prompt",
        "evaluation",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09712v1",
      "title": "TraceMem: Weaving Narrative Memory Schemata from User Conversational Traces",
      "summary": "Sustaining long-term interactions remains a bottleneck for Large Language Models (LLMs), as their limited context windows struggle to manage dialogue histories that extend over time. Existing memory systems often treat interactions as disjointed snippets, failing to capture the underlying narrative coherence of the dialogue stream. We propose TraceMem, a cognitively-inspired framework that weaves structured, narrative memory schemata from user conversational traces through a three-stage pipeline: (1) Short-term Memory Processing, which employs a deductive topic segmentation approach to demarcate episode boundaries and extract semantic representation; (2) Synaptic Memory Consolidation, a process that summarizes episodes into episodic memories before distilling them alongside semantics into ",
      "authors": [
        "Yiming Shu",
        "Pei Liu",
        "Tiange Zhang",
        "Ruiyang Gao",
        "Jun Ma",
        "Chen Sun"
      ],
      "published": "2026-02-10T12:14:58Z",
      "updated": "2026-02-10T12:14:58Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09712v1",
      "abs_url": "https://arxiv.org/abs/2602.09712v1",
      "relevance_score": 15,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "agentic",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09642v1",
      "title": "MATA: Multi-Agent Framework for Reliable and Flexible Table Question Answering",
      "summary": "Recent advances in Large Language Models (LLMs) have significantly improved table understanding tasks such as Table Question Answering (TableQA), yet challenges remain in ensuring reliability, scalability, and efficiency, especially in resource-constrained or privacy-sensitive environments. In this paper, we introduce MATA, a multi-agent TableQA framework that leverages multiple complementary reasoning paths and a set of tools built with small language models. MATA generates candidate answers through diverse reasoning styles for a given table and question, then refines or selects the optimal answer with the help of these tools. Furthermore, it incorporates an algorithm designed to minimize expensive LLM agent calls, enhancing overall efficiency. MATA maintains strong performance with small",
      "authors": [
        "Sieun Hyeon",
        "Jusang Oh",
        "Sunghwan Steve Cho",
        "Jaeyoung Do"
      ],
      "published": "2026-02-10T10:43:02Z",
      "updated": "2026-02-10T10:43:02Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09642v1",
      "abs_url": "https://arxiv.org/abs/2602.09642v1",
      "relevance_score": 14,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.10081v1",
      "title": "Anagent For Enhancing Scientific Table &amp; Figure Analysis",
      "summary": "In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \\&amp; figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhance",
      "authors": [
        "Xuehang Guo",
        "Zhiyong Lu",
        "Tom Hope",
        "Qingyun Wang"
      ],
      "published": "2026-02-10T18:46:28Z",
      "updated": "2026-02-10T18:46:28Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10081v1",
      "abs_url": "https://arxiv.org/abs/2602.10081v1",
      "relevance_score": 13,
      "matched_keywords": [
        "agent",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning",
        "rag",
        "multimodal"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09629v1",
      "title": "Stop Testing Attacks, Start Diagnosing Defenses: The Four-Checkpoint Framework Reveals Where LLM Safety Breaks",
      "summary": "Large Language Models (LLMs) deploy safety mechanisms to prevent harmful outputs, yet these defenses remain vulnerable to adversarial prompts. While existing research demonstrates that jailbreak attacks succeed, it does not explain \\textit{where} defenses fail or \\textit{why}. To address this gap, we propose that LLM safety operates as a sequential pipeline with distinct checkpoints. We introduce the \\textbf{Four-Checkpoint Framework}, which organizes safety mechanisms along two dimensions: processing stage (input vs.\\ output) and detection level (literal vs.\\ intent). This creates four checkpoints, CP1 through CP4, each representing a defensive layer that can be independently evaluated. We design 13 evasion techniques, each targeting a specific checkpoint, enabling controlled testing of i",
      "authors": [
        "Hayfa Dhabhi",
        "Kashyap Thimmaraju"
      ],
      "published": "2026-02-10T10:17:25Z",
      "updated": "2026-02-10T10:17:25Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "cs.ET",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09629v1",
      "abs_url": "https://arxiv.org/abs/2602.09629v1",
      "relevance_score": 13,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "claude",
        "gemini",
        "prompt",
        "evaluation",
        "safety"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.09438v1",
      "title": "Breaking the Pre-Sampling Barrier: Activation-Informed Difficulty-Aware Self-Consistency",
      "summary": "Self-Consistency (SC) is an effective decoding strategy that improves the reasoning performance of Large Language Models (LLMs) by generating multiple chain-of-thought reasoning paths and selecting the final answer via majority voting. However, it suffers from substantial inference costs because it requires a large number of samples. To mitigate this issue, Difficulty-Adaptive Self-Consistency (DSC) was proposed to reduce unnecessary token usage for easy problems by adjusting the number of samples according to problem difficulty. However, DSC requires additional model calls and pre-sampling to estimate difficulty, and this process is repeated when applying to each dataset, leading to significant computational overhead. In this work, we propose Activation-Informed Difficulty-Aware Self-Cons",
      "authors": [
        "Taewoong Yoon",
        "Geunyeong Jeong",
        "Geon Park",
        "Sihyeong Yeom",
        "Harksoo Kim"
      ],
      "published": "2026-02-10T06:05:11Z",
      "updated": "2026-02-10T06:05:11Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09438v1",
      "abs_url": "https://arxiv.org/abs/2602.09438v1",
      "relevance_score": 13,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.09805v1",
      "title": "Decomposing Reasoning Efficiency in Large Language Models",
      "summary": "Large language models trained for reasoning trade off inference tokens against accuracy, yet standard evaluations report only final accuracy, obscuring where tokens are spent or wasted. We introduce a trace-optional framework that decomposes token efficiency into interpretable factors: completion under a fixed token budget (avoiding truncation), conditional correctness given completion, and verbosity (token usage). When benchmark metadata provides per-instance workload proxies, we further factor verbosity into two components: mean verbalization overhead (tokens per work unit) and a coupling coefficient capturing how overhead scales with task workload. When reasoning traces are available, we add deterministic trace-quality measures (grounding, repetition, prompt copying) to separate degener",
      "authors": [
        "Daniel Kaiser",
        "Arnoldo Frigessi",
        "Ali Ramezani-Kebrya",
        "Benjamin Ricaud"
      ],
      "published": "2026-02-10T14:09:18Z",
      "updated": "2026-02-10T14:09:18Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09805v1",
      "abs_url": "https://arxiv.org/abs/2602.09805v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "prompt",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.09794v1",
      "title": "GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis",
      "summary": "Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. T",
      "authors": [
        "Jiaquan Zhang",
        "Chaoning Zhang",
        "Shuxu Chen",
        "Xudong Wang",
        "Zhenzhen Huang",
        "Pengcheng Zheng",
        "Shuai Yuan",
        "Sheng Zheng",
        "Qigan Sun",
        "Jie Zou",
        "Lik-Hang Lee",
        "Yang Yang"
      ],
      "published": "2026-02-10T14:00:30Z",
      "updated": "2026-02-10T14:00:30Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09794v1",
      "abs_url": "https://arxiv.org/abs/2602.09794v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "cot",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.09634v1",
      "title": "LLM-FS: Zero-Shot Feature Selection for Effective and Interpretable Malware Detection",
      "summary": "Feature selection (FS) remains essential for building accurate and interpretable detection models, particularly in high-dimensional malware datasets. Conventional FS methods such as Extra Trees, Variance Threshold, Tree-based models, Chi-Squared tests, ANOVA, Random Selection, and Sequential Attention rely primarily on statistical heuristics or model-driven importance scores, often overlooking the semantic context of features. Motivated by recent progress in LLM-driven FS, we investigate whether large language models (LLMs) can guide feature selection in a zero-shot setting, using only feature names and task descriptions, as a viable alternative to traditional approaches. We evaluate multiple LLMs (GPT-5.0, GPT-4.0, Gemini-2.5 etc.) on the EMBOD dataset (a fusion of EMBER and BODMAS benchm",
      "authors": [
        "Naveen Gill",
        "Ajvad Haneef K",
        "Madhu Kumar S D"
      ],
      "published": "2026-02-10T10:29:34Z",
      "updated": "2026-02-10T10:29:34Z",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09634v1",
      "abs_url": "https://arxiv.org/abs/2602.09634v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "gemini",
        "benchmark"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.09486v1",
      "title": "Listen to the Layers: Mitigating Hallucinations with Inter-Layer Disagreement",
      "summary": "Pretrained Large Language Models (LLMs) are prone to generating fluent yet factually incorrect text-a phenomenon known as hallucinations, undermining their reliability and utility in downstream tasks. We hypothesize that a generated text span's factuality is correlated with its representational instability across the model's internal layers. Based on this, we propose the CoCoA (Confusion and Consistency Aware) decoder, a novel, training-free decoding algorithm that mitigates hallucinations at inference time by listening to these signals in the middle layers. We propose two metrics to quantify this instability in the middle layers, and use it to penalize outputs that exhibit high internal confusion, thereby steering the model towards more internally consistent and factually grounded outputs",
      "authors": [
        "Koduvayur Subbalakshmi",
        "Sabbir Hossain Ujjal",
        "Venkata Krishna Teja Mangichetty",
        "Nastaran Jamalipour Soofi"
      ],
      "published": "2026-02-10T07:32:37Z",
      "updated": "2026-02-10T07:32:37Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09486v1",
      "abs_url": "https://arxiv.org/abs/2602.09486v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "llama",
        "mistral",
        "code generation",
        "rag",
        "hallucination"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.10090v1",
      "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
      "summary": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compare",
      "authors": [
        "Zhaoyang Wang",
        "Canwen Xu",
        "Boyi Liu",
        "Yite Wang",
        "Siwei Han",
        "Zhewei Yao",
        "Huaxiu Yao",
        "Yuxiong He"
      ],
      "published": "2026-02-10T18:55:41Z",
      "updated": "2026-02-10T18:55:41Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10090v1",
      "abs_url": "https://arxiv.org/abs/2602.10090v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "agentic",
        "benchmark",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.10048v1",
      "title": "Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization",
      "summary": "Large Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose \\textbf{F}ine-grained \\textbf{G}roup policy \\textbf{O}ptimization (\\textbf{FGO}), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning appropriate weights based on length and entropy, thereby enabling effective CoT compression. Meanwhile, as an enhanced variant of Group Relative Policy Optimization (GRPO), FGO successfully addresses two major limitations of the GRPO: inefficient data utilization and entropy collapse. We evaluate FGO on multiple reasoning LLMs and benchmarks, including MATH500, AIME24, AMC23, and Minerva. Experimenta",
      "authors": [
        "Xinchen Han",
        "Hossam Afifi",
        "Michel Marot",
        "Xilu Wang",
        "Lu Yin"
      ],
      "published": "2026-02-10T18:15:58Z",
      "updated": "2026-02-10T18:15:58Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10048v1",
      "abs_url": "https://arxiv.org/abs/2602.10048v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "cot",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.10009v1",
      "title": "Discovering High Level Patterns from Simulation Traces",
      "summary": "Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics. The LM's capability for physical reasoning is learned from observational data, rather than being grounded in simulation. A common approach is to include simulation traces as context, but this suffers from poor scalability as simulation traces contain larger volumes of fine-grained numerical and semantic data. In this paper, we propose a natural language guided method to dis",
      "authors": [
        "Sean Memery",
        "Kartic Subr"
      ],
      "published": "2026-02-10T17:31:39Z",
      "updated": "2026-02-10T17:31:39Z",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10009v1",
      "abs_url": "https://arxiv.org/abs/2602.10009v1",
      "relevance_score": 11,
      "matched_keywords": [
        "language model",
        "agent",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09782v1",
      "title": "Flexible Entropy Control in RLVR with Gradient-Preserving Perspective",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a critical method for enhancing the reasoning capabilities of Large Language Models (LLMs). However, continuous training often leads to policy entropy collapse, characterized by a rapid decay in entropy that results in premature overconfidence, reduced output diversity, and vanishing gradient norms that inhibit learning. Gradient-Preserving Clipping is a primary factor influencing these dynamics, but existing mitigation strategies are largely static and lack a framework connecting clipping mechanisms to precise entropy control. This paper proposes reshaping entropy control in RL from the perspective of Gradient-Preserving Clipping. We first theoretically and empirically verify the contributions of specific importance samp",
      "authors": [
        "Kun Chen",
        "Peng Shi",
        "Fanfan Liu",
        "Haibo Qiu",
        "Zhixiong Zeng",
        "Siqi Yang",
        "Wenji Mao"
      ],
      "published": "2026-02-10T13:42:12Z",
      "updated": "2026-02-10T13:42:12Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09782v1",
      "abs_url": "https://arxiv.org/abs/2602.09782v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.09621v1",
      "title": "AlignTune: Modular Toolkit for Post-Training Alignment of Large Language Models",
      "summary": "Post-training alignment is central to deploying large language models (LLMs), yet practical workflows remain split across backend-specific tools and ad-hoc glue code, making experiments hard to reproduce. We identify backend interference, reward fragmentation, and irreproducible pipelines as key obstacles in alignment research. We introduce AlignTune, a modular toolkit exposing a unified interface for supervised fine-tuning (SFT) and RLHF-style optimization with interchangeable TRL and Unsloth backends. AlignTune standardizes configuration, provides an extensible reward layer (rule-based and learned), and integrates evaluation over standard benchmarks and custom tasks. By isolating backend-specific logic behind a single factory boundary, AlignTune enables controlled comparisons and reprodu",
      "authors": [
        "R E Zera Marveen Lyngkhoi",
        "Chirag Chawla",
        "Pratinav Seth",
        "Utsav Avaiya",
        "Soham Bhattacharjee",
        "Mykola Khandoga",
        "Rui Yuan",
        "Vinay Kumar Sankarapu"
      ],
      "published": "2026-02-10T10:08:51Z",
      "updated": "2026-02-10T10:08:51Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09621v1",
      "abs_url": "https://arxiv.org/abs/2602.09621v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "fine-tuning",
        "rlhf",
        "benchmark",
        "evaluation",
        "rag",
        "alignment"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.09464v1",
      "title": "AlgoVeri: An Aligned Benchmark for Verified Code Generation on Classical Algorithms",
      "summary": "Vericoding refers to the generation of formally verified code from rigorous specifications. Recent AI models show promise in vericoding, but a unified methodology for cross-paradigm evaluation is lacking. Existing benchmarks test only individual languages/tools (e.g., Dafny, Verus, and Lean) and each covers very different tasks, so the performance numbers are not directly comparable. We address this gap with AlgoVeri, a benchmark that evaluates vericoding of $77$ classical algorithms in Dafny, Verus, and Lean. By enforcing identical functional contracts, AlgoVeri reveals critical capability gaps in verification systems. While frontier models achieve tractable success in Dafny ($40.3$% for Gemini-3 Flash), where high-level abstractions and SMT automation simplify the workflow, performance c",
      "authors": [
        "Haoyu Zhao",
        "Ziran Yang",
        "Jiawei Li",
        "Deyuan He",
        "Zenan Li",
        "Chi Jin",
        "Venugopal V. Veeravalli",
        "Aarti Gupta",
        "Sanjeev Arora"
      ],
      "published": "2026-02-10T06:58:26Z",
      "updated": "2026-02-10T06:58:26Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09464v1",
      "abs_url": "https://arxiv.org/abs/2602.09464v1",
      "relevance_score": 11,
      "matched_keywords": [
        "gpt",
        "gemini",
        "code generation",
        "benchmark",
        "evaluation"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.09442v1",
      "title": "Evaluating Social Bias in RAG Systems: When External Context Helps and Reasoning Hurts",
      "summary": "Social biases inherent in large language models (LLMs) raise significant fairness concerns. Retrieval-Augmented Generation (RAG) architectures, which retrieve external knowledge sources to enhance the generative capabilities of LLMs, remain susceptible to the same bias-related challenges. This work focuses on evaluating and understanding the social bias implications of RAG. Through extensive experiments across various retrieval corpora, LLMs, and bias evaluation datasets, encompassing more than 13 different bias types, we surprisingly observe a reduction in bias in RAG. This suggests that the inclusion of external context can help counteract stereotype-driven predictions, potentially improving fairness by diversifying the contextual grounding of the model's outputs. To better understand th",
      "authors": [
        "Shweta Parihar",
        "Lu Cheng"
      ],
      "published": "2026-02-10T06:27:56Z",
      "updated": "2026-02-10T06:27:56Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09442v1",
      "abs_url": "https://arxiv.org/abs/2602.09442v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "prompt",
        "evaluation",
        "reasoning",
        "cot",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.09870v1",
      "title": "Steer2Edit: From Activation Steering to Component-Level Editing",
      "summary": "Steering methods influence Large Language Model behavior by identifying semantic directions in hidden representations, but are typically realized through inference-time activation interventions that apply a fixed, global modification to the model's internal states. While effective, such interventions often induce unfavorable attribute-utility trade-offs under strong control, as they ignore the fact that many behaviors are governed by a small and heterogeneous subset of model components. We propose Steer2Edit, a theoretically grounded, training-free framework that transforms steering vectors from inference-time control signals into diagnostic signals for component-level rank-1 weight editing. Instead of uniformly injecting a steering direction during generation, Steer2Edit selectively redis",
      "authors": [
        "Chung-En Sun",
        "Ge Yan",
        "Zimo Wang",
        "Tsui-Wei Weng"
      ],
      "published": "2026-02-10T15:15:15Z",
      "updated": "2026-02-10T15:15:15Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09870v1",
      "abs_url": "https://arxiv.org/abs/2602.09870v1",
      "relevance_score": 10,
      "matched_keywords": [
        "large language model",
        "language model",
        "reasoning",
        "reasoning",
        "rag",
        "safety",
        "alignment",
        "hallucination"
      ],
      "category": "safety"
    },
    {
      "id": "2602.09856v1",
      "title": "Code2World: A GUI World Model via Renderable Code Generation",
      "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs",
      "authors": [
        "Yuhao Zheng",
        "Li'an Zhong",
        "Yi Wang",
        "Rui Dai",
        "Kaikui Liu",
        "Xiangxiang Chu",
        "Linyuan Lv",
        "Philip Torr",
        "Kevin Qinghong Lin"
      ],
      "published": "2026-02-10T14:56:19Z",
      "updated": "2026-02-10T14:56:19Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09856v1",
      "abs_url": "https://arxiv.org/abs/2602.09856v1",
      "relevance_score": 10,
      "matched_keywords": [
        "gpt",
        "gemini",
        "agent",
        "code generation"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09689v1",
      "title": "Model soups need only one ingredient",
      "summary": "Fine-tuning large pre-trained models on a target distribution often improves in-distribution (ID) accuracy, but at the cost of out-of-distribution (OOD) robustness as representations specialize to the fine-tuning data. Weight-space ensembling methods, such as Model Soups, mitigate this effect by averaging multiple checkpoints, but they are computationally prohibitive, requiring the training and storage of dozens of fine-tuned models. In this paper, we introduce MonoSoup, a simple, data-free, hyperparameter-free, post-hoc method that achieves a strong ID-OOD balance using only a single checkpoint. Our method applies Singular Value Decomposition (SVD) to each layer's update and decomposes it into high-energy directions that capture task-specific adaptation and low-energy directions that intr",
      "authors": [
        "Alireza Abdollahpoorrostam",
        "Nikolaos Dimitriadis",
        "Adam Hazimeh",
        "Pascal Frossard"
      ],
      "published": "2026-02-10T11:44:19Z",
      "updated": "2026-02-10T11:44:19Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09689v1",
      "abs_url": "https://arxiv.org/abs/2602.09689v1",
      "relevance_score": 10,
      "matched_keywords": [
        "language model",
        "fine-tuning",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.09485v1",
      "title": "Bridging Efficiency and Transparency: Explainable CoT Compression in Multimodal Large Reasoning Models",
      "summary": "Long chains of thought (Long CoTs) are widely employed in multimodal reasoning models to tackle complex tasks by capturing detailed visual information. However, these Long CoTs are often excessively lengthy and contain redundant reasoning steps, which can hinder inference efficiency. Compressing these long CoTs is a natural solution, yet existing approaches face two major challenges: (1) they may compromise the integrity of visual-textual reasoning by removing essential alignment cues, and (2) the compression process lacks explainability, making it difficult to discern which information is critical. To address these problems, we propose XMCC, an eXplainable Multimodal CoT Compressor that formulates compression as a sequential decision-making process optimized via reinforcement learning. XM",
      "authors": [
        "Yizhi Wang",
        "Linan Yue",
        "Min-Ling Zhang"
      ],
      "published": "2026-02-10T07:29:50Z",
      "updated": "2026-02-10T07:29:50Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09485v1",
      "abs_url": "https://arxiv.org/abs/2602.09485v1",
      "relevance_score": 10,
      "matched_keywords": [
        "benchmark",
        "reasoning",
        "cot",
        "reasoning",
        "multimodal",
        "alignment"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.10117v1",
      "title": "Biases in the Blind Spot: Detecting What LLMs Fail to Mention",
      "summary": "Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets. In this work, we introduce a fully automated, black-box pipeline for detecting task-specific unverbalized biases. Given a task dataset, the pipeline uses LLM autoraters to generate candidate bias concepts. It then tests each concept on progressively larger input samples by generating positive and negative variations, and applies statistical techniques for multiple testing and early stopping. A concept is flagged as an unverbalized bias if it yields statist",
      "authors": [
        "Ivn Arcuschin",
        "David Chanin",
        "Adri Garriga-Alonso",
        "Oana-Maria Camburu"
      ],
      "published": "2026-02-10T18:59:56Z",
      "updated": "2026-02-10T18:59:56Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10117v1",
      "abs_url": "https://arxiv.org/abs/2602.10117v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "evaluation",
        "reasoning",
        "cot",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.10042v1",
      "title": "Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection",
      "summary": "Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-R",
      "authors": [
        "Changjiang Jiang",
        "Xinkuan Sha",
        "Fengchang Yu",
        "Jingjing Liu",
        "Jian Liu",
        "Mingqi Fang",
        "Chenfeng Zhang",
        "Wei Lu"
      ],
      "published": "2026-02-10T18:10:08Z",
      "updated": "2026-02-10T18:10:08Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10042v1",
      "abs_url": "https://arxiv.org/abs/2602.10042v1",
      "relevance_score": 9,
      "matched_keywords": [
        "llm",
        "language model",
        "fine-tuning",
        "reasoning",
        "cot",
        "reasoning",
        "vision language"
      ],
      "category": "reasoning"
    },
    {
      "id": "2602.10021v1",
      "title": "Decoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference",
      "summary": "The integration of extensive, dynamic knowledge into Large Language Models (LLMs) remains a significant challenge due to the inherent entanglement of factual data and reasoning patterns. Existing solutions, ranging from non-parametric Retrieval-Augmented Generation (RAG) to parametric knowledge editing, are often constrained in practice by finite context windows, retriever noise, or the risk of catastrophic forgetting. In this paper, we propose DRIFT, a novel dual-model architecture designed to explicitly decouple knowledge extraction from the reasoning process. Unlike static prompt compression, DRIFT employs a lightweight knowledge model to dynamically compress document chunks into implicit fact tokens conditioned on the query. These dense representations are projected into the reasoning ",
      "authors": [
        "Wenxuan Xie",
        "Yujia Wang",
        "Xin Tan",
        "Chaochao Lu",
        "Xia Hu",
        "Xuhong Wang"
      ],
      "published": "2026-02-10T17:42:31Z",
      "updated": "2026-02-10T17:42:31Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10021v1",
      "abs_url": "https://arxiv.org/abs/2602.10021v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "prompt",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.09832v1",
      "title": "LLM Reasoning Predicts When Models Are Right: Evidence from Coding Classroom Discourse",
      "summary": "Large Language Models (LLMs) are increasingly deployed to automatically label and analyze educational dialogue at scale, yet current pipelines lack reliable ways to detect when models are wrong. We investigate whether reasoning generated by LLMs can be used to predict the correctness of a model's own predictions. We analyze 30,300 teacher utterances from classroom dialogue, each labeled by multiple state-of-the-art LLMs with an instructional move construct and an accompanying reasoning. Using human-verified ground-truth labels, we frame the task as predicting whether a model's assigned label for a given utterance is correct. We encode LLM reasoning using Term Frequency-Inverse Document Frequency (TF-IDF) and evaluate five supervised classifiers. A Random Forest classifier achieves an F1 sc",
      "authors": [
        "Bakhtawar Ahtisham",
        "Kirk Vanacore",
        "Zhuqian Zhou",
        "Jinsook Lee",
        "Rene F. Kizilcec"
      ],
      "published": "2026-02-10T14:38:13Z",
      "updated": "2026-02-10T14:38:13Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09832v1",
      "abs_url": "https://arxiv.org/abs/2602.09832v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "reasoning",
        "reasoning"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.09501v1",
      "title": "Where-to-Unmask: Ground-Truth-Guided Unmasking Order Learning for Masked Diffusion Language Models",
      "summary": "Masked Diffusion Language Models (MDLMs) generate text by iteratively filling masked tokens, requiring two coupled decisions at each step: which positions to unmask (where-to-unmask) and which tokens to place (what-to-unmask). While standard MDLM training directly optimizes token prediction (what-to-unmask), inference-time unmasking orders (where-to-unmask) are typically determined by heuristic confidence measures or trained through reinforcement learning with costly on-policy rollouts. To address this, we introduce Gt-Margin, a position-wise score derived from ground-truth tokens, defined as the probability margin between the correct token and its strongest alternative. Gt-Margin yields an oracle unmasking order that prioritizes easier positions first under each partially masked state. We",
      "authors": [
        "Hikaru Asano",
        "Tadashi Kozuno",
        "Kuniaki Saito",
        "Yukino Baba"
      ],
      "published": "2026-02-10T07:56:46Z",
      "updated": "2026-02-10T07:56:46Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09501v1",
      "abs_url": "https://arxiv.org/abs/2602.09501v1",
      "relevance_score": 9,
      "matched_keywords": [
        "language model",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.09416v1",
      "title": "Are Language Models Sensitive to Morally Irrelevant Distractors?",
      "summary": "With the rapid development and uptake of large language models (LLMs) across high-stakes settings, it is increasingly important to ensure that LLMs behave in ways that align with human values. Existing moral benchmarks prompt LLMs with value statements, moral scenarios, or psychological questionnaires, with the implicit underlying assumption that LLMs report somewhat stable moral preferences. However, moral psychology research has shown that human moral judgements are sensitive to morally irrelevant situational factors, such as smelling cinnamon rolls or the level of ambient noise, thereby challenging moral theories that assume the stability of human moral judgements. Here, we draw inspiration from this \"situationist\" view of moral psychology to evaluate whether LLMs exhibit similar cognit",
      "authors": [
        "Andrew Shaw",
        "Christina Hahn",
        "Catherine Rasgaitis",
        "Yash Mishra",
        "Alisa Liu",
        "Natasha Jaques",
        "Yulia Tsvetkov",
        "Amy X. Zhang"
      ],
      "published": "2026-02-10T05:18:05Z",
      "updated": "2026-02-10T05:18:05Z",
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09416v1",
      "abs_url": "https://arxiv.org/abs/2602.09416v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "prompt",
        "benchmark",
        "evaluation",
        "multimodal"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.10095v1",
      "title": "Causality in Video Diffusers is Separable from Denoising",
      "summary": "Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and prim",
      "authors": [
        "Xingjian Bai",
        "Guande He",
        "Zhengqi Li",
        "Eli Shechtman",
        "Xun Huang",
        "Zongze Wu"
      ],
      "published": "2026-02-10T18:57:21Z",
      "updated": "2026-02-10T18:57:21Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10095v1",
      "abs_url": "https://arxiv.org/abs/2602.10095v1",
      "relevance_score": 8,
      "matched_keywords": [
        "transformer",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.10014v1",
      "title": "A Task-Centric Theory for Iterative Self-Improvement with Easy-to-Hard Curricula",
      "summary": "Iterative self-improvement fine-tunes an autoregressive large language model (LLM) on reward-verified outputs generated by the LLM itself. In contrast to the empirical success of self-improvement, the theoretical foundation of this generative, iterative procedure in a practical, finite-sample setting remains limited. We make progress toward this goal by modeling each round of self-improvement as maximum-likelihood fine-tuning on a reward-filtered distribution and deriving finite-sample guarantees for the expected reward. Our analysis reveals an explicit feedback loop where better models accept more data per iteration, supporting sustained self-improvement while explaining eventual saturation of such improvement. Adopting a task-centric view by considering reasoning tasks with multiple diff",
      "authors": [
        "Chenruo Liu",
        "Yijun Dong",
        "Yiqiu Shen",
        "Qi Lei"
      ],
      "published": "2026-02-10T17:36:41Z",
      "updated": "2026-02-10T17:36:41Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10014v1",
      "abs_url": "https://arxiv.org/abs/2602.10014v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "fine-tuning",
        "reasoning",
        "reasoning"
      ],
      "category": "reasoning"
    },
    {
      "id": "2602.10006v1",
      "title": "Answer First, Reason Later: Aligning Search Relevance via Mode-Balanced Reinforcement Learning",
      "summary": "Building a search relevance model that achieves both low latency and high performance is a long-standing challenge in the search industry. To satisfy the millisecond-level response requirements of online systems while retaining the interpretable reasoning traces of Large Language Models (LLMs), we propose a novel \\textbf{Answer-First, Reason Later (AFRL)} paradigm. This paradigm requires the model to output the definitive relevance score in the very first token, followed by a structured logical explanation. Inspired by the success of reasoning models, we adopt a \"Supervised Fine-Tuning (SFT) + Reinforcement Learning (RL)\" pipeline to achieve AFRL. However, directly applying existing RL training often leads to \\textbf{mode collapse} in the search relevance task, where the model forgets comp",
      "authors": [
        "Shijie Zhang",
        "Xiang Guo",
        "Rujun Guo",
        "Shaoyu Liu",
        "Xiaozhao Wang",
        "Guanjun Jiang",
        "Kevin Zhang"
      ],
      "published": "2026-02-10T17:28:12Z",
      "updated": "2026-02-10T17:28:12Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10006v1",
      "abs_url": "https://arxiv.org/abs/2602.10006v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "fine-tuning",
        "reasoning",
        "reasoning"
      ],
      "category": "reasoning"
    },
    {
      "id": "2602.09953v1",
      "title": "ATTNPO: Attention-Guided Process Supervision for Efficient Reasoning",
      "summary": "Large reasoning models trained with reinforcement learning and verifiable rewards (RLVR) achieve strong performance on complex reasoning tasks, yet often overthink, generating redundant reasoning without performance gains. Existing trajectory-level length penalties often fail to effectively shorten reasoning length and degrade accuracy, as they uniformly treat all reasoning steps and lack fine-grained signals to distinguish redundancy from necessity. Meanwhile, process-supervised methods are typically resource-intensive and suffer from inaccurate credit assignment. To address these issues, we propose ATTNPO, a low-overhead process-supervised RL framework that leverages the model's intrinsic attention signals for step-level credit assignment. We first identify a set of special attention hea",
      "authors": [
        "Shuaiyi Nie",
        "Siyu Ding",
        "Wenyuan Zhang",
        "Linhao Yu",
        "Tianmeng Yang",
        "Yao Chen",
        "Tingwen Liu",
        "Weichong Yin",
        "Yu Sun",
        "Hua Wu"
      ],
      "published": "2026-02-10T16:40:22Z",
      "updated": "2026-02-10T16:40:22Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09953v1",
      "abs_url": "https://arxiv.org/abs/2602.09953v1",
      "relevance_score": 8,
      "matched_keywords": [
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.09924v1",
      "title": "LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations",
      "summary": "Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these pr",
      "authors": [
        "William Lugoloobi",
        "Thomas Foster",
        "William Bankes",
        "Chris Russell"
      ],
      "published": "2026-02-10T15:57:00Z",
      "updated": "2026-02-10T15:57:00Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09924v1",
      "abs_url": "https://arxiv.org/abs/2602.09924v1",
      "relevance_score": 8,
      "matched_keywords": [
        "llm",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.09877v1",
      "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies",
      "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety",
      "authors": [
        "Chenxu Wang",
        "Chaozhuo Li",
        "Songyang Liu",
        "Zejian Chen",
        "Jinyu Hou",
        "Ji Qi",
        "Rui Li",
        "Litian Zhang",
        "Qiwei Ye",
        "Zheng Liu",
        "Xu Chen",
        "Xi Zhang",
        "Philip S. Yu"
      ],
      "published": "2026-02-10T15:18:19Z",
      "updated": "2026-02-10T15:18:19Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09877v1",
      "abs_url": "https://arxiv.org/abs/2602.09877v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "safety",
        "alignment"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09851v1",
      "title": "CoFEH: LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization",
      "summary": "Feature Engineering (FE) is pivotal in automated machine learning (AutoML) but remains a bottleneck for traditional methods, which treat it as a black-box search, operating within rigid, predefined search spaces and lacking domain awareness. While Large Language Models (LLMs) offer a promising alternative by leveraging semantic reasoning to generate unbounded operators, existing methods fail to construct free-form FE pipelines, remaining confined to isolated subtasks such as feature generation. Most importantly, they are rarely optimized jointly with hyperparameter optimization (HPO) of the ML model, leading to greedy \"FE-then-HPO\" workflows that cannot capture strong FE-HPO interactions. In this paper, we present CoFEH, a collaborative framework that interleaves LLM-based FE and Bayesian ",
      "authors": [
        "Beicheng Xu",
        "Keyao Ding",
        "Wei Liu",
        "Yupeng Lu",
        "Bin Cui"
      ],
      "published": "2026-02-10T14:54:17Z",
      "updated": "2026-02-10T14:54:17Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09851v1",
      "abs_url": "https://arxiv.org/abs/2602.09851v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "reasoning"
    },
    {
      "id": "2602.09823v1",
      "title": "Covo-Audio Technical Report",
      "summary": "In this work, we present Covo-Audio, a 7B-parameter end-to-end LALM that directly processes continuous audio inputs and generates audio outputs within a single unified architecture. Through large-scale curated pretraining and targeted post-training, Covo-Audio achieves state-of-the-art or competitive performance among models of comparable scale across a broad spectrum of tasks, including speech-text modeling, spoken dialogue, speech understanding, audio understanding, and full-duplex voice interaction. Extensive evaluations demonstrate that the pretrained foundation model exhibits strong speech-text comprehension and semantic reasoning capabilities on multiple benchmarks, outperforming representative open-source models of comparable scale. Furthermore, Covo-Audio-Chat, the dialogue-oriente",
      "authors": [
        "Wenfu Wang",
        "Chenxing Li",
        "Liqiang Zhang",
        "Yiyang Zhao",
        "Yuxiang Zou",
        "Hanzhao Li",
        "Mingyu Cui",
        "Hao Zhang",
        "Kun Wei",
        "Le Xu",
        "Zikang Huang",
        "Jiajun Xu",
        "Jiliang Hu",
        "Xiang He",
        "Zeyu Xie",
        "Jiawen Kang",
        "Youjun Chen",
        "Meng Yu",
        "Dong Yu",
        "Rilin Chen",
        "Linlin Di",
        "Shulin Feng",
        "Na Hu",
        "Yang Liu",
        "Bang Wang",
        "Shan Yang"
      ],
      "published": "2026-02-10T14:31:11Z",
      "updated": "2026-02-10T14:31:11Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09823v1",
      "abs_url": "https://arxiv.org/abs/2602.09823v1",
      "relevance_score": 8,
      "matched_keywords": [
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.09514v1",
      "title": "EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies",
      "summary": "Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics. We introduce EcoGym, a generalizable benchmark for continuous plan-and-execute decision making in interactive economies. EcoGym comprises three diverse environments: Vending, Freelance, and Operation, implemented in a unified decision-making process with standardized interfaces, and budgeted actions over an effectively unbounded horizon (1000+ steps if 365 day-loops for evaluation). The evaluation of EcoGym is based on business-relevant outcomes (e.g., net worth, income, and DAU), targeting long-term strategic coherence and robustness under parti",
      "authors": [
        "Xavier Hu",
        "Jinxiang Xia",
        "Shengze Xu",
        "Kangqi Song",
        "Yishuo Yuan",
        "Guibin Zhang",
        "Jincheng Ren",
        "Boyu Feng",
        "Li Lu",
        "Tieyong Zeng",
        "Jiaheng Liu",
        "Minghao Liu",
        "Yuchen Elenor Jiang",
        "Wei Wang",
        "He Zhu",
        "Wangchunshu Zhou"
      ],
      "published": "2026-02-10T08:12:23Z",
      "updated": "2026-02-10T08:12:23Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09514v1",
      "abs_url": "https://arxiv.org/abs/2602.09514v1",
      "relevance_score": 8,
      "matched_keywords": [
        "llm",
        "agent",
        "benchmark",
        "evaluation"
      ],
      "category": "agents"
    },
    {
      "id": "2602.09961v1",
      "title": "ViMultiChoice: Toward a Method That Gives Explanation for Multiple-Choice Reading Comprehension in Vietnamese",
      "summary": "Multiple-choice Reading Comprehension (MCRC) models aim to select the correct answer from a set of candidate options for a given question. However, they typically lack the ability to explain the reasoning behind their choices. In this paper, we introduce a novel Vietnamese dataset designed to train and evaluate MCRC models with explanation generation capabilities. Furthermore, we propose ViMultiChoice, a new method specifically designed for modeling Vietnamese reading comprehension that jointly predicts the correct answer and generates a corresponding explanation. Experimental results demonstrate that ViMultiChoice outperforms existing MCRC baselines, achieving state-of-the-art (SotA) performance on both the ViMMRC 2.0 benchmark and the newly introduced dataset. Additionally, we show that ",
      "authors": [
        "Trung Tien Cao",
        "Lam Minh Thai",
        "Nghia Hieu Nguyen",
        "Duc-Vu Nguyen",
        "Ngan Luu-Thuy Nguyen"
      ],
      "published": "2026-02-10T16:48:07Z",
      "updated": "2026-02-10T16:48:07Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09961v1",
      "abs_url": "https://arxiv.org/abs/2602.09961v1",
      "relevance_score": 7,
      "matched_keywords": [
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.09914v1",
      "title": "AmharicIR+Instr: A Two-Dataset Resource for Neural Retrieval and Instruction Tuning",
      "summary": "Neural retrieval and GPT-style generative models rely on large, high-quality supervised data, which is still scarce for low-resource languages such as Amharic. We release an Amharic data resource consisting of two datasets that supports research on (i) neural retrieval-ranking and (ii) instruction-following text generation. The retrieval-ranking dataset contains 1,091 manually verified query-positive-negative document triplets drawn from diverse Amharic sources and constructed to support contrastive training and benchmarking of neural retrievers (e.g., DPR, ColBERT-style late interaction and SPLADE-style sparse neural retrieval). Triplets are created through a combination of expert-curated queries, web-derived queries, and LLM-assisted generation, with positive/negative documents selected ",
      "authors": [
        "Tilahun Yeshambel",
        "Moncef Garouani",
        "Josiane Mothe"
      ],
      "published": "2026-02-10T15:45:20Z",
      "updated": "2026-02-10T15:45:20Z",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09914v1",
      "abs_url": "https://arxiv.org/abs/2602.09914v1",
      "relevance_score": 7,
      "matched_keywords": [
        "llm",
        "gpt",
        "instruction tuning",
        "prompt",
        "benchmark"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.09907v1",
      "title": "Self-Regulated Reading with AI Support: An Eight-Week Study with Students",
      "summary": "College students increasingly use AI chatbots to support academic reading, yet we lack granular understanding of how these interactions shape their reading experience and cognitive engagement. We conducted an eight-week longitudinal study with 15 undergraduates who used AI to support assigned readings in a course. We collected 838 prompts across 239 reading sessions and developed a coding schema categorizing prompts into four cognitive themes: Decoding, Comprehension, Reasoning, and Metacognition. Comprehension prompts dominated (59.6%), with Reasoning (29.8%), Metacognition (8.5%), and Decoding (2.1%) less frequent. Most sessions (72%) contained exactly three prompts, the required minimum of the reading assignment. Within sessions, students showed natural cognitive progression from compre",
      "authors": [
        "Yue Fu",
        "Joel Wester",
        "Niels Van Berkel",
        "Alexis Hiniker"
      ],
      "published": "2026-02-10T15:41:15Z",
      "updated": "2026-02-10T15:41:15Z",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09907v1",
      "abs_url": "https://arxiv.org/abs/2602.09907v1",
      "relevance_score": 7,
      "matched_keywords": [
        "prompt",
        "reasoning",
        "reasoning"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.09902v1",
      "title": "Routing, Cascades, and User Choice for LLMs",
      "summary": "To mitigate the trade-offs between performance and costs, LLM providers route user tasks to different models based on task difficulty and latency. We study the effect of LLM routing with respect to user behavior. We propose a game between an LLM provider with two models (standard and reasoning) and a user who can re-prompt or abandon tasks if the routed model cannot solve them. The user's goal is to maximize their utility minus the delay from using the model, while the provider minimizes the cost of servicing the user. We solve this Stackelberg game by fully characterizing the user best response and simplifying the provider problem. We observe that in nearly all cases, the optimal routing policy involves a static policy with no cascading that depends on the expected utility of the models t",
      "authors": [
        "Rafid Mahmood"
      ],
      "published": "2026-02-10T15:39:31Z",
      "updated": "2026-02-10T15:39:31Z",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09902v1",
      "abs_url": "https://arxiv.org/abs/2602.09902v1",
      "relevance_score": 7,
      "matched_keywords": [
        "llm",
        "prompt",
        "reasoning",
        "reasoning",
        "alignment"
      ],
      "category": "safety"
    },
    {
      "id": "2602.09821v1",
      "title": "Text summarization via global structure awareness",
      "summary": "Text summarization is a fundamental task in natural language processing (NLP), and the information explosion has made long-document processing increasingly demanding, making summarization essential. Existing research mainly focuses on model improvements and sentence-level pruning, but often overlooks global structure, leading to disrupted coherence and weakened downstream performance. Some studies employ large language models (LLMs), which achieve higher accuracy but incur substantial resource and time costs. To address these issues, we introduce GloSA-sum, the first summarization approach that achieves global structure awareness via topological data analysis (TDA). GloSA-sum summarizes text efficiently while preserving semantic cores and logical dependencies. Specifically, we construct a ",
      "authors": [
        "Jiaquan Zhang",
        "Chaoning Zhang",
        "Shuxu Chen",
        "Yibei Liu",
        "Chenghao Li",
        "Qigan Sun",
        "Shuai Yuan",
        "Fachrina Dewi Puspitasari",
        "Dongshen Han",
        "Guoqing Wang",
        "Sung-Ho Bae",
        "Yang Yang"
      ],
      "published": "2026-02-10T14:29:54Z",
      "updated": "2026-02-10T14:29:54Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09821v1",
      "abs_url": "https://arxiv.org/abs/2602.09821v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    }
  ],
  "recent_papers": [
    {
      "id": "2602.10100v1",
      "title": "Towards Explainable Federated Learning: Understanding the Impact of Differential Privacy",
      "summary": "Data privacy and eXplainable Artificial Intelligence (XAI) are two important aspects for modern Machine Learning systems. To enhance data privacy, recent machine learning models have been designed as a Federated Learning (FL) system. On top of that, additional privacy layers can be added, via Differential Privacy (DP). On the other hand, to improve explainability, ML must consider more interpretable approaches with reduced number of features and less complex internal architecture. In this context, this paper aims to achieve a machine learning (ML) model that combines enhanced data privacy with explainability. So, we propose a FL solution, called Federated EXplainable Trees with Differential Privacy (FEXT-DP), that: (i) is based on Decision Trees, since they are lightweight and have superio",
      "authors": [
        "Jlio Oliveira",
        "Rodrigo Ferreira",
        "Andr Riker",
        "Glaucio H. S. Carvalho",
        "Eirini Eleni Tsilopoulou"
      ],
      "published": "2026-02-10T18:58:11Z",
      "updated": "2026-02-10T18:58:11Z",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10100v1",
      "abs_url": "https://arxiv.org/abs/2602.10100v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.10044v1",
      "title": "Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning",
      "summary": "Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style exploration methods, OWMs incorporate optimism directly into model learning by augmentation with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes. This fully gradient-based loss requires neither uncertainty estimates nor constrained optimization. Our approach is plug-and-play with existing world model frameworks, preserving scalability while requiring only minim",
      "authors": [
        "Akshay Mete",
        "Shahid Aamir Sheikh",
        "Tzu-Hsiang Lin",
        "Dileep Kalathil",
        "P. R. Kumar"
      ],
      "published": "2026-02-10T18:11:00Z",
      "updated": "2026-02-10T18:11:00Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10044v1",
      "abs_url": "https://arxiv.org/abs/2602.10044v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.10031v1",
      "title": "Position: Message-passing and spectral GNNs are two sides of the same coin",
      "summary": "Graph neural networks (GNNs) are commonly divided into message-passing neural networks (MPNNs) and spectral graph neural networks, reflecting two largely separate research traditions in machine learning and signal processing. This paper argues that this divide is mostly artificial, hindering progress in the field. We propose a viewpoint in which both MPNNs and spectral GNNs are understood as different parametrizations of permutation-equivariant operators acting on graph signals. From this perspective, many popular architectures are equivalent in expressive power, while genuine gaps arise only in specific regimes. We further argue that MPNNs and spectral GNNs offer complementary strengths. That is, MPNNs provide a natural language for discrete structure and expressivity analysis using tools",
      "authors": [
        "Antonis Vasileiou",
        "Juan Cervino",
        "Pascal Frossard",
        "Charilaos I. Kanatsoulis",
        "Christopher Morris",
        "Michael T. Schaub",
        "Pierre Vandergheynst",
        "Zhiyang Wang",
        "Guy Wolf",
        "Ron Levie"
      ],
      "published": "2026-02-10T17:53:40Z",
      "updated": "2026-02-10T17:53:40Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10031v1",
      "abs_url": "https://arxiv.org/abs/2602.10031v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.10003v1",
      "title": "ViSpeechFormer: A Phonemic Approach for Vietnamese Automatic Speech Recognition",
      "summary": "Vietnamese has a phonetic orthography, where each grapheme corresponds to at most one phoneme and vice versa. Exploiting this high grapheme-phoneme transparency, we propose ViSpeechFormer (\\textbf{Vi}etnamese \\textbf{Speech} Trans\\textbf{Former}), a phoneme-based approach for Vietnamese Automatic Speech Recognition (ASR). To the best of our knowledge, this is the first Vietnamese ASR framework that explicitly models phonemic representations. Experiments on two publicly available Vietnamese ASR datasets show that ViSpeechFormer achieves strong performance, generalizes better to out-of-vocabulary words, and is less affected by training bias. This phoneme-based paradigm is also promising for other languages with phonetic orthographies. The code will be released upon acceptance of this paper.",
      "authors": [
        "Khoa Anh Nguyen",
        "Long Minh Hoang",
        "Nghia Hieu Nguyen",
        "Luan Thanh Nguyen",
        "Ngan Luu-Thuy Nguyen"
      ],
      "published": "2026-02-10T17:26:55Z",
      "updated": "2026-02-10T17:26:55Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10003v1",
      "abs_url": "https://arxiv.org/abs/2602.10003v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.09987v1",
      "title": "Infusion: Shaping Model Behavior by Editing Training Data via Influence Functions",
      "summary": "Influence functions are commonly used to attribute model behavior to training documents. We explore the reverse: crafting training data that induces model behavior. Our framework, Infusion, uses scalable influence-function approximations to compute small perturbations to training documents that induce targeted changes in model behavior through parameter shifts. We evaluate Infusion on data poisoning tasks across vision and language domains. On CIFAR-10, we show that making subtle edits via Infusion to just 0.2% (100/45,000) of the training documents can be competitive with the baseline of inserting a small number of explicit behavior examples. We also find that Infusion transfers across architectures (ResNet $\\leftrightarrow$ CNN), suggesting a single poisoned corpus can affect multiple in",
      "authors": [
        "J Rosser",
        "Robert Kirk",
        "Edward Grefenstette",
        "Jakob Foerster",
        "Laura Ruis"
      ],
      "published": "2026-02-10T17:13:42Z",
      "updated": "2026-02-10T17:13:42Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09987v1",
      "abs_url": "https://arxiv.org/abs/2602.09987v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.09985v1",
      "title": "Online Monitoring Framework for Automotive Time Series Data using JEPA Embeddings",
      "summary": "As autonomous vehicles are rolled out, measures must be taken to ensure their safe operation. In order to supervise a system that is already in operation, monitoring frameworks are frequently employed. These run continuously online in the background, supervising the system status and recording anomalies. This work proposes an online monitoring framework to detect anomalies in object state representations. Thereby, a key challenge is creating a framework for anomaly detection without anomaly labels, which are usually unavailable for unknown anomalies. To address this issue, this work applies a self-supervised embedding method to translate object data into a latent representation space. For this, a JEPA-based self-supervised prediction task is constructed, allowing training without anomaly l",
      "authors": [
        "Alexander Fertig",
        "Karthikeyan Chandra Sekaran",
        "Lakshman Balasubramanian",
        "Michael Botsch"
      ],
      "published": "2026-02-10T17:10:29Z",
      "updated": "2026-02-10T17:10:29Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09985v1",
      "abs_url": "https://arxiv.org/abs/2602.09985v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.09936v1",
      "title": "The Catastrophic Failure of The k-Means Algorithm in High Dimensions, and How Hartigan's Algorithm Avoids It",
      "summary": "Lloyd's k-means algorithm is one of the most widely used clustering methods. We prove that in high-dimensional, high-noise settings, the algorithm exhibits catastrophic failure: with high probability, essentially every partition of the data is a fixed point. Consequently, Lloyd's algorithm simply returns its initial partition - even when the underlying clusters are trivially recoverable by other methods. In contrast, we prove that Hartigan's k-means algorithm does not exhibit this pathology. Our results show the stark difference between these algorithms and offer a theoretical explanation for the empirical difficulties often observed with k-means in high dimensions.",
      "authors": [
        "Roy R. Lederman",
        "David Silva-Snchez",
        "Ziling Chen",
        "Gilles Mordant",
        "Amnon Balanov",
        "Tamir Bendory"
      ],
      "published": "2026-02-10T16:10:59Z",
      "updated": "2026-02-10T16:10:59Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09936v1",
      "abs_url": "https://arxiv.org/abs/2602.09936v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.09933v1",
      "title": "Unbalanced optimal transport for robust longitudinal lesion evolution with registration-aware and appearance-guided priors",
      "summary": "Evaluating lesion evolution in longitudinal CT scans of can cer patients is essential for assessing treatment response, yet establishing reliable lesion correspondence across time remains challenging. Standard bipartite matchers, which rely on geometric proximity, struggle when lesions appear, disappear, merge, or split. We propose a registration-aware matcher based on unbalanced optimal transport (UOT) that accommodates unequal lesion mass and adapts priors to patient-level tumor-load changes. Our transport cost blends (i) size-normalized geometry, (ii) local registration trust from the deformation-field Jacobian, and (iii) optional patch-level appearance consistency. The resulting transport plan is sparsified by relative pruning, yielding one-to-one matches as well as new, disappearing, ",
      "authors": [
        "Melika Qahqaie",
        "Dominik Neumann",
        "Tobias Heimann",
        "Andreas Maier",
        "Veronika A. Zimmer"
      ],
      "published": "2026-02-10T16:06:57Z",
      "updated": "2026-02-10T16:06:57Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09933v1",
      "abs_url": "https://arxiv.org/abs/2602.09933v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.09918v1",
      "title": "SARS: A Novel Face and Body Shape and Appearance Aware 3D Reconstruction System extends Morphable Models",
      "summary": "Morphable Models (3DMMs) are a type of morphable model that takes 2D images as inputs and recreates the structure and physical appearance of 3D objects, especially human faces and bodies. 3DMM combines identity and expression blendshapes with a basic face mesh to create a detailed 3D model. The variability in the 3D Morphable models can be controlled by tuning diverse parameters. They are high-level image descriptors, such as shape, texture, illumination, and camera parameters. Previous research in 3D human reconstruction concentrated solely on global face structure or geometry, ignoring face semantic features such as age, gender, and facial landmarks characterizing facial boundaries, curves, dips, and wrinkles. In order to accommodate changes in these high-level facial characteristics, th",
      "authors": [
        "Gulraiz Khan",
        "Kenneth Y. Wertheim",
        "Kevin Pimbblet",
        "Waqas Ahmed"
      ],
      "published": "2026-02-10T15:52:30Z",
      "updated": "2026-02-10T15:52:30Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09918v1",
      "abs_url": "https://arxiv.org/abs/2602.09918v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.09864v1",
      "title": "Differentiable Tripartite Modularity for Clustering Heterogeneous Graphs",
      "summary": "Clustering heterogeneous relational data remains a central challenge in graph learning, particularly when interactions involve more than two types of entities. While differentiable modularity objectives such as DMoN have enabled end-to-end community detection on homogeneous and bipartite graphs, extending these approaches to higher-order relational structures remains non-trivial. In this work, we introduce a differentiable formulation of tripartite modularity for graphs composed of three node types connected through mediated interactions. Community structure is defined in terms of weighted co-paths across the tripartite graph, together with an exact factorized computation that avoids the explicit construction of dense third-order tensors. A structural normalization at pivot nodes is introd",
      "authors": [
        "Benot Hurpeau"
      ],
      "published": "2026-02-10T15:06:53Z",
      "updated": "2026-02-10T15:06:53Z",
      "categories": [
        "cs.LG",
        "cs.SI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09864v1",
      "abs_url": "https://arxiv.org/abs/2602.09864v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.09848v1",
      "title": "Robust Processing and Learning: Principles, Methods, and Wireless Applications",
      "summary": "This tutorial-style overview article examines the fundamental principles and methods of robustness, using wireless sensing and communication (WSC) as the narrative and exemplifying framework. First, we formalize the conceptual and mathematical foundations of robustness, highlighting the interpretations and relations across robust statistics, optimization, and machine learning. Key techniques, such as robust estimation and testing, distributionally robust optimization, and regularized and adversary training, are investigated. Together, the costs of robustness in system design, for example, the compromised nominal performances and the extra computational burdens, are discussed. Second, we review recent robust signal processing solutions for WSC that address model mismatch, data scarcity, adv",
      "authors": [
        "Shixiong Wang",
        "Wei Dai",
        "Li-Chun Wang",
        "Geoffrey Ye Li"
      ],
      "published": "2026-02-10T14:53:52Z",
      "updated": "2026-02-10T14:53:52Z",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09848v1",
      "abs_url": "https://arxiv.org/abs/2602.09848v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.09838v1",
      "title": "How Do People Quantify Naturally: Evidence from Mandarin Picture Description",
      "summary": "Quantification is a fundamental component of everyday language use, yet little is known about how speakers decide whether and how to quantify in naturalistic production. We investigate quantification in Mandarin Chinese using a picture-based elicited description task in which speakers freely described scenes containing multiple objects, without explicit instructions to count or quantify. Across both spoken and written modalities, we examine three aspects of quantification: whether speakers choose to quantify at all, how precise their quantification is, and which quantificational strategies they adopt. Results show that object numerosity, animacy, and production modality systematically shape quantificational behaviour. In particular, increasing numerosity reduces both the likelihood and the",
      "authors": [
        "Yayun Zhang",
        "Guanyi Chen",
        "Fahime Same",
        "Saad Mahamood",
        "Tingting He"
      ],
      "published": "2026-02-10T14:45:00Z",
      "updated": "2026-02-10T14:45:00Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09838v1",
      "abs_url": "https://arxiv.org/abs/2602.09838v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.09748v1",
      "title": "Linear Model Extraction via Factual and Counterfactual Queries",
      "summary": "In model extraction attacks, the goal is to reveal the parameters of a black-box machine learning model by querying the model for a selected set of data points. Due to an increasing demand for explanations, this may involve counterfactual queries besides the typically considered factual queries. In this work, we consider linear models and three types of queries: factual, counterfactual, and robust counterfactual. First, for an arbitrary set of queries, we derive novel mathematical formulations for the classification regions for which the decision of the unknown model is known, without recovering any of the model parameters. Second, we derive bounds on the number of queries needed to extract the model's parameters for (robust) counterfactual queries under arbitrary norm-based distances. We ",
      "authors": [
        "Daan Otto",
        "Jannis Kurtz",
        "Dick den Hertog",
        "Ilker Birbil"
      ],
      "published": "2026-02-10T12:57:53Z",
      "updated": "2026-02-10T12:57:53Z",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09748v1",
      "abs_url": "https://arxiv.org/abs/2602.09748v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.09730v1",
      "title": "Allure of Craquelure: A Variational-Generative Approach to Crack Detection in Paintings",
      "summary": "Recent advances in imaging technologies, deep learning and numerical performance have enabled non-invasive detailed analysis of artworks, supporting their documentation and conservation. In particular, automated detection of craquelure in digitized paintings is crucial for assessing degradation and guiding restoration, yet remains challenging due to the possibly complex scenery and the visual similarity between cracks and crack-like artistic features such as brush strokes or hair. We propose a hybrid approach that models crack detection as an inverse problem, decomposing an observed image into a crack-free painting and a crack component. A deep generative model is employed as powerful prior for the underlying artwork, while crack structures are captured using a Mumford--Shah-type variation",
      "authors": [
        "Laura Paul",
        "Holger Rauhut",
        "Martin Burger",
        "Samira Kabri",
        "Tim Roith"
      ],
      "published": "2026-02-10T12:34:53Z",
      "updated": "2026-02-10T12:34:53Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "math.NA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09730v1",
      "abs_url": "https://arxiv.org/abs/2602.09730v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.09718v1",
      "title": "SAQNN: Spectral Adaptive Quantum Neural Network as a Universal Approximator",
      "summary": "Quantum machine learning (QML), as an interdisciplinary field bridging quantum computing and machine learning, has garnered significant attention in recent years. Currently, the field as a whole faces challenges due to incomplete theoretical foundations for the expressivity of quantum neural networks (QNNs). In this paper we propose a constructive QNN model and demonstrate that it possesses the universal approximation property (UAP), which means it can approximate any square-integrable function up to arbitrary accuracy. Furthermore, it supports switching function bases, thus adaptable to various scenarios in numerical approximation and machine learning. Our model has asymptotic advantages over the best classical feed-forward neural networks in terms of circuit size and achieves optimal par",
      "authors": [
        "Jialiang Tang",
        "Jialin Zhang",
        "Xiaoming Sun"
      ],
      "published": "2026-02-10T12:22:02Z",
      "updated": "2026-02-10T12:22:02Z",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09718v1",
      "abs_url": "https://arxiv.org/abs/2602.09718v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.09708v1",
      "title": "Physics-informed diffusion models in spectral space",
      "summary": "We propose a methodology that combines generative latent diffusion models with physics-informed machine learning to generate solutions of parametric partial differential equations (PDEs) conditioned on partial observations, which includes, in particular, forward and inverse PDE problems. We learn the joint distribution of PDE parameters and solutions via a diffusion process in a latent space of scaled spectral representations, where Gaussian noise corresponds to functions with controlled regularity. This spectral formulation enables significant dimensionality reduction compared to grid-based diffusion models and ensures that the induced process in function space remains within a class of functions for which the PDE operators are well defined. Building on diffusion posterior sampling, we en",
      "authors": [
        "Davide Gallon",
        "Philippe von Wurstemberger",
        "Patrick Cheridito",
        "Arnulf Jentzen"
      ],
      "published": "2026-02-10T12:11:07Z",
      "updated": "2026-02-10T12:11:07Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "math.NA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09708v1",
      "abs_url": "https://arxiv.org/abs/2602.09708v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.09651v1",
      "title": "The Entropic Signature of Class Speciation in Diffusion Models",
      "summary": "Diffusion models do not recover semantic structure uniformly over time. Instead, samples transition from semantic ambiguity to class commitment within a narrow regime. Recent theoretical work attributes this transition to dynamical instabilities along class-separating directions, but practical methods to detect and exploit these windows in trained models are still limited. We show that tracking the class-conditional entropy of a latent semantic variable given the noisy state provides a reliable signature of these transition regimes. By restricting the entropy to semantic partitions, the entropy can furthermore resolve semantic decisions at different levels of abstraction. We analyze this behavior in high-dimensional Gaussian mixture models and show that the entropy rate concentrates on the",
      "authors": [
        "Florian Handke",
        "Dejan Stanevi",
        "Felix Koulischer",
        "Thomas Demeester",
        "Luca Ambrogioni"
      ],
      "published": "2026-02-10T10:56:46Z",
      "updated": "2026-02-10T10:56:46Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09651v1",
      "abs_url": "https://arxiv.org/abs/2602.09651v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.09639v1",
      "title": "Blind denoising diffusion models and the blessings of dimensionality",
      "summary": "We analyze, theoretically and empirically, the performance of generative diffusion models based on \\emph{blind denoisers}, in which the denoiser is not given the noise amplitude in either the training or sampling processes. Assuming that the data distribution has low intrinsic dimensionality, we prove that blind denoising diffusion models (BDDMs), despite not having access to the noise amplitude, \\emph{automatically} track a particular \\emph{implicit} noise schedule along the reverse process. Our analysis shows that BDDMs can accurately sample from the data distribution in polynomially many steps as a function of the intrinsic dimension. Empirical results corroborate these mathematical findings on both synthetic and image data, demonstrating that the noise variance is accurately estimated ",
      "authors": [
        "Zahra Kadkhodaie",
        "Aram-Alexandre Pooladian",
        "Sinho Chewi",
        "Eero Simoncelli"
      ],
      "published": "2026-02-10T10:38:16Z",
      "updated": "2026-02-10T10:38:16Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09639v1",
      "abs_url": "https://arxiv.org/abs/2602.09639v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.09620v1",
      "title": "FLINGO -- Instilling ASP Expressiveness into Linear Integer Constraints",
      "summary": "Constraint Answer Set Programming (CASP) is a hybrid paradigm that enriches Answer Set Programming (ASP) with numerical constraint processing, something required in many real-world applications. The usual specification of constraints in most CASP solvers is closer to the numerical back-end expressiveness and semantics, rather than to standard specification in ASP. In the latter, numerical attributes are represented with predicates and this allows declaring default values, leaving the attribute undefined, making non-deterministic assignments with choice rules or using aggregated values. In CASP, most (if not all) of these features are lost once we switch to a constraint-based representation of those same attributes. In this paper, we present the FLINGO language (and tool) that incorporates ",
      "authors": [
        "Jorge Fandinno",
        "Pedro Cabalar",
        "Philipp Wanko",
        "Torsten Schaub"
      ],
      "published": "2026-02-10T10:08:05Z",
      "updated": "2026-02-10T10:08:05Z",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09620v1",
      "abs_url": "https://arxiv.org/abs/2602.09620v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.09613v1",
      "title": "Tracking Finite-Time Lyapunov Exponents to Robustify Neural ODEs",
      "summary": "We investigate finite-time Lyapunov exponents (FTLEs), a measure for exponential separation of input perturbations, of deep neural networks within the framework of continuous-depth neural ODEs. We demonstrate that FTLEs are powerful organizers for input-output dynamics, allowing for better interpretability and the comparison of distinct model architectures. We establish a direct connection between Lyapunov exponents and adversarial vulnerability, and propose a novel training algorithm that improves robustness by FTLE regularization. The key idea is to suppress exponents far from zero in the early stage of the input dynamics. This approach enhances robustness and reduces computational cost compared to full-interval regularization, as it avoids a full ``double'' backpropagation.",
      "authors": [
        "Tobias Whrer",
        "Christian Kuehn"
      ],
      "published": "2026-02-10T10:04:08Z",
      "updated": "2026-02-10T10:04:08Z",
      "categories": [
        "math.DS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09613v1",
      "abs_url": "https://arxiv.org/abs/2602.09613v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    }
  ]
}