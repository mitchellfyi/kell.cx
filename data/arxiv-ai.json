{
  "scraped_at": "2026-02-23T05:30:46.535Z",
  "categories_searched": [
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "stats": {
    "total_fetched": 200,
    "relevant_count": 50,
    "recent_count": 20,
    "by_category": {
      "agents": 16,
      "code-generation": 16,
      "benchmarks": 17,
      "reasoning": 1
    }
  },
  "relevant_papers": [
    {
      "id": "2602.17902v1",
      "title": "El Agente Gráfico: Structured Execution Graphs for Scientific Agents",
      "summary": "Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gráfico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in a",
      "authors": [
        "Jiaru Bai",
        "Abdulrahman Aldossary",
        "Thomas Swanick",
        "Marcel Müller",
        "Yeonghun Kang",
        "Zijian Zhang",
        "Jin Won Lee",
        "Tsz Wai Ko",
        "Mohammad Ghazi Vakili",
        "Varinia Bernales",
        "Alán Aspuru-Guzik"
      ],
      "published": "2026-02-19T23:47:05Z",
      "updated": "2026-02-19T23:47:05Z",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.SE",
        "physics.chem-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17902v1",
      "abs_url": "https://arxiv.org/abs/2602.17902v1",
      "relevance_score": 17,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "agentic",
        "prompt",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag",
        "safety"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17937v1",
      "title": "Analyzing LLM Instruction Optimization for Tabular Fact Verification",
      "summary": "Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest",
      "authors": [
        "Xiaotang Du",
        "Giwon Hong",
        "Wai-Chung Kwan",
        "Rohit Saxena",
        "Ivan Titov",
        "Pasquale Minervini",
        "Emily Allaway"
      ],
      "published": "2026-02-20T01:56:27Z",
      "updated": "2026-02-20T01:56:27Z",
      "categories": [
        "cs.CL",
        "cs.PL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17937v1",
      "abs_url": "https://arxiv.org/abs/2602.17937v1",
      "relevance_score": 16,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "prompt",
        "benchmark",
        "reasoning",
        "cot",
        "reasoning",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.18429v1",
      "title": "VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning",
      "summary": "Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian",
      "authors": [
        "Harshul Raj Surana",
        "Arijit Maji",
        "Aryan Vats",
        "Akash Ghosh",
        "Sriparna Saha",
        "Amit Sheth"
      ],
      "published": "2026-02-20T18:53:07Z",
      "updated": "2026-02-20T18:53:07Z",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18429v1",
      "abs_url": "https://arxiv.org/abs/2602.18429v1",
      "relevance_score": 15,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "fine-tuning",
        "benchmark",
        "reasoning",
        "cot",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.18346v1",
      "title": "Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System",
      "summary": "In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabl",
      "authors": [
        "Pavithra PM Nair",
        "Preethu Rose Anish"
      ],
      "published": "2026-02-20T16:57:44Z",
      "updated": "2026-02-20T16:57:44Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18346v1",
      "abs_url": "https://arxiv.org/abs/2602.18346v1",
      "relevance_score": 15,
      "matched_keywords": [
        "large language model",
        "language model",
        "gpt",
        "llama",
        "mistral",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.18137v1",
      "title": "Agentic Adversarial QA for Improving Domain-Specific LLMs",
      "summary": "Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overc",
      "authors": [
        "Vincent Grari",
        "Ciprian Tomoiaga",
        "Sylvain Lamprier",
        "Tatsunori Hashimoto",
        "Marcin Detyniecki"
      ],
      "published": "2026-02-20T10:53:09Z",
      "updated": "2026-02-20T10:53:09Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18137v1",
      "abs_url": "https://arxiv.org/abs/2602.18137v1",
      "relevance_score": 14,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "agentic",
        "fine-tuning",
        "evaluation",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17999v1",
      "title": "Aurora: Neuro-Symbolic AI Driven Advising Agent",
      "summary": "Academic advising in higher education is under severe strain, with advisor-to-student ratios commonly exceeding 300:1. These structural bottlenecks limit timely access to guidance, increase the risk of delayed graduation, and contribute to inequities in student support. We introduce Aurora, a modular neuro-symbolic advising agent that unifies retrieval-augmented generation (RAG), symbolic reasoning, and normalized curricular databases to deliver policy-compliant, verifiable recommendations at scale. Aurora integrates three components: (i) a Boyce-Codd Normal Form (BCNF) catalog schema for consistent program rules, (ii) a Prolog engine for prerequisite and credit enforcement, and (iii) an instruction-tuned large language model for natural-language explanations of its recommendations. To ass",
      "authors": [
        "Lorena Amanda Quincoso Lugones",
        "Christopher Kverne",
        "Nityam Sharadkumar Bhimani",
        "Ana Carolina Oliveira",
        "Agoritsa Polyzou",
        "Christine Lisetti",
        "Janki Bhimani"
      ],
      "published": "2026-02-20T05:26:45Z",
      "updated": "2026-02-20T05:26:45Z",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17999v1",
      "abs_url": "https://arxiv.org/abs/2602.17999v1",
      "relevance_score": 14,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "prompt",
        "evaluation",
        "reasoning",
        "reasoning",
        "rag",
        "alignment"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17949v1",
      "title": "CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications",
      "summary": "Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large",
      "authors": [
        "Victoria Blake",
        "Mathew Miller",
        "Jamie Novak",
        "Sze-yuan Ooi",
        "Blanca Gallego"
      ],
      "published": "2026-02-20T03:00:13Z",
      "updated": "2026-02-20T03:00:13Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17949v1",
      "abs_url": "https://arxiv.org/abs/2602.17949v1",
      "relevance_score": 14,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.18095v1",
      "title": "Neurosymbolic Language Reasoning as Satisfiability Modulo Theory",
      "summary": "Natural language understanding requires interleaving textual and logical reasoning, yet large language models often fail to perform such reasoning reliably. Existing neurosymbolic systems combine LLMs with solvers but remain limited to fully formalizable tasks such as math or program synthesis, leaving natural documents with only partial logical structure unaddressed. We introduce Logitext, a neurosymbolic language that represents documents as natural language text constraints (NLTCs), making partial logical structure explicit. We develop an algorithm that integrates LLM-based constraint evaluation with satisfiability modulo theory (SMT) solving, enabling joint textual-logical reasoning. Experiments on a new content moderation benchmark, together with LegalBench and Super-Natural Instructi",
      "authors": [
        "Hyunseok Oh",
        "Sam Stern",
        "Youngki Lee",
        "Matthai Philipose"
      ],
      "published": "2026-02-20T09:35:26Z",
      "updated": "2026-02-20T09:35:26Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18095v1",
      "abs_url": "https://arxiv.org/abs/2602.18095v1",
      "relevance_score": 13,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "program synthesis",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.18232v1",
      "title": "Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning",
      "summary": "Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal",
      "authors": [
        "Lexiang Tang",
        "Weihao Gao",
        "Bingchen Zhao",
        "Lu Ma",
        "Qiao jin",
        "Bang Yang",
        "Yuexian Zou"
      ],
      "published": "2026-02-20T14:13:22Z",
      "updated": "2026-02-20T14:13:22Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18232v1",
      "abs_url": "https://arxiv.org/abs/2602.18232v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17826v1",
      "title": "Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge",
      "summary": "Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro",
      "authors": [
        "Marcelo Labre"
      ],
      "published": "2026-02-19T20:45:16Z",
      "updated": "2026-02-19T20:45:16Z",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17826v1",
      "abs_url": "https://arxiv.org/abs/2602.17826v1",
      "relevance_score": 12,
      "matched_keywords": [
        "language model",
        "prompt",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning",
        "rag",
        "hallucination"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.18154v1",
      "title": "FENCE: A Financial and Multimodal Jailbreak Detection Dataset",
      "summary": "Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector traine",
      "authors": [
        "Mirae Kim",
        "Seonghun Jeong",
        "Youngjun Kwak"
      ],
      "published": "2026-02-20T11:40:41Z",
      "updated": "2026-02-20T11:40:41Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18154v1",
      "abs_url": "https://arxiv.org/abs/2602.18154v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "benchmark",
        "multimodal",
        "vision language"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17871v1",
      "title": "Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models",
      "summary": "Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vis",
      "authors": [
        "Dhruba Ghosh",
        "Yuhui Zhang",
        "Ludwig Schmidt"
      ],
      "published": "2026-02-19T22:07:29Z",
      "updated": "2026-02-19T22:07:29Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17871v1",
      "abs_url": "https://arxiv.org/abs/2602.17871v1",
      "relevance_score": 11,
      "matched_keywords": [
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning",
        "multimodal",
        "alignment"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17831v1",
      "title": "The Token Games: Evaluating Language Model Reasoning with Puzzle Duels",
      "summary": "Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using",
      "authors": [
        "Simon Henniger",
        "Gabriel Poesia"
      ],
      "published": "2026-02-19T20:49:15Z",
      "updated": "2026-02-19T20:49:15Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17831v1",
      "abs_url": "https://arxiv.org/abs/2602.17831v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "language model",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17645v1",
      "title": "Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting",
      "summary": "Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA)",
      "authors": [
        "Xiaohan Zhao",
        "Zhaoyi Li",
        "Yaxin Luo",
        "Jiacheng Cui",
        "Zhiqiang Shen"
      ],
      "published": "2026-02-19T18:54:32Z",
      "updated": "2026-02-19T18:54:32Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17645v1",
      "abs_url": "https://arxiv.org/abs/2602.17645v1",
      "relevance_score": 11,
      "matched_keywords": [
        "language model",
        "gpt",
        "claude",
        "gemini",
        "rag",
        "multimodal",
        "alignment"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.18230v1",
      "title": "[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games",
      "summary": "Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objec",
      "authors": [
        "Jorge Carrasco Pollo",
        "Ioannis Kapetangeorgis",
        "Joshua Rosenthal",
        "John Hua Yao"
      ],
      "published": "2026-02-20T14:11:31Z",
      "updated": "2026-02-20T14:11:31Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18230v1",
      "abs_url": "https://arxiv.org/abs/2602.18230v1",
      "relevance_score": 10,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "benchmark",
        "evaluation"
      ],
      "category": "agents"
    },
    {
      "id": "2602.18109v1",
      "title": "TempoNet: Slack-Quantized Transformer-Guided Reinforcement Scheduler for Adaptive Deadline-Centric Real-Time Dispatchs",
      "summary": "Real-time schedulers must reason about tight deadlines under strict compute budgets. We present TempoNet, a reinforcement learning scheduler that pairs a permutation-invariant Transformer with a deep Q-approximation. An Urgency Tokenizer discretizes temporal slack into learnable embeddings, stabilizing value learning and capturing deadline proximity. A latency-aware sparse attention stack with blockwise top-k selection and locality-sensitive chunking enables global reasoning over unordered task sets with near-linear scaling and sub-millisecond inference. A multicore mapping layer converts contextualized Q-scores into processor assignments through masked-greedy selection or differentiable matching. Extensive evaluations on industrial mixed-criticality traces and large multiprocessor setting",
      "authors": [
        "Rong Fu",
        "Yibo Meng",
        "Guangzhen Yao",
        "Jiaxuan Lu",
        "Zeyu Zhang",
        "Zhaolu Kang",
        "Ziming Guo",
        "Jia Yee Tan",
        "Xiaojing Du",
        "Simon James Fong"
      ],
      "published": "2026-02-20T09:56:23Z",
      "updated": "2026-02-20T09:56:23Z",
      "categories": [
        "cs.LG",
        "cs.OS",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18109v1",
      "abs_url": "https://arxiv.org/abs/2602.18109v1",
      "relevance_score": 10,
      "matched_keywords": [
        "llm",
        "transformer",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17993v1",
      "title": "Turbo Connection: Reasoning as Information Flow from Higher to Lower Layers",
      "summary": "Complex problems, whether in math, logic, or planning, are solved by humans through a sequence of steps where the result of one step informs the next. In this work, we adopt the perspective that the reasoning power of Transformers is fundamentally limited by a fixed maximum number of steps along any latent path of computation. To address this, we introduce Turbo Connection (TurboConn), a novel architecture that overcomes the fixed-depth constraint by routing multiple residual connections from the higher-layer hidden states of each token $t$ to the lower layers of token $t+1$. Fine-tuning pre-trained LLMs with our method not only yields accuracy gains of 0.9% to over 10% on benchmarks like GSM8K, Parity, and multi-step arithmetic, but also demonstrates that the density of these backward con",
      "authors": [
        "Mohan Tang",
        "Sidi Lu"
      ],
      "published": "2026-02-20T05:01:32Z",
      "updated": "2026-02-20T05:01:32Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17993v1",
      "abs_url": "https://arxiv.org/abs/2602.17993v1",
      "relevance_score": 10,
      "matched_keywords": [
        "llm",
        "transformer",
        "fine-tuning",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17623v1",
      "title": "Unmasking the Factual-Conceptual Gap in Persian Language Models",
      "summary": "While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to d",
      "authors": [
        "Alireza Sakhaeirad",
        "Ali Ma'manpoosh",
        "Arshia Hemmat"
      ],
      "published": "2026-02-19T18:42:46Z",
      "updated": "2026-02-19T18:42:46Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17623v1",
      "abs_url": "https://arxiv.org/abs/2602.17623v1",
      "relevance_score": 10,
      "matched_keywords": [
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17616v1",
      "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
      "summary": "Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\\textbf{V}$ariance $\\te",
      "authors": [
        "Luke Huang",
        "Zhuoyang Zhang",
        "Qinghao Hu",
        "Shang Yang",
        "Song Han"
      ],
      "published": "2026-02-19T18:40:51Z",
      "updated": "2026-02-19T18:40:51Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17616v1",
      "abs_url": "https://arxiv.org/abs/2602.17616v1",
      "relevance_score": 10,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.18297v1",
      "title": "Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory",
      "summary": "Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved thr",
      "authors": [
        "Usman Anwar",
        "Tim Bakker",
        "Dana Kianfar",
        "Cristina Pinneri",
        "Christos Louizos"
      ],
      "published": "2026-02-20T15:50:30Z",
      "updated": "2026-02-20T15:50:30Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18297v1",
      "abs_url": "https://arxiv.org/abs/2602.18297v1",
      "relevance_score": 9,
      "matched_keywords": [
        "llm",
        "code generation",
        "reasoning",
        "cot",
        "reasoning"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17931v1",
      "title": "Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning",
      "summary": "In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without ",
      "authors": [
        "Narjes Nourzad",
        "Carlee Joe-Wong"
      ],
      "published": "2026-02-20T01:44:35Z",
      "updated": "2026-02-20T01:44:35Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17931v1",
      "abs_url": "https://arxiv.org/abs/2602.17931v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "benchmark"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17837v1",
      "title": "TFL: Targeted Bit-Flip Attack on Large Language Model",
      "summary": "Large language models (LLMs) are increasingly deployed in safety and security critical applications, raising concerns about their robustness to model parameter fault injection attacks. Recent studies have shown that bit-flip attacks (BFAs), which exploit computer main memory (i.e., DRAM) vulnerabilities to flip a small number of bits in model weights, can severely disrupt LLM behavior. However, existing BFA on LLM largely induce un-targeted failure or general performance degradation, offering limited control over manipulating specific or targeted outputs. In this paper, we present TFL, a novel targeted bit-flip attack framework that enables precise manipulation of LLM outputs for selected prompts while maintaining almost no or minor degradation on unrelated inputs. Within our TFL framework",
      "authors": [
        "Jingkai Guo",
        "Chaitali Chakrabarti",
        "Deliang Fan"
      ],
      "published": "2026-02-19T20:59:47Z",
      "updated": "2026-02-19T20:59:47Z",
      "categories": [
        "cs.CR",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17837v1",
      "abs_url": "https://arxiv.org/abs/2602.17837v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "llama",
        "prompt",
        "benchmark",
        "safety"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17809v1",
      "title": "Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning",
      "summary": "Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without re",
      "authors": [
        "Ibne Farabi Shihab",
        "Sanjeda Akter",
        "Anuj Sharma"
      ],
      "published": "2026-02-19T20:17:54Z",
      "updated": "2026-02-19T20:17:54Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17809v1",
      "abs_url": "https://arxiv.org/abs/2602.17809v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "language model",
        "llama",
        "mistral",
        "fine-tuning",
        "benchmark",
        "evaluation"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17641v1",
      "title": "FAMOSE: A ReAct Approach to Automated Feature Discovery",
      "summary": "Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on ",
      "authors": [
        "Keith Burghardt",
        "Jienan Liu",
        "Sadman Sakib",
        "Yuning Hao",
        "Bo Li"
      ],
      "published": "2026-02-19T18:53:15Z",
      "updated": "2026-02-19T18:53:15Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17641v1",
      "abs_url": "https://arxiv.org/abs/2602.17641v1",
      "relevance_score": 9,
      "matched_keywords": [
        "llm",
        "agent",
        "agentic",
        "prompt",
        "few-shot",
        "evaluation",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.18420v1",
      "title": "SPQ: An Ensemble Technique for Large Language Model Compression",
      "summary": "This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.",
      "authors": [
        "Jiamin Yao",
        "Eren Gultepe"
      ],
      "published": "2026-02-20T18:44:16Z",
      "updated": "2026-02-20T18:44:16Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18420v1",
      "abs_url": "https://arxiv.org/abs/2602.18420v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "llama",
        "benchmark"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.18176v1",
      "title": "Improving Sampling for Masked Diffusion Models via Information Gain",
      "summary": "Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding ",
      "authors": [
        "Kaisen Yang",
        "Jayden Teoh",
        "Kaicheng Yang",
        "Yitong Zhang",
        "Alex Lamb"
      ],
      "published": "2026-02-20T12:26:03Z",
      "updated": "2026-02-20T12:26:03Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18176v1",
      "abs_url": "https://arxiv.org/abs/2602.18176v1",
      "relevance_score": 8,
      "matched_keywords": [
        "evaluation",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.18145v1",
      "title": "Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention",
      "summary": "Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable ground",
      "authors": [
        "Siya Qi",
        "Yudong Chen",
        "Runcong Zhao",
        "Qinglin Zhu",
        "Zhanghao Hu",
        "Wei Liu",
        "Yulan He",
        "Zheng Yuan",
        "Lin Gui"
      ],
      "published": "2026-02-20T11:18:45Z",
      "updated": "2026-02-20T11:18:45Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18145v1",
      "abs_url": "https://arxiv.org/abs/2602.18145v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "rag",
        "hallucination"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.18141v1",
      "title": "Advection-Diffusion on Graphs: A Bakry-Emery Laplacian for Spectral Graph Neural Networks",
      "summary": "Graph Neural Networks (GNNs) often struggle to propagate information across long distances due to oversmoothing and oversquashing. Existing remedies such as graph transformers or rewiring typically incur high computational cost or require altering the graph structure. We introduce a Bakry-Emery graph Laplacian that integrates diffusion and advection through a learnable node-wise potential, inducing task-dependent propagation dynamics without modifying topology. This operator has a well-behaved spectral decomposition and acts as a drop-in replacement for standard Laplacians in spectral GNNs. Building on this insight, we develop mu-ChebNet, a spectral architecture that jointly learns the potential and Chebyshev filters, effectively bridging message-passing adaptivity and spectral efficiency.",
      "authors": [
        "Pierre-Gabriel Berlureau",
        "Ali Hariri",
        "Victor Kawasaki-Borruat",
        "Mia Zosso",
        "Pierre Vandergheynst"
      ],
      "published": "2026-02-20T11:01:12Z",
      "updated": "2026-02-20T11:01:12Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18141v1",
      "abs_url": "https://arxiv.org/abs/2602.18141v1",
      "relevance_score": 8,
      "matched_keywords": [
        "transformer",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.18029v1",
      "title": "Towards More Standardized AI Evaluation: From Models to Agents",
      "summary": "Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer \"How good is the model?\" but \"Can we trust the system to behave as intended, under change, at scale?\". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance meas",
      "authors": [
        "Ali El Filali",
        "Inès Bedar"
      ],
      "published": "2026-02-20T06:54:44Z",
      "updated": "2026-02-20T06:54:44Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18029v1",
      "abs_url": "https://arxiv.org/abs/2602.18029v1",
      "relevance_score": 8,
      "matched_keywords": [
        "agent",
        "agentic",
        "benchmark",
        "evaluation"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17990v1",
      "title": "WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics",
      "summary": "LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and re",
      "authors": [
        "Madhav Kanda",
        "Pedro Las-Casas",
        "Alok Gautam Kumbhare",
        "Rodrigo Fonseca",
        "Sharad Agarwal"
      ],
      "published": "2026-02-20T04:54:31Z",
      "updated": "2026-02-20T04:54:31Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17990v1",
      "abs_url": "https://arxiv.org/abs/2602.17990v1",
      "relevance_score": 8,
      "matched_keywords": [
        "llm",
        "agent",
        "benchmark",
        "evaluation"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17875v1",
      "title": "MultiVer: Zero-Shot Multi-Agent Vulnerability Detection",
      "summary": "We present MultiVer, a zero-shot multi-agent system for vulnerability detection that achieves state-of-the-art recall without fine-tuning. A four-agent ensemble (security, correctness, performance, style) with union voting achieves 82.7% recall on PyVul, exceeding fine-tuned GPT-3.5 (81.3%) by 1.4 percentage points -- the first zeroshot system to surpass fine-tuned performance on this benchmark. On SecurityEval, the same architecture achieves 91.7% detection rate, matching specialized systems. The recall improvement comes at a precision cost: 48.8% precision versus 63.9% for fine-tuned baselines, yielding 61.4% F1. Ablation experiments isolate component contributions: the multi-agent ensemble adds 17 percentage points recall over single-agent security analysis. These results demonstrate th",
      "authors": [
        "Shreshth Rajan"
      ],
      "published": "2026-02-19T22:20:17Z",
      "updated": "2026-02-19T22:20:17Z",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17875v1",
      "abs_url": "https://arxiv.org/abs/2602.17875v1",
      "relevance_score": 8,
      "matched_keywords": [
        "gpt",
        "agent",
        "fine-tuning",
        "benchmark"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17608v1",
      "title": "Towards Anytime-Valid Statistical Watermarking",
      "summary": "The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detect",
      "authors": [
        "Baihe Huang",
        "Eric Xu",
        "Kannan Ramchandran",
        "Jiantao Jiao",
        "Michael I. Jordan"
      ],
      "published": "2026-02-19T18:32:26Z",
      "updated": "2026-02-19T18:32:26Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17608v1",
      "abs_url": "https://arxiv.org/abs/2602.17608v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "evaluation",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.18374v1",
      "title": "Zero-shot Interactive Perception",
      "summary": "Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module ",
      "authors": [
        "Venkatesh Sripada",
        "Frank Guerin",
        "Amir Ghalamzan"
      ],
      "published": "2026-02-20T17:30:25Z",
      "updated": "2026-02-20T17:30:25Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18374v1",
      "abs_url": "https://arxiv.org/abs/2602.18374v1",
      "relevance_score": 7,
      "matched_keywords": [
        "language model",
        "prompt",
        "reasoning",
        "reasoning",
        "vision language"
      ],
      "category": "reasoning"
    },
    {
      "id": "2602.18351v1",
      "title": "Validating Political Position Predictions of Arguments",
      "summary": "Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \\textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $α=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substanti",
      "authors": [
        "Jordan Robinson",
        "Angus R. Williams",
        "Katie Atkinson",
        "Anthony G. Cohn"
      ],
      "published": "2026-02-20T17:03:44Z",
      "updated": "2026-02-20T17:03:44Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18351v1",
      "abs_url": "https://arxiv.org/abs/2602.18351v1",
      "relevance_score": 7,
      "matched_keywords": [
        "language model",
        "evaluation",
        "reasoning",
        "reasoning",
        "alignment"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.18307v1",
      "title": "VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean",
      "summary": "Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repos",
      "authors": [
        "Yutong Xin",
        "Qiaochu Chen",
        "Greg Durrett",
        "Işil Dillig"
      ],
      "published": "2026-02-20T16:05:06Z",
      "updated": "2026-02-20T16:05:06Z",
      "categories": [
        "cs.SE",
        "cs.CL",
        "cs.LG",
        "cs.PL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18307v1",
      "abs_url": "https://arxiv.org/abs/2602.18307v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "evaluation"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.18277v1",
      "title": "PRISM: Parallel Reward Integration with Symmetry for MORL",
      "summary": "This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring an",
      "authors": [
        "Finn van der Knaap",
        "Kejiang Qian",
        "Zheng Xu",
        "Fengxiang He"
      ],
      "published": "2026-02-20T15:02:42Z",
      "updated": "2026-02-20T15:02:42Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18277v1",
      "abs_url": "https://arxiv.org/abs/2602.18277v1",
      "relevance_score": 7,
      "matched_keywords": [
        "agent",
        "benchmark",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.18172v1",
      "title": "Can AI Lower the Barrier to Cybersecurity? A Human-Centered Mixed-Methods Study of Novice CTF Learning",
      "summary": "Capture-the-Flag (CTF) competitions serve as gateways into offensive cybersecurity, yet they often present steep barriers for novices due to complex toolchains and opaque workflows. Recently, agentic AI frameworks for cybersecurity promise to lower these barriers by automating and coordinating penetration testing tasks. However, their role in shaping novice learning remains underexplored. We present a human-centered, mixed-methods case study examining how agentic AI frameworks -- here Cybersecurity AI (CAI) -- mediates novice entry into CTF-based penetration testing. An undergraduate student without prior hacking experience attempted to approach performance benchmarks from a national cybersecurity challenge using CAI. Quantitative performance metrics were complemented by structured reflect",
      "authors": [
        "Cathrin Schachner",
        "Jasmin Wachter"
      ],
      "published": "2026-02-20T12:20:36Z",
      "updated": "2026-02-20T12:20:36Z",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18172v1",
      "abs_url": "https://arxiv.org/abs/2602.18172v1",
      "relevance_score": 7,
      "matched_keywords": [
        "agent",
        "agentic",
        "benchmark"
      ],
      "category": "agents"
    },
    {
      "id": "2602.18094v1",
      "title": "OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models",
      "summary": "Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benc",
      "authors": [
        "Ling Lin",
        "Yang Bai",
        "Heng Su",
        "Congcong Zhu",
        "Yaoxing Wang",
        "Yang Zhou",
        "Huazhu Fu",
        "Jingrun Chen"
      ],
      "published": "2026-02-20T09:34:21Z",
      "updated": "2026-02-20T09:34:21Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18094v1",
      "abs_url": "https://arxiv.org/abs/2602.18094v1",
      "relevance_score": 7,
      "matched_keywords": [
        "language model",
        "prompt",
        "benchmark",
        "evaluation",
        "safety"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17978v1",
      "title": "Learning Optimal and Sample-Efficient Decision Policies with Guarantees",
      "summary": "The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of ",
      "authors": [
        "Daqian Shao"
      ],
      "published": "2026-02-20T04:24:49Z",
      "updated": "2026-02-20T04:24:49Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17978v1",
      "abs_url": "https://arxiv.org/abs/2602.17978v1",
      "relevance_score": 7,
      "matched_keywords": [
        "agent",
        "benchmark",
        "evaluation"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17929v1",
      "title": "ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging",
      "summary": "Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term \"Zero-token\" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve traini",
      "authors": [
        "Athanasios Angelakis"
      ],
      "published": "2026-02-20T01:38:59Z",
      "updated": "2026-02-20T01:38:59Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17929v1",
      "abs_url": "https://arxiv.org/abs/2602.17929v1",
      "relevance_score": 7,
      "matched_keywords": [
        "transformer",
        "few-shot",
        "benchmark",
        "evaluation",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17911v1",
      "title": "Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering",
      "summary": "Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or pr",
      "authors": [
        "Jash Rajesh Parekh",
        "Wonbin Kweon",
        "Joey Chan",
        "Rezarta Islamaj",
        "Robert Leaman",
        "Pengcheng Jiang",
        "Chih-Hsuan Wei",
        "Zhizheng Wang",
        "Zhiyong Lu",
        "Jiawei Han"
      ],
      "published": "2026-02-20T00:17:14Z",
      "updated": "2026-02-20T00:17:14Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17911v1",
      "abs_url": "https://arxiv.org/abs/2602.17911v1",
      "relevance_score": 7,
      "matched_keywords": [
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17910v1",
      "title": "Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems",
      "summary": "Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results refra",
      "authors": [
        "Hanjing Shi",
        "Dominic DiFranzo"
      ],
      "published": "2026-02-20T00:16:07Z",
      "updated": "2026-02-20T00:16:07Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17910v1",
      "abs_url": "https://arxiv.org/abs/2602.17910v1",
      "relevance_score": 7,
      "matched_keywords": [
        "llm",
        "agent",
        "agentic",
        "evaluation",
        "alignment"
      ],
      "category": "agents"
    },
    {
      "id": "2602.17835v1",
      "title": "Influence-Preserving Proxies for Gradient-Based Data Selection in LLM Fine-tuning",
      "summary": "Supervised fine-tuning (SFT) relies critically on selecting training data that most benefits a model's downstream performance. Gradient-based data selection methods such as TracIn and Influence Functions leverage influence to identify useful samples, but their computational cost scales poorly, making them impractical for multi-billion-parameter large language models (LLMs). A common alternative is to use off-the-shelf smaller models as proxies, but they remain suboptimal since their learning dynamics are unclear, their sizes cannot be flexibly adjusted, and they cannot be further aligned with the target model in terms of gradient-based influence estimation. To address these challenges, we introduce Iprox, a two-stage framework that derives influence-preserving proxies directly from the tar",
      "authors": [
        "Sirui Chen",
        "Yunzhe Qi",
        "Mengting Ai",
        "Yifan Sun",
        "Ruizhong Qiu",
        "Jiaru Zou",
        "Jingrui He"
      ],
      "published": "2026-02-19T20:57:30Z",
      "updated": "2026-02-19T20:57:30Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17835v1",
      "abs_url": "https://arxiv.org/abs/2602.17835v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "llama",
        "fine-tuning",
        "evaluation",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17829v1",
      "title": "Causality by Abstraction: Symbolic Rule Learning in Multivariate Timeseries with Large Language Models",
      "summary": "Inferring causal relations in timeseries data with delayed effects is a fundamental challenge, especially when the underlying system exhibits complex dynamics that cannot be captured by simple functional mappings. Traditional approaches often fail to produce generalized and interpretable explanations, as multiple distinct input trajectories may yield nearly indistinguishable outputs. In this work, we present ruleXplain, a framework that leverages Large Language Models (LLMs) to extract formal explanations for input-output relations in simulation-driven dynamical systems. Our method introduces a constrained symbolic rule language with temporal operators and delay semantics, enabling LLMs to generate verifiable causal rules through structured prompting. ruleXplain relies on the availability ",
      "authors": [
        "Preetom Biswas",
        "Giulia Pedrielli",
        "K. Selçuk Candan"
      ],
      "published": "2026-02-19T20:49:06Z",
      "updated": "2026-02-19T20:49:06Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17829v1",
      "abs_url": "https://arxiv.org/abs/2602.17829v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "prompt",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17814v1",
      "title": "VQPP: Video Query Performance Prediction Benchmark",
      "summary": "Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval perform",
      "authors": [
        "Adrian Catalin Lutu",
        "Eduard Poesina",
        "Radu Tudor Ionescu"
      ],
      "published": "2026-02-19T20:32:25Z",
      "updated": "2026-02-19T20:32:25Z",
      "categories": [
        "cs.CV",
        "cs.IR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17814v1",
      "abs_url": "https://arxiv.org/abs/2602.17814v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "dpo",
        "benchmark"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.17778v1",
      "title": "Asking Forever: Universal Activations Behind Turn Amplification in Conversational LLMs",
      "summary": "Multi-turn interaction length is a dominant factor in the operational costs of conversational LLMs. In this work, we present a new failure mode in conversational LLMs: turn amplification, in which a model consistently prolongs multi-turn interactions without completing the underlying task. We show that an adversary can systematically exploit clarification-seeking behavior$-$commonly encouraged in multi-turn conversation settings$-$to scalably prolong interactions. Moving beyond prompt-level behaviors, we take a mechanistic perspective and identify a query-independent, universal activation subspace associated with clarification-seeking responses. Unlike prior cost-amplification attacks that rely on per-turn prompt optimization, our attack arises from conversational dynamics and persists acr",
      "authors": [
        "Zachary Coalson",
        "Bo Fang",
        "Sanghyun Hong"
      ],
      "published": "2026-02-19T19:21:09Z",
      "updated": "2026-02-19T19:21:09Z",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17778v1",
      "abs_url": "https://arxiv.org/abs/2602.17778v1",
      "relevance_score": 7,
      "matched_keywords": [
        "llm",
        "fine-tuning",
        "prompt",
        "benchmark",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17655v1",
      "title": "What Language is This? Ask Your Tokenizer",
      "summary": "Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental ",
      "authors": [
        "Clara Meister",
        "Ahmetcan Yavuz",
        "Pietro Lesci",
        "Tiago Pimentel"
      ],
      "published": "2026-02-19T18:58:39Z",
      "updated": "2026-02-19T18:58:39Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17655v1",
      "abs_url": "https://arxiv.org/abs/2602.17655v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "language model",
        "benchmark",
        "evaluation",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.17594v1",
      "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
      "summary": "Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \\textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans f",
      "authors": [
        "Lance Ying",
        "Ryan Truong",
        "Prafull Sharma",
        "Kaiya Ivy Zhao",
        "Nathan Cloos",
        "Kelsey R. Allen",
        "Thomas L. Griffiths",
        "Katherine M. Collins",
        "José Hernández-Orallo",
        "Phillip Isola",
        "Samuel J. Gershman",
        "Joshua B. Tenenbaum"
      ],
      "published": "2026-02-19T18:17:25Z",
      "updated": "2026-02-19T18:17:25Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17594v1",
      "abs_url": "https://arxiv.org/abs/2602.17594v1",
      "relevance_score": 7,
      "matched_keywords": [
        "llm",
        "language model",
        "benchmark",
        "evaluation",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.18425v1",
      "title": "RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering",
      "summary": "Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall ",
      "authors": [
        "Deniz Qian",
        "Hung-Ting Chen",
        "Eunsol Choi"
      ],
      "published": "2026-02-20T18:48:05Z",
      "updated": "2026-02-20T18:48:05Z",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18425v1",
      "abs_url": "https://arxiv.org/abs/2602.18425v1",
      "relevance_score": 6,
      "matched_keywords": [
        "agent",
        "agentic",
        "fine-tuning",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.18291v1",
      "title": "Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies",
      "summary": "Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \\underline{O}nline off-policy \\underline{MA}RL framework using \\underline{D}iffusion policies (\\textbf{OMAD}) to orchestrate coordination. Our key innovation is a r",
      "authors": [
        "Zhuoran Li",
        "Hai Zhong",
        "Xun Wang",
        "Qingxin Xia",
        "Lihua Zhang",
        "Longbo Huang"
      ],
      "published": "2026-02-20T15:38:02Z",
      "updated": "2026-02-20T15:38:02Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18291v1",
      "abs_url": "https://arxiv.org/abs/2602.18291v1",
      "relevance_score": 6,
      "matched_keywords": [
        "agent",
        "evaluation",
        "rag",
        "multimodal"
      ],
      "category": "agents"
    }
  ],
  "recent_papers": [
    {
      "id": "2602.18435v1",
      "title": "Assigning Confidence: K-partition Ensembles",
      "summary": "Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluat",
      "authors": [
        "Aggelos Semoglou",
        "John Pavlopoulos"
      ],
      "published": "2026-02-20T18:59:53Z",
      "updated": "2026-02-20T18:59:53Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18435v1",
      "abs_url": "https://arxiv.org/abs/2602.18435v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.18428v1",
      "title": "The Geometry of Noise: Why Diffusion Models Don't Need Noise Conditioning",
      "summary": "Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\\text{marg}}(\\mathbf{u}) = -\\log p(\\mathbf{u})$, where $p(\\mathbf{u}) = \\int p(\\mathbf{u}|t)p(t)dt$ is t",
      "authors": [
        "Mojtaba Sahraee-Ardakan",
        "Mauricio Delbracio",
        "Peyman Milanfar"
      ],
      "published": "2026-02-20T18:49:00Z",
      "updated": "2026-02-20T18:49:00Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18428v1",
      "abs_url": "https://arxiv.org/abs/2602.18428v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.18406v1",
      "title": "Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges",
      "summary": "Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and e",
      "authors": [
        "Minh Dinh",
        "Stéphane Deny"
      ],
      "published": "2026-02-20T18:14:05Z",
      "updated": "2026-02-20T18:14:05Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18406v1",
      "abs_url": "https://arxiv.org/abs/2602.18406v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.18403v1",
      "title": "Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study",
      "summary": "Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed",
      "authors": [
        "Orfeas Bourchas",
        "George Papalambrou"
      ],
      "published": "2026-02-20T18:12:14Z",
      "updated": "2026-02-20T18:12:14Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18403v1",
      "abs_url": "https://arxiv.org/abs/2602.18403v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.18386v1",
      "title": "Learning to Tune Pure Pursuit in Autonomous Racing: Joint Lookahead and Steering-Gain Control with PPO",
      "summary": "Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. Standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. We propose a reinforcement-learning (RL) approach that jointly chooses the lookahead Ld and a steering gain g online using Proximal Policy Optimization (PPO). The policy observes compact state features (speed and curvature taps) and outputs (Ld, g) at each control step. Trained in F1TENTH Gym and deployed in a ROS 2 stack, the policy drives PP directly (with light smoothing) and requires no per-map retuning. Across simulation and real-",
      "authors": [
        "Mohamed Elgouhary",
        "Amr S. El-Wakeel"
      ],
      "published": "2026-02-20T17:48:21Z",
      "updated": "2026-02-20T17:48:21Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18386v1",
      "abs_url": "https://arxiv.org/abs/2602.18386v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.18350v1",
      "title": "Quantum-enhanced satellite image classification",
      "summary": "We demonstrate the application of a quantum feature extraction method to enhance multi-class image classification for space applications. By harnessing the dynamics of many-body spin Hamiltonians, the method generates expressive quantum features that, when combined with classical processing, lead to quantum-enhanced classification accuracy. Using a strong and well-established ResNet50 baseline, we achieved a maximum classical accuracy of 83%, which can be improved to 84% with a transfer learning approach. In contrast, applying our quantum-classical method the performance is increased to 87% accuracy, demonstrating a clear and reproducible improvement over robust classical approaches. Implemented on several of IBM's quantum processors, our hybrid quantum-classical approach delivers consiste",
      "authors": [
        "Qi Zhang",
        "Anton Simen",
        "Carlos Flores-Garrigós",
        "Gabriel Alvarado Barrios",
        "Paolo A. Erdman",
        "Enrique Solano",
        "Aaron C. Kemp",
        "Vincent Beltrani",
        "Vedangi Pathak",
        "Hamed Mohammadbagherpoor"
      ],
      "published": "2026-02-20T17:02:16Z",
      "updated": "2026-02-20T17:02:16Z",
      "categories": [
        "quant-ph",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18350v1",
      "abs_url": "https://arxiv.org/abs/2602.18350v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.18326v1",
      "title": "Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning",
      "summary": "We describe a modern deep learning system that automatically identifies informative contextual examples (\\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \\qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance",
      "authors": [
        "Tao Wu",
        "Adam Kapelner"
      ],
      "published": "2026-02-20T16:32:14Z",
      "updated": "2026-02-20T16:32:14Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18326v1",
      "abs_url": "https://arxiv.org/abs/2602.18326v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "training"
    },
    {
      "id": "2602.18313v1",
      "title": "Clapeyron Neural Networks for Single-Species Vapor-Liquid Equilibria",
      "summary": "Machine learning (ML) approaches have shown promising results for predicting molecular properties relevant for chemical process design. However, they are often limited by scarce experimental property data and lack thermodynamic consistency. As such, thermodynamics-informed ML, i.e., incorporating thermodynamic relations into the loss function as regularization term for training, has been proposed. We herein transfer the concept of thermodynamics-informed graph neural networks (GNNs) from the Gibbs-Duhem to the Clapeyron equation, predicting several pure component properties in a multi-task manner, namely: vapor pressure, liquid molar volume, vapor molar volume and enthalpy of vaporization. We find improved prediction accuracy of the Clapeyron-GNN compared to the single-task learning settin",
      "authors": [
        "Jan Pavšek",
        "Alexander Mitsos",
        "Elvis J. Sim",
        "Jan G. Rittig"
      ],
      "published": "2026-02-20T16:11:42Z",
      "updated": "2026-02-20T16:11:42Z",
      "categories": [
        "physics.chem-ph",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18313v1",
      "abs_url": "https://arxiv.org/abs/2602.18313v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.18250v1",
      "title": "Variational Distributional Neuron",
      "summary": "We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This \"contraction\" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constra",
      "authors": [
        "Yves Ruffenach"
      ],
      "published": "2026-02-20T14:35:53Z",
      "updated": "2026-02-20T14:35:53Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18250v1",
      "abs_url": "https://arxiv.org/abs/2602.18250v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.18248v1",
      "title": "Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver",
      "summary": "Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regi",
      "authors": [
        "Pietro Sittoni",
        "Emanuele Zangrando",
        "Angelo A. Casulli",
        "Nicola Guglielmi",
        "Francesco Tudisco"
      ],
      "published": "2026-02-20T14:31:08Z",
      "updated": "2026-02-20T14:31:08Z",
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18248v1",
      "abs_url": "https://arxiv.org/abs/2602.18248v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.18213v1",
      "title": "Machine-learning force-field models for dynamical simulations of metallic magnets",
      "summary": "We review recent advances in machine learning (ML) force-field methods for Landau-Lifshitz-Gilbert (LLG) simulations of itinerant electron magnets, focusing on scalability and transferability. Built on the principle of locality, a deep neural network model is developed to efficiently and accurately predict the electron-mediated forces governing spin dynamics. Symmetry-aware descriptors constructed through a group-theoretical approach ensure rigorous incorporation of both lattice and spin-rotation symmetries. The framework is demonstrated using the prototypical s-d exchange model widely employed in spintronics. ML-enabled large-scale simulations reveal novel nonequilibrium phenomena, including anomalous coarsening of tetrahedral spin order on the triangular lattice and the freezing of phase",
      "authors": [
        "Gia-Wei Chern",
        "Yunhao Fan",
        "Sheng Zhang",
        "Puhan Zhang"
      ],
      "published": "2026-02-20T13:51:29Z",
      "updated": "2026-02-20T13:51:29Z",
      "categories": [
        "cond-mat.str-el",
        "cs.LG",
        "physics.comp-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18213v1",
      "abs_url": "https://arxiv.org/abs/2602.18213v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.18201v1",
      "title": "SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps",
      "summary": "Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.3",
      "authors": [
        "Joseph Bingham",
        "Netanel Arussy",
        "Dvir Aran"
      ],
      "published": "2026-02-20T13:25:28Z",
      "updated": "2026-02-20T13:25:28Z",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18201v1",
      "abs_url": "https://arxiv.org/abs/2602.18201v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.18160v1",
      "title": "Unifying Formal Explanations: A Complexity-Theoretic Perspective",
      "summary": "Previous work has explored the computational complexity of deriving two fundamental types of explanations for ML model predictions: (1) *sufficient reasons*, which are subsets of input features that, when fixed, determine a prediction, and (2) *contrastive reasons*, which are subsets of input features that, when modified, alter a prediction. Prior studies have examined these explanations in different contexts, such as non-probabilistic versus probabilistic frameworks and local versus global settings. In this study, we introduce a unified framework for analyzing these explanations, demonstrating that they can all be characterized through the minimization of a unified probabilistic value function. We then prove that the complexity of these computations is influenced by three key properties o",
      "authors": [
        "Shahaf Bassan",
        "Xuanxiang Huang",
        "Guy Katz"
      ],
      "published": "2026-02-20T11:52:26Z",
      "updated": "2026-02-20T11:52:26Z",
      "categories": [
        "cs.LG",
        "cs.CC",
        "cs.DS",
        "math.OC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18160v1",
      "abs_url": "https://arxiv.org/abs/2602.18160v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.18151v1",
      "title": "Rethinking Beam Management: Generalization Limits Under Hardware Heterogeneity",
      "summary": "Hardware heterogeneity across diverse user devices poses new challenges for beam-based communication in 5G and beyond. This heterogeneity limits the applicability of machine learning (ML)-based algorithms. This article highlights the critical need to treat hardware heterogeneity as a first-class design concern in ML-aided beam management. We analyze key failure modes in the presence of heterogeneity and present case studies demonstrating their performance impact. Finally, we discuss potential strategies to improve generalization in beam management.",
      "authors": [
        "Nikita Zeulin",
        "Olga Galinina",
        "Ibrahim Kilinc",
        "Sergey Andreev",
        "Robert W. Heath"
      ],
      "published": "2026-02-20T11:30:13Z",
      "updated": "2026-02-20T11:30:13Z",
      "categories": [
        "cs.NI",
        "cs.IT",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18151v1",
      "abs_url": "https://arxiv.org/abs/2602.18151v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.18146v1",
      "title": "Stable Long-Horizon Spatiotemporal Prediction on Meshes Using Latent Multiscale Recurrent Graph Neural Networks",
      "summary": "Accurate long-horizon prediction of spatiotemporal fields on complex geometries is a fundamental challenge in scientific machine learning, with applications such as additive manufacturing where temperature histories govern defect formation and mechanical properties. High-fidelity simulations are accurate but computationally costly, and despite recent advances, machine learning methods remain challenged by long-horizon temperature and gradient prediction. We propose a deep learning framework for predicting full temperature histories directly on meshes, conditioned on geometry and process parameters, while maintaining stability over thousands of time steps and generalizing across heterogeneous geometries. The framework adopts a temporal multiscale architecture composed of two coupled models ",
      "authors": [
        "Lionel Salesses",
        "Larbi Arbaoui",
        "Tariq Benamara",
        "Arnaud Francois",
        "Caroline Sainvitu"
      ],
      "published": "2026-02-20T11:22:47Z",
      "updated": "2026-02-20T11:22:47Z",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "physics.app-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18146v1",
      "abs_url": "https://arxiv.org/abs/2602.18146v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.18119v1",
      "title": "RamanSeg: Interpretability-driven Deep Learning on Raman Spectra for Cancer Diagnosis",
      "summary": "Histopathology, the current gold standard for cancer diagnosis, involves the manual examination of tissue samples after chemical staining, a time-consuming process requiring expert analysis. Raman spectroscopy is an alternative, stain-free method of extracting information from samples. Using nnU-Net, we trained a segmentation model on a novel dataset of spatial Raman spectra aligned with tumour annotations, achieving a mean foreground Dice score of 80.9%, surpassing previous work. Furthermore, we propose a novel, interpretable, prototype-based architecture called RamanSeg. RamanSeg classifies pixels based on discovered regions of the training set, generating a segmentation mask. Two variants of RamanSeg allow a trade-off between interpretability and performance: one with prototype projecti",
      "authors": [
        "Chris Tomy",
        "Mo Vali",
        "David Pertzborn",
        "Tammam Alamatouri",
        "Anna Mühlig",
        "Orlando Guntinas-Lichius",
        "Anna Xylander",
        "Eric Michele Fantuzzi",
        "Matteo Negro",
        "Francesco Crisafi",
        "Pietro Lio",
        "Tiago Azevedo"
      ],
      "published": "2026-02-20T10:18:27Z",
      "updated": "2026-02-20T10:18:27Z",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18119v1",
      "abs_url": "https://arxiv.org/abs/2602.18119v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.18114v1",
      "title": "Non-Stationary Online Resource Allocation: Learning from a Single Sample",
      "summary": "We study online resource allocation under non-stationary demand with a minimum offline data requirement. In this problem, a decision-maker must allocate multiple types of resources to sequentially arriving queries over a finite horizon. Each query belongs to a finite set of types with fixed resource consumption and a stochastic reward drawn from an unknown, type-specific distribution. Critically, the environment exhibits arbitrary non-stationarity -- arrival distributions may shift unpredictably-while the algorithm requires only one historical sample per period to operate effectively. We distinguish two settings based on sample informativeness: (i) reward-observed samples containing both query type and reward realization, and (ii) the more challenging type-only samples revealing only query",
      "authors": [
        "Yiding Feng",
        "Jiashuo Jiang",
        "Yige Wang"
      ],
      "published": "2026-02-20T10:07:35Z",
      "updated": "2026-02-20T10:07:35Z",
      "categories": [
        "cs.LG",
        "cs.DS",
        "math.OC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.18114v1",
      "abs_url": "https://arxiv.org/abs/2602.18114v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.17997v1",
      "title": "Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly",
      "summary": "Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly's brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control acro",
      "authors": [
        "Zehao Jin",
        "Yaoye Zhu",
        "Chen Zhang",
        "Yanan Sui"
      ],
      "published": "2026-02-20T05:09:28Z",
      "updated": "2026-02-20T05:09:28Z",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17997v1",
      "abs_url": "https://arxiv.org/abs/2602.17997v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.17958v1",
      "title": "Bayesian Online Model Selection",
      "summary": "Online model selection in Bayesian bandits raises a fundamental exploration challenge: When an environment instance is sampled from a prior distribution, how can we design an adaptive strategy that explores multiple bandit learners and competes with the best one in hindsight? We address this problem by introducing a new Bayesian algorithm for online model selection in stochastic bandits. We prove an oracle-style guarantee of $O\\left( d^* M \\sqrt{T} + \\sqrt{(MT)} \\right)$ on the Bayesian regret, where $M$ is the number of base learners, $d^*$ is the regret coefficient of the optimal base learner, and $T$ is the time horizon. We also validate our method empirically across a range of stochastic bandit settings, demonstrating performance that is competitive with the best base learner. Addition",
      "authors": [
        "Aida Afshar",
        "Yuke Zhang",
        "Aldo Pacchiano"
      ],
      "published": "2026-02-20T03:23:55Z",
      "updated": "2026-02-20T03:23:55Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17958v1",
      "abs_url": "https://arxiv.org/abs/2602.17958v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.17940v1",
      "title": "Tighter Regret Lower Bound for Gaussian Process Bandits with Squared Exponential Kernel in Hypersphere",
      "summary": "We study an algorithm-independent, worst-case lower bound for the Gaussian process (GP) bandit problem in the frequentist setting, where the reward function is fixed and has a bounded norm in the known reproducing kernel Hilbert space (RKHS). Specifically, we focus on the squared exponential (SE) kernel, one of the most widely used kernel functions in GP bandits. One of the remaining open questions for this problem is the gap in the \\emph{dimension-dependent} logarithmic factors between upper and lower bounds. This paper partially resolves this open question under a hyperspherical input domain. We show that any algorithm suffers $Ω(\\sqrt{T (\\ln T)^{d} (\\ln \\ln T)^{-d}})$ cumulative regret, where $T$ and $d$ represent the total number of steps and the dimension of the hyperspherical domain,",
      "authors": [
        "Shogo Iwazaki"
      ],
      "published": "2026-02-20T02:17:47Z",
      "updated": "2026-02-20T02:17:47Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.17940v1",
      "abs_url": "https://arxiv.org/abs/2602.17940v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    }
  ]
}