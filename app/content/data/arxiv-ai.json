{
  "scraped_at": "2026-02-26T05:27:29.076Z",
  "categories_searched": [
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "stats": {
    "total_fetched": 200,
    "relevant_count": 50,
    "recent_count": 20,
    "by_category": {
      "benchmarks": 18,
      "agents": 16,
      "code-generation": 14,
      "safety": 1,
      "reasoning": 1
    }
  },
  "relevant_papers": [
    {
      "id": "2602.21854v1",
      "title": "FewMMBench: A Benchmark for Multimodal Few-Shot Learning",
      "summary": "As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit mi",
      "authors": [
        "Mustafa Dogan",
        "Ilker Kesen",
        "Iacer Calixto",
        "Aykut Erdem",
        "Erkut Erdem"
      ],
      "published": "2026-02-25T12:30:18Z",
      "updated": "2026-02-25T12:30:18Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21854v1",
      "abs_url": "https://arxiv.org/abs/2602.21854v1",
      "relevance_score": 15,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "prompt",
        "in-context learning",
        "few-shot",
        "benchmark",
        "reasoning",
        "cot",
        "reasoning",
        "multimodal"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21496v1",
      "title": "Beyond Refusal: Probing the Limits of Agentic Self-Correction for Semantic Sensitive Information",
      "summary": "While defenses for structured PII are mature, Large Language Models (LLMs) pose a new threat: Semantic Sensitive Information (SemSI), where models infer sensitive identity attributes, generate reputation-harmful content, or hallucinate potentially wrong information. The capacity of LLMs to self-regulate these complex, context-dependent sensitive information leaks without destroying utility remains an open scientific question. To address this, we introduce SemSIEdit, an inference-time framework where an agentic \"Editor\" iteratively critiques and rewrites sensitive spans to preserve narrative flow rather than simply refusing to answer. Our analysis reveals a Privacy-Utility Pareto Frontier, where this agentic rewriting reduces leakage by 34.6% across all three SemSI categories while incurrin",
      "authors": [
        "Umid Suleymanov",
        "Zaur Rajabov",
        "Emil Mirzazada",
        "Murat Kantarcioglu"
      ],
      "published": "2026-02-25T02:09:23Z",
      "updated": "2026-02-25T02:09:23Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21496v1",
      "abs_url": "https://arxiv.org/abs/2602.21496v1",
      "relevance_score": 15,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "agent",
        "agentic",
        "reasoning",
        "reasoning",
        "safety"
      ],
      "category": "agents"
    },
    {
      "id": "2602.21858v1",
      "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices",
      "summary": "Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of o",
      "authors": [
        "Dezhi Kong",
        "Zhengzhao Feng",
        "Qiliang Liang",
        "Hao Wang",
        "Haofei Sun",
        "Changpeng Yang",
        "Yang Li",
        "Peng Zhou",
        "Shuai Nie",
        "Hongzhen Wang",
        "Linfeng Zhou",
        "Hao Jia",
        "Jiaming Xu",
        "Runyu Shi",
        "Ying Huang"
      ],
      "published": "2026-02-25T12:32:37Z",
      "updated": "2026-02-25T12:32:37Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21858v1",
      "abs_url": "https://arxiv.org/abs/2602.21858v1",
      "relevance_score": 14,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "agent",
        "benchmark",
        "evaluation",
        "multimodal"
      ],
      "category": "agents"
    },
    {
      "id": "2602.21814v1",
      "title": "Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem",
      "summary": "Large language models consistently fail the \"car wash problem,\" a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These re",
      "authors": [
        "Heejin Jo"
      ],
      "published": "2026-02-25T11:40:15Z",
      "updated": "2026-02-25T11:40:15Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21814v1",
      "abs_url": "https://arxiv.org/abs/2602.21814v1",
      "relevance_score": 14,
      "matched_keywords": [
        "large language model",
        "language model",
        "claude",
        "prompt",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21728v1",
      "title": "Explore-on-Graph: Incentivizing Autonomous Exploration of Large Language Models on Knowledge Graphs with Path-refined Reward Modeling",
      "summary": "The reasoning process of Large Language Models (LLMs) is often plagued by hallucinations and missing facts in question-answering tasks. A promising solution is to ground LLMs' answers in verifiable knowledge sources, such as Knowledge Graphs (KGs). Prevailing KG-enhanced methods typically constrained LLM reasoning either by enforcing rules during generation or by imitating paths from a fixed set of demonstrations. However, they naturally confined the reasoning patterns of LLMs within the scope of prior experience or fine-tuning data, limiting their generalizability to out-of-distribution graph reasoning problems. To tackle this problem, in this paper, we propose Explore-on-Graph (EoG), a novel framework that encourages LLMs to autonomously explore a more diverse reasoning space on KGs. To ",
      "authors": [
        "Shiqi Yan",
        "Yubo Chen",
        "Ruiqi Zhou",
        "Zhengxi Yao",
        "Shuai Chen",
        "Tianyi Zhang",
        "Shijie Zhang",
        "Wei Qiang Zhang",
        "Yongfeng Huang",
        "Haixin Duan",
        "Yunqi Zhang"
      ],
      "published": "2026-02-25T09:35:18Z",
      "updated": "2026-02-25T09:35:18Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21728v1",
      "abs_url": "https://arxiv.org/abs/2602.21728v1",
      "relevance_score": 13,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "fine-tuning",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag",
        "hallucination"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21420v1",
      "title": "Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from a well-documented pathology: while they improve Pass@1 accuracy through sharpened sampling, they simultaneously narrow the model's reasoning boundary and reduce generation diversity. We identify a root cause that existing methods overlook: the uniform penalization of errors. Current approaches -- whether data-filtering methods that select prompts by difficulty, or advantage normalization schemes -- treat all incorrect rollouts within a group identically. We show that this uniformity allows overconfident errors (incorrect reasoning paths that the RL process has spuriously reinforced) to persist and monopo",
      "authors": [
        "Yuanda Xu",
        "Hejian Sang",
        "Zhengze Zhou",
        "Ran He",
        "Zhipeng Wang"
      ],
      "published": "2026-02-24T22:46:43Z",
      "updated": "2026-02-24T22:46:43Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21420v1",
      "abs_url": "https://arxiv.org/abs/2602.21420v1",
      "relevance_score": 13,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "llama",
        "fine-tuning",
        "prompt",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.22072v1",
      "title": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models",
      "summary": "Theory of Mind (ToM) refers to an agent's ability to model the internal states of others. Contributing to the debate whether large language models (LLMs) exhibit genuine ToM capabilities, our study investigates their ToM robustness using perturbations on false-belief tasks and examines the potential of Chain-of-Thought prompting (CoT) to enhance performance and explain the LLM's decision. We introduce a handcrafted, richly annotated ToM dataset, including classic and perturbed false belief tasks, the corresponding spaces of valid reasoning chains for correct task completion, subsequent reasoning faithfulness, task solutions, and propose metrics to evaluate reasoning chain correctness and to what extent final answers are faithful to reasoning traces of the generated CoT. We show a steep dro",
      "authors": [
        "Christian Nickel",
        "Laura Schrewe",
        "Florian Mai",
        "Lucie Flek"
      ],
      "published": "2026-02-25T16:24:35Z",
      "updated": "2026-02-25T16:24:35Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22072v1",
      "abs_url": "https://arxiv.org/abs/2602.22072v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "prompt",
        "reasoning",
        "cot",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.21951v1",
      "title": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning",
      "summary": "Knowledge graph reasoning (KGR) infers missing facts, with recent advances increasingly harnessing the semantic priors and reasoning abilities of Large Language Models (LLMs). However, prevailing generative paradigms are prone to memorizing surface-level co-occurrences rather than learning genuine relational semantics, limiting out-of-distribution generalization. To address this, we propose RADAR, which reformulates KGR from generative pattern matching to discriminative relational reasoning. We recast KGR as discriminative entity selection, where reinforcement learning enforces relative entity separability beyond token-likelihood imitation. Leveraging this separability, inference operates directly in representation space, ensuring consistency with the discriminative optimization and bypass",
      "authors": [
        "Bo Xue",
        "Yuan Jin",
        "Luoyi Fu",
        "Jiaxin Ding",
        "Xinbing Wang"
      ],
      "published": "2026-02-25T14:34:02Z",
      "updated": "2026-02-25T14:34:02Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21951v1",
      "abs_url": "https://arxiv.org/abs/2602.21951v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag",
        "hallucination"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21693v1",
      "title": "TiMi: Empower Time Series Transformers with Multimodal Mixture of Experts",
      "summary": "Multimodal time series forecasting has garnered significant attention for its potential to provide more accurate predictions than traditional single-modality models by leveraging rich information inherent in other modalities. However, due to fundamental challenges in modality alignment, existing methods often struggle to effectively incorporate multimodal data into predictions, particularly textual information that has a causal influence on time series fluctuations, such as emergency reports and policy announcements. In this paper, we reflect on the role of textual information in numerical forecasting and propose Time series transformers with Multimodal Mixture-of-Experts, TiMi, to unleash the causal reasoning capabilities of LLMs. Concretely, TiMi utilizes LLMs to generate inferences on f",
      "authors": [
        "Jiafeng Lin",
        "Yuxuan Wang",
        "Huakun Luo",
        "Zhongyi Pei",
        "Jianmin Wang"
      ],
      "published": "2026-02-25T08:51:03Z",
      "updated": "2026-02-25T08:51:03Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21693v1",
      "abs_url": "https://arxiv.org/abs/2602.21693v1",
      "relevance_score": 12,
      "matched_keywords": [
        "llm",
        "transformer",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag",
        "multimodal",
        "alignment"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21628v1",
      "title": "RuCL: Stratified Rubric-Based Curriculum Learning for Multimodal Large Language Model Reasoning",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a prevailing paradigm for enhancing reasoning in Multimodal Large Language Models (MLLMs). However, relying solely on outcome supervision risks reward hacking, where models learn spurious reasoning patterns to satisfy final answer checks. While recent rubric-based approaches offer fine-grained supervision signals, they suffer from high computational costs of instance-level generation and inefficient training dynamics caused by treating all rubrics as equally learnable. In this paper, we propose Stratified Rubric-based Curriculum Learning (RuCL), a novel framework that reformulates curriculum learning by shifting the focus from data selection to reward design. RuCL generates generalized rubrics for broad applicability and ",
      "authors": [
        "Yukun Chen",
        "Jiaming Li",
        "Longze Chen",
        "Ze Gong",
        "Jingpeng Li",
        "Zhen Qin",
        "Hengyu Chang",
        "Ancheng Xu",
        "Zhihao Yang",
        "Hamid Alinejad-Rokny",
        "Qiang Qu",
        "Bo Zheng",
        "Min Yang"
      ],
      "published": "2026-02-25T06:46:24Z",
      "updated": "2026-02-25T06:46:24Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21628v1",
      "abs_url": "https://arxiv.org/abs/2602.21628v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag",
        "multimodal"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21611v1",
      "title": "Structurally Aligned Subtask-Level Memory for Software Engineering Agents",
      "summary": "Large Language Models (LLMs) have demonstrated significant potential as autonomous software engineering (SWE) agents. Recent work has further explored augmenting these agents with memory mechanisms to support long-horizon reasoning. However, these approaches typically operate at a coarse instance granularity, treating the entire problem-solving episode as the atomic unit of storage and retrieval. We empirically demonstrate that instance-level memory suffers from a fundamental granularity mismatch, resulting in misguided retrieval when tasks with similar surface descriptions require distinct reasoning logic at specific stages. To address this, we propose Structurally Aligned Subtask-Level Memory, a method that aligns memory storage, retrieval, and updating with the agent's functional decomp",
      "authors": [
        "Kangning Shen",
        "Jingyuan Zhang",
        "Chenxi Sun",
        "Wencong Zeng",
        "Yang Yue"
      ],
      "published": "2026-02-25T06:13:25Z",
      "updated": "2026-02-25T06:13:25Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21611v1",
      "abs_url": "https://arxiv.org/abs/2602.21611v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gemini",
        "agent",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.21533v1",
      "title": "Reasoning-Driven Design of Single Atom Catalysts via a Multi-Agent Large Language Model Framework",
      "summary": "Large language models (LLMs) are becoming increasingly applied beyond natural language processing, demonstrating strong capabilities in complex scientific tasks that traditionally require human expertise. This progress has extended into materials discovery, where LLMs introduce a new paradigm by leveraging reasoning and in-context learning, capabilities absent from conventional machine learning approaches. Here, we present a Multi-Agent-based Electrocatalyst Search Through Reasoning and Optimization (MAESTRO) framework in which multiple LLMs with specialized roles collaboratively discover high-performance single atom catalysts for the oxygen reduction reaction. Within an autonomous design loop, agents iteratively reason, propose modifications, reflect on results and accumulate design histo",
      "authors": [
        "Dong Hyeon Mok",
        "Seoin Back",
        "Victor Fung",
        "Guoxiang Hu"
      ],
      "published": "2026-02-25T03:43:24Z",
      "updated": "2026-02-25T03:43:24Z",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21533v1",
      "abs_url": "https://arxiv.org/abs/2602.21533v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "in-context learning",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.21447v1",
      "title": "Adversarial Intent is a Latent Variable: Stateful Trust Inference for Securing Multimodal Agentic RAG",
      "summary": "Current stateless defences for multimodal agentic RAG fail to detect adversarial strategies that distribute malicious semantics across retrieval, planning, and generation components. We formulate this security challenge as a Partially Observable Markov Decision Process (POMDP), where adversarial intent is a latent variable inferred from noisy multi-stage observations. We introduce MMA-RAG^T, an inference-time control framework governed by a Modular Trust Agent (MTA) that maintains an approximate belief state via structured LLM reasoning. Operating as a model-agnostic overlay, MMA-RAGT mediates a configurable set of internal checkpoints to enforce stateful defence-in-depth. Extensive evaluation on 43,774 instances demonstrates a 6.50x average reduction factor in Attack Success Rate relative",
      "authors": [
        "Inderjeet Singh",
        "Vikas Pahuja",
        "Aishvariya Priya Rathina Sabapathy",
        "Chiara Picardi",
        "Amit Giloni",
        "Roman Vainshtein",
        "Andr√©s Murillo",
        "Hisashi Kojima",
        "Motoyoshi Sekiya",
        "Yuki Unno",
        "Junichi Suga"
      ],
      "published": "2026-02-24T23:52:27Z",
      "updated": "2026-02-24T23:52:27Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21447v1",
      "abs_url": "https://arxiv.org/abs/2602.21447v1",
      "relevance_score": 12,
      "matched_keywords": [
        "llm",
        "agent",
        "agentic",
        "evaluation",
        "reasoning",
        "reasoning",
        "rag",
        "multimodal"
      ],
      "category": "agents"
    },
    {
      "id": "2602.22190v1",
      "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL",
      "summary": "Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of ",
      "authors": [
        "Rui Yang",
        "Qianhui Wu",
        "Zhaoyang Wang",
        "Hanyang Chen",
        "Ke Yang",
        "Hao Cheng",
        "Huaxiu Yao",
        "Baoling Peng",
        "Huan Zhang",
        "Jianfeng Gao",
        "Tong Zhang"
      ],
      "published": "2026-02-25T18:34:57Z",
      "updated": "2026-02-25T18:34:57Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22190v1",
      "abs_url": "https://arxiv.org/abs/2602.22190v1",
      "relevance_score": 11,
      "matched_keywords": [
        "agent",
        "benchmark",
        "reasoning",
        "cot",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.22175v1",
      "title": "DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs",
      "summary": "Understanding and reasoning over long contexts is a crucial capability for language models (LMs). Although recent models support increasingly long context windows, their accuracy often deteriorates as input length grows. In practice, models often struggle to keep attention aligned with the most relevant context throughout decoding. In this work, we propose DySCO, a novel decoding algorithm for improving long-context reasoning. DySCO leverages retrieval heads--a subset of attention heads specialized for long-context retrieval--to identify task-relevant tokens at each decoding step and explicitly up-weight them. By doing so, DySCO dynamically adjusts attention during generation to better utilize relevant context. The method is training-free and can be applied directly to any off-the-shelf LM",
      "authors": [
        "Xi Ye",
        "Wuwei Zhang",
        "Fangcong Yin",
        "Howard Yen",
        "Danqi Chen"
      ],
      "published": "2026-02-25T18:21:35Z",
      "updated": "2026-02-25T18:21:35Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22175v1",
      "abs_url": "https://arxiv.org/abs/2602.22175v1",
      "relevance_score": 11,
      "matched_keywords": [
        "language model",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.21715v1",
      "title": "Two-Stage Active Distribution Network Voltage Control via LLM-RL Collaboration: A Hybrid Knowledge-Data-Driven Approach",
      "summary": "The growing integration of distributed photovoltaics (PVs) into active distribution networks (ADNs) has exacerbated operational challenges, making it imperative to coordinate diverse equipment to mitigate voltage violations and enhance power quality. Although existing data-driven approaches have demonstrated effectiveness in the voltage control problem, they often require extensive trial-and-error exploration and struggle to incorporate heterogeneous information, such as day-ahead forecasts and semantic-based grid codes. Considering the operational scenarios and requirements in real-world ADNs, in this paper, we propose a hybrid knowledge-data-driven approach that leverages dynamic collaboration between a large language model (LLM) agent and a reinforcement learning (RL) agent to achieve t",
      "authors": [
        "Xu Yang",
        "Chenhui Lin",
        "Xiang Ma",
        "Dong Liu",
        "Ran Zheng",
        "Haotian Liu",
        "Wenchuan Wu"
      ],
      "published": "2026-02-25T09:22:27Z",
      "updated": "2026-02-25T09:22:27Z",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21715v1",
      "abs_url": "https://arxiv.org/abs/2602.21715v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.21706v1",
      "title": "SurGo-R1: Benchmarking and Modeling Contextual Reasoning for Operative Zone in Surgical Video",
      "summary": "Minimally invasive surgery has dramatically improved patient operative outcomes, yet identifying safe operative zones remains challenging in critical phases, requiring surgeons to integrate visual cues, procedural phase, and anatomical context under high cognitive load. Existing AI systems offer binary safety verification or static detection, ignoring the phase-dependent nature of intraoperative reasoning. We introduce ResGo, a benchmark of laparoscopic frames annotated with Go Zone bounding boxes and clinician-authored rationales covering phase, exposure quality reasoning, next action and risk reminder. We introduce evaluation metrics that treat correct grounding under incorrect phase as failures, revealing that most vision-language models cannot handle such tasks and perform poorly. We t",
      "authors": [
        "Guanyi Qin",
        "Xiaozhen Wang",
        "Zhu Zhuo",
        "Chang Han Low",
        "Yuancan Xiao",
        "Yibing Fu",
        "Haofeng Liu",
        "Kai Wang",
        "Chunjiang Li",
        "Yueming Jin"
      ],
      "published": "2026-02-25T09:11:45Z",
      "updated": "2026-02-25T09:11:45Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21706v1",
      "abs_url": "https://arxiv.org/abs/2602.21706v1",
      "relevance_score": 11,
      "matched_keywords": [
        "language model",
        "rlhf",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning",
        "safety"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.21638v1",
      "title": "Multi-dimensional Assessment and Explainable Feedback for Counselor Responses to Client Resistance in Text-based Counseling with LLMs",
      "summary": "Effectively addressing client resistance is a sophisticated clinical skill in psychological counseling, yet practitioners often lack timely and scalable supervisory feedback to refine their approaches. Although current NLP research has examined overall counseling quality and general therapeutic skills, it fails to provide granular evaluations of high-stakes moments where clients exhibit resistance. In this work, we present a comprehensive pipeline for the multi-dimensional evaluation of human counselors' interventions specifically targeting client resistance in text-based therapy. We introduce a theory-driven framework that decomposes counselor responses into four distinct communication mechanisms. Leveraging this framework, we curate and share an expert-annotated dataset of real-world cou",
      "authors": [
        "Anqi Li",
        "Ruihan Wang",
        "Zhaoming Chen",
        "Yuqian Chen",
        "Yu Lu",
        "Yi Zhu",
        "Yuan Xie",
        "Zhenzhong Lan"
      ],
      "published": "2026-02-25T07:05:05Z",
      "updated": "2026-02-25T07:05:05Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21638v1",
      "abs_url": "https://arxiv.org/abs/2602.21638v1",
      "relevance_score": 11,
      "matched_keywords": [
        "llm",
        "gpt",
        "claude",
        "llama",
        "instruction tuning",
        "evaluation",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21619v1",
      "title": "When More Is Less: A Systematic Analysis of Spatial and Commonsense Information for Visual Spatial Reasoning",
      "summary": "Visual spatial reasoning (VSR) remains challenging for modern vision-language models (VLMs), despite advances in multimodal architectures. A common strategy is to inject additional information at inference time, such as explicit spatial cues, external commonsense knowledge, or chain-of-thought (CoT) reasoning instructions. However, it remains unclear when such information genuinely improves reasoning and when it introduces noise. In this paper, we conduct a hypothesis-driven analysis of information injection for VSR across three representative VLMs and two public benchmarks. We examine (i) the type and number of spatial contexts, (ii) the amount and relevance of injected commonsense knowledge, and (iii) the interaction between spatial grounding and CoT prompting. Our results reveal a consi",
      "authors": [
        "Muku Akasaka",
        "Soyeon Caren Han"
      ],
      "published": "2026-02-25T06:22:48Z",
      "updated": "2026-02-25T06:22:48Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21619v1",
      "abs_url": "https://arxiv.org/abs/2602.21619v1",
      "relevance_score": 11,
      "matched_keywords": [
        "language model",
        "prompt",
        "benchmark",
        "reasoning",
        "cot",
        "reasoning",
        "multimodal"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21371v1",
      "title": "Interleaved Head Attention",
      "summary": "Multi-Head Attention (MHA) is the core computational primitive underlying modern Large Language Models (LLMs). However, MHA suffers from a fundamental linear scaling limitation: $H$ attention heads produce exactly $H$ independent attention matrices, with no communication between heads during attention computation. This becomes problematic for multi-step reasoning, where correct answers depend on aggregating evidence from multiple parts of the context and composing latent token-to-token relations over a chain of intermediate inferences. To address this, we propose Interleaved Head Attention (IHA), which enables cross-head mixing by constructing $P$ pseudo-heads per head (typically $P=H$), where each pseudo query/key/value is a learned linear combination of all $H$ original queries, keys and",
      "authors": [
        "Sai Surya Duvvuri",
        "Chanakya Ekbote",
        "Rachit Bansal",
        "Rishabh Tiwari",
        "Devvrit Khatri",
        "David Brandfonbrener",
        "Paul Liang",
        "Inderjit Dhillon",
        "Manzil Zaheer"
      ],
      "published": "2026-02-24T21:05:23Z",
      "updated": "2026-02-24T21:05:23Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21371v1",
      "abs_url": "https://arxiv.org/abs/2602.21371v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "fine-tuning",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.22125v1",
      "title": "IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages",
      "summary": "Instruction-following benchmarks remain predominantly English-centric, leaving a critical evaluation gap for the hundreds of millions of Indic language speakers. We introduce IndicIFEval, a benchmark evaluating constrained generation of LLMs across 14 Indic languages using automatically verifiable, rule-based instructions. It comprises around 800 human-verified examples per language spread across two complementary subsets: IndicIFEval-Ground, translated prompts from IFEval (Zhou et al., 2023) carefully localized for Indic contexts, and IndicIFEval-Ground, synthetically generated instructions grounded in native Indic content. We conduct a comprehensive evaluation of major open-weight and proprietary models spanning both reasoning and non-reasoning models. While models maintain strong adhere",
      "authors": [
        "Thanmay Jayakumar",
        "Mohammed Safi Ur Rahman Khan",
        "Raj Dabre",
        "Ratish Puduppully",
        "Anoop Kunchukuttan"
      ],
      "published": "2026-02-25T17:12:37Z",
      "updated": "2026-02-25T17:12:37Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22125v1",
      "abs_url": "https://arxiv.org/abs/2602.22125v1",
      "relevance_score": 10,
      "matched_keywords": [
        "llm",
        "prompt",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21947v1",
      "title": "Large Language Models are Algorithmically Blind",
      "summary": "Large language models (LLMs) demonstrate remarkable breadth of knowledge, yet their ability to reason about computational processes remains poorly understood. Closing this gap matters for practitioners who rely on LLMs to guide algorithm selection and deployment. We address this limitation using causal discovery as a testbed and evaluate eight frontier LLMs against ground truth derived from large-scale algorithm executions and find systematic, near-total failure. Models produce ranges far wider than true confidence intervals yet still fail to contain the true algorithmic mean in the majority of instances; most perform worse than random guessing and the marginal above-random performance of the best model is most consistent with benchmark memorization rather than principled reasoning. We ter",
      "authors": [
        "Sohan Venkatesh",
        "Ashish Mahendran Kurapath",
        "Tejas Melkote"
      ],
      "published": "2026-02-25T14:32:15Z",
      "updated": "2026-02-25T14:32:15Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21947v1",
      "abs_url": "https://arxiv.org/abs/2602.21947v1",
      "relevance_score": 10,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21800v1",
      "title": "An Evaluation of Context Length Extrapolation in Long Code via Positional Embeddings and Efficient Attention",
      "summary": "The rapid advancement of large language models (LLMs) has led to a significant increase in automated tools in the software engineering, capable of performing various code-related tasks such as code generation, completion, and translation. Despite these advancements, its effectiveness is constrained by fixed context lengths, limiting its ability to generalize across long, domain-specific code sequences. To address this challenge, we investigate zero-shot, inference-only methods aimed at improving position encodings and optimizing attention mechanisms. Our goal is to provide a thorough analysis of current approaches that facilitate context length extrapolation in code, particularly in the context of long code completion tasks.",
      "authors": [
        "Madhusudan Ghosh",
        "Rishabh Gupta"
      ],
      "published": "2026-02-25T11:27:34Z",
      "updated": "2026-02-25T11:27:34Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21800v1",
      "abs_url": "https://arxiv.org/abs/2602.21800v1",
      "relevance_score": 10,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "attention mechanism",
        "code generation",
        "evaluation"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.21670v1",
      "title": "Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning",
      "summary": "Multi-robot task planning requires decomposing natural-language instructions into executable actions for heterogeneous robot teams. Conventional Planning Domain Definition Language (PDDL) planners provide rigorous guarantees but struggle to handle ambiguous or long-horizon missions, while large language models (LLMs) can interpret instructions and propose plans but may hallucinate or produce infeasible actions. We present a hierarchical multi-agent LLM-based planner with prompt optimization: an upper layer decomposes tasks and assigns them to lower-layer agents, which generate PDDL problems solved by a classical planner. When plans fail, the system applies TextGrad-inspired textual-gradient updates to optimize each agent's prompt and thereby improve planning accuracy. In addition, meta-pro",
      "authors": [
        "Tomoya Kawabe",
        "Rin Takano"
      ],
      "published": "2026-02-25T08:08:26Z",
      "updated": "2026-02-25T08:08:26Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21670v1",
      "abs_url": "https://arxiv.org/abs/2602.21670v1",
      "relevance_score": 10,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "prompt",
        "benchmark"
      ],
      "category": "agents"
    },
    {
      "id": "2602.21441v1",
      "title": "Causal Decoding for Hallucination-Resistant Multimodal Large Language Models",
      "summary": "Multimodal Large Language Models (MLLMs) deliver detailed responses on vision-language tasks, yet remain susceptible to object hallucination (introducing objects not present in the image), undermining reliability in practice. Prior efforts often rely on heuristic penalties, post-hoc correction, or generic decoding tweaks, which do not directly intervene in the mechanisms that trigger object hallucination and thus yield limited gains. To address this challenge, we propose a causal decoding framework that applies targeted causal interventions during generation to curb spurious object mentions. By reshaping the decoding dynamics to attenuate spurious dependencies, our approach reduces false object tokens while maintaining descriptive quality. Across captioning and QA benchmarks, our framework",
      "authors": [
        "Shiwei Tan",
        "Hengyi Wang",
        "Weiyi Qin",
        "Qi Xu",
        "Zhigang Hua",
        "Hao Wang"
      ],
      "published": "2026-02-24T23:35:46Z",
      "updated": "2026-02-24T23:35:46Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21441v1",
      "abs_url": "https://arxiv.org/abs/2602.21441v1",
      "relevance_score": 10,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "multimodal",
        "hallucination"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.21368v1",
      "title": "Black-Box Reliability Certification for AI Agents via Self-Consistency Sampling and Conformal Calibration",
      "summary": "Given a black-box AI system and a task, at what confidence level can a practitioner trust the system's output? We answer with a reliability level -- a single number per system-task pair, derived from self-consistency sampling and conformal calibration, that serves as a black-box deployment gate with exact, finite-sample, distribution-free guarantees. Self-consistency sampling reduces uncertainty exponentially; conformal calibration guarantees correctness within 1/(n+1) of the target level, regardless of the system's errors -- made transparently visible through larger answer sets for harder questions. Weaker models earn lower reliability levels (not accuracy -- see Definition 2.4): GPT-4.1 earns 94.6% on GSM8K and 96.8% on TruthfulQA, while GPT-4.1-nano earns 89.8% on GSM8K and 66.5% on MML",
      "authors": [
        "Charafeddine Mouzouni"
      ],
      "published": "2026-02-24T21:03:50Z",
      "updated": "2026-02-24T21:03:50Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21368v1",
      "abs_url": "https://arxiv.org/abs/2602.21368v1",
      "relevance_score": 10,
      "matched_keywords": [
        "gpt",
        "agent",
        "benchmark",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.22090v1",
      "title": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference",
      "summary": "Large Language Models (LLMs) have revolutionized inference across diverse natural language tasks, with larger models performing better but at higher computational costs. We propose a confidence-driven strategy that dynamically selects the most suitable model based on confidence estimates. By assessing a model's confidence in handling the task and response accuracy, tasks that are likely to be solved correctly are retained, while more uncertain or complex cases are delegated to a larger model, ensuring reliability while minimizing computation. Specifically, we evaluate a model's likelihood of knowing the correct answer and the probability that its response is accurate. Experiments on the Massive Multitask Language Understanding (MMLU) benchmark show that our approach achieves accuracy compa",
      "authors": [
        "Bo-Wei Chen",
        "Chung-Chi Chen",
        "An-Zi Yen"
      ],
      "published": "2026-02-25T16:38:03Z",
      "updated": "2026-02-25T16:38:03Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22090v1",
      "abs_url": "https://arxiv.org/abs/2602.22090v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "benchmark"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.22070v1",
      "title": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts",
      "summary": "Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algorithmic agent. To be inclusive of different evaluation formats, we conduct our study with two task presentations: stated preferences, modeled through direct queries about trust towards either agent, an",
      "authors": [
        "Jessica Y. Bo",
        "Lillio Mok",
        "Ashton Anderson"
      ],
      "published": "2026-02-25T16:18:28Z",
      "updated": "2026-02-25T16:18:28Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22070v1",
      "abs_url": "https://arxiv.org/abs/2602.22070v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "prompt",
        "evaluation",
        "safety"
      ],
      "category": "agents"
    },
    {
      "id": "2602.21997v1",
      "title": "Enhancing LLM-Based Test Generation by Eliminating Covered Code",
      "summary": "Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LLM-based unit test generation method. Our approach consists of two key steps. The first step is context information retrieval, which uses both LLMs and static analysis to gather relevant contextual information associated with the complex methods under test. The second step, iterative test generation ",
      "authors": [
        "WeiZhe Xu",
        "Mengyu Liu",
        "Fanxin Kong"
      ],
      "published": "2026-02-25T15:16:43Z",
      "updated": "2026-02-25T15:16:43Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21997v1",
      "abs_url": "https://arxiv.org/abs/2602.21997v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "evaluation",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.21779v1",
      "title": "Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models",
      "summary": "Current Vision-Language Models (VLMs) for deepfake detection excel at identifying spatial artifacts but overlook a critical dimension: temporal inconsistencies in video forgeries. Adapting VLMs to reason about these dynamic cues remains a distinct challenge. To bridge this gap, we propose Forensic Answer-Questioning (FAQ), a large-scale benchmark that formulates temporal deepfake analysis as a multiple-choice task. FAQ introduces a three-level hierarchy to progressively evaluate and equip VLMs with forensic capabilities: (1) Facial Perception, testing the ability to identify static visual artifacts; (2) Temporal Deepfake Grounding, requiring the localization of dynamic forgery artifacts across frames; and (3) Forensic Reasoning, challenging models to synthesize evidence for final authentic",
      "authors": [
        "Zheyuan Gu",
        "Qingsong Zhao",
        "Yusong Wang",
        "Zhaohong Huang",
        "Xinqi Li",
        "Cheng Yuan",
        "Jiaowei Shao",
        "Chi Zhang",
        "Xuelong Li"
      ],
      "published": "2026-02-25T10:54:55Z",
      "updated": "2026-02-25T10:54:55Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21779v1",
      "abs_url": "https://arxiv.org/abs/2602.21779v1",
      "relevance_score": 9,
      "matched_keywords": [
        "language model",
        "benchmark",
        "reasoning",
        "reasoning",
        "vision language"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21763v1",
      "title": "Improving Implicit Discourse Relation Recognition with Natural Language Explanations from LLMs",
      "summary": "Implicit Discourse Relation Recognition (IDRR) remains a challenging task due to the requirement for deep semantic understanding in the absence of explicit discourse markers. A further limitation is that existing methods only predict relations without providing any supporting explanations. Recent advances in large language models (LLMs) have shown strong reasoning capabilities in both deep language understanding and natural language explanation generation. In this work, we propose a simple yet effective approach to distill the reasoning capabilities of LLMs into lightweight IDRR models to improve both performance and interpretability. Specifically, we first prompt an LLM to generate explanations for each training instance conditioned on its gold label. Then, we introduce a novel classifica",
      "authors": [
        "Heng Wang",
        "Changxing Wu"
      ],
      "published": "2026-02-25T10:28:45Z",
      "updated": "2026-02-25T10:28:45Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21763v1",
      "abs_url": "https://arxiv.org/abs/2602.21763v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "prompt",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21646v1",
      "title": "Scalable Multilingual Multimodal Machine Translation with Speech-Text Fusion",
      "summary": "Multimodal Large Language Models (MLLMs) have achieved notable success in enhancing translation performance by integrating multimodal information. However, existing research primarily focuses on image-guided methods, whose applicability is constrained by the scarcity of multilingual image-text pairs. The speech modality overcomes this limitation due to its natural alignment with text and the abundance of existing speech datasets, which enable scalable language coverage. In this paper, we propose a Speech-guided Machine Translation (SMT) framework that integrates speech and text as fused inputs into an MLLM to improve translation quality. To mitigate reliance on low-resource data, we introduce a Self-Evolution Mechanism. The core components of this framework include a text-to-speech model, ",
      "authors": [
        "Yexing Du",
        "Youcheng Pan",
        "Zekun Wang",
        "Zheng Chu",
        "Yichong Huang",
        "Kaiyuan Liu",
        "Bo Yang",
        "Yang Xiang",
        "Ming Liu",
        "Bing Qin"
      ],
      "published": "2026-02-25T07:19:34Z",
      "updated": "2026-02-25T07:19:34Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21646v1",
      "abs_url": "https://arxiv.org/abs/2602.21646v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "rag",
        "multimodal",
        "alignment"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.21593v1",
      "title": "Breaking Semantic-Aware Watermarks via LLM-Guided Coherence-Preserving Semantic Injection",
      "summary": "Generative images have proliferated on Web platforms in social media and online copyright distribution scenarios, and semantic watermarking has increasingly been integrated into diffusion models to support reliable provenance tracking and forgery prevention for web content. Traditional noise-layer-based watermarking, however, remains vulnerable to inversion attacks that can recover embedded signals. To mitigate this, recent content-aware semantic watermarking schemes bind watermark signals to high-level image semantics, constraining local edits that would otherwise disrupt global coherence. Yet, large language models (LLMs) possess structured reasoning capabilities that enable targeted exploration of semantic spaces, allowing locally fine-grained but globally coherent semantic alterations ",
      "authors": [
        "Zheng Gao",
        "Xiaoyu Li",
        "Zhicheng Bao",
        "Xiaoyan Feng",
        "Jiaojiao Jiang"
      ],
      "published": "2026-02-25T05:38:08Z",
      "updated": "2026-02-25T05:38:08Z",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21593v1",
      "abs_url": "https://arxiv.org/abs/2602.21593v1",
      "relevance_score": 9,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "reasoning",
        "reasoning",
        "rag",
        "alignment"
      ],
      "category": "safety"
    },
    {
      "id": "2602.21933v1",
      "title": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text",
      "summary": "Sarcasm detection in multilingual and code-mixed environments remains a challenging task for natural language processing models due to structural variations, informal expressions, and low-resource linguistic availability. This study compares four large language models, Llama 3.1, Mistral, Gemma 3, and Phi-4, with a fine-tuned DistilBERT model for sarcasm detection in code-mixed Hinglish text. The results indicate that the smaller, sequentially fine-tuned DistilBERT model achieved the highest overall accuracy of 84%, outperforming all of the LLMs in zero and few-shot set ups, using minimal LLM generated code-mixed data used for fine-tuning. These findings indicate that domain-adaptive fine-tuning of smaller transformer based models may significantly improve sarcasm detection over general LL",
      "authors": [
        "Bitan Majumder",
        "Anirban Sen"
      ],
      "published": "2026-02-25T14:12:16Z",
      "updated": "2026-02-25T14:12:16Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21933v1",
      "abs_url": "https://arxiv.org/abs/2602.21933v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "llama",
        "mistral",
        "transformer",
        "fine-tuning",
        "few-shot"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.21857v1",
      "title": "Distill and Align Decomposition for Enhanced Claim Verification",
      "summary": "Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human eva",
      "authors": [
        "Jabez Magomere",
        "Elena Kochkina",
        "Samuel Mensah",
        "Simerjot Kaur",
        "Fernando Acero",
        "Arturo Oncevay",
        "Charese H. Smiley",
        "Xiaomo Liu",
        "Manuela Veloso"
      ],
      "published": "2026-02-25T12:32:04Z",
      "updated": "2026-02-25T12:32:04Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21857v1",
      "abs_url": "https://arxiv.org/abs/2602.21857v1",
      "relevance_score": 8,
      "matched_keywords": [
        "language model",
        "prompt",
        "evaluation",
        "reasoning",
        "reasoning",
        "alignment"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21786v1",
      "title": "D-COT: Disciplined Chain-of-Thought Learning for Efficient Reasoning in Small Language Models",
      "summary": "Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) often induces \"overthinking\" in Small Language Models (SLMs), leading to performance degradation and excessive token consumption. In this study, we propose Disciplined Chain-of-Thought (D-CoT), a novel framework that enforces a structured reasoning process using control tags -- such as &lt;TEMP_LOW&gt; for fact-checking and &lt;TEMP_HIGH&gt; for multi-perspective exploration -- as auxiliary scaffolding during training. By optimizing the CoT trajectory, D-CoT suppresses reasoning drift and simultaneously achieves token reduction and performance improvement. We demonstrate the efficacy of our approach on Qwen3-8B: with only 5,000 training samples, D-CoT significantly boosts accuracy on GPQA-diamond by 9.9% and MMLU-Pro (0-",
      "authors": [
        "Shunsuke Ubukata"
      ],
      "published": "2026-02-25T11:08:38Z",
      "updated": "2026-02-25T11:08:38Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21786v1",
      "abs_url": "https://arxiv.org/abs/2602.21786v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "reasoning",
        "cot",
        "reasoning"
      ],
      "category": "reasoning"
    },
    {
      "id": "2602.21756v1",
      "title": "Offline Reasoning for Efficient Recommendation: LLM-Empowered Persona-Profiled Item Indexing",
      "summary": "Recent advances in large language models (LLMs) offer new opportunities for recommender systems by capturing the nuanced semantics of user interests and item characteristics through rich semantic understanding and contextual reasoning. In particular, LLMs have been employed as rerankers that reorder candidate items based on inferred user-item relevance. However, these approaches often require expensive online inference-time reasoning, leading to high latency that hampers real-world deployment. In this work, we introduce Persona4Rec, a recommendation framework that performs offline reasoning to construct interpretable persona representations of items, enabling lightweight and scalable real-time inference. In the offline stage, Persona4Rec leverages LLMs to reason over item reviews, inferrin",
      "authors": [
        "Deogyong Kim",
        "Junseong Lee",
        "Jeongeun Lee",
        "Changhoe Kim",
        "Junguel Lee",
        "Jungseok Lee",
        "Dongha Lee"
      ],
      "published": "2026-02-25T10:14:30Z",
      "updated": "2026-02-25T10:14:30Z",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21756v1",
      "abs_url": "https://arxiv.org/abs/2602.21756v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.21669v1",
      "title": "DWA-KD: Dual-Space Weighting and Time-Warped Alignment for Cross-Tokenizer Knowledge Distillation",
      "summary": "Knowledge Distillation (KD) has emerged as a crucial technique for compressing Large Language Models (LLMs). Although existing cross-tokenizer KD methods have made notable progress, their effectiveness remains constrained by suboptimal alignment across sequence and vocabulary levels. To address these limitations, we introduce Dual-Space Weighting and Time-Warped Alignment (DWA-KD), a novel cross-tokenizer distillation framework that enhances token-wise distillation through dual-space entropy-based weighting and achieves precise sequence-level alignment by leveraging both lexical and semantic information. At the token level, DWA-KD maps teacher representations into the student space and vice versa, performing dual-space KD via Kullback-Leibler divergence (KL). The process is modulated by du",
      "authors": [
        "Duc Trung Vu",
        "Pham Khanh Chi",
        "Dat Phi Van",
        "Linh Ngo Van",
        "Sang Dinh",
        "Trung Le"
      ],
      "published": "2026-02-25T08:04:44Z",
      "updated": "2026-02-25T08:04:44Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21669v1",
      "abs_url": "https://arxiv.org/abs/2602.21669v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "rag",
        "alignment"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.21633v1",
      "title": "Self-Correcting VLA: Online Action Refinement via Sparse World Imagination",
      "summary": "Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive",
      "authors": [
        "Chenyv Liu",
        "Wentao Tan",
        "Lei Zhu",
        "Fengling Li",
        "Jingjing Li",
        "Guoli Yang",
        "Heng Tao Shen"
      ],
      "published": "2026-02-25T06:58:06Z",
      "updated": "2026-02-25T06:58:06Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21633v1",
      "abs_url": "https://arxiv.org/abs/2602.21633v1",
      "relevance_score": 8,
      "matched_keywords": [
        "agent",
        "benchmark",
        "evaluation",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.21608v1",
      "title": "MixSarc: A Bangla-English Code-Mixed Corpus for Implicit Meaning Identification",
      "summary": "Bangla-English code-mixing is widespread across South Asian social media, yet resources for implicit meaning identification in this setting remain scarce. Existing sentiment and sarcasm models largely focus on monolingual English or high-resource languages and struggle with transliteration variation, cultural references, and intra-sentential language switching. To address this gap, we introduce MixSarc, the first publicly available Bangla-English code-mixed corpus for implicit meaning identification. The dataset contains 9,087 manually annotated sentences labeled for humor, sarcasm, offensiveness, and vulgarity. We construct the corpus through targeted social media collection, systematic filtering, and multi-annotator validation. We benchmark transformer-based models and evaluate zero-shot",
      "authors": [
        "Kazi Samin Yasar Alam",
        "Md Tanbir Chowdhury",
        "Tamim Ahmed",
        "Ajwad Abrar",
        "Md Rafid Haque"
      ],
      "published": "2026-02-25T06:12:06Z",
      "updated": "2026-02-25T06:12:06Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21608v1",
      "abs_url": "https://arxiv.org/abs/2602.21608v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "language model",
        "transformer",
        "prompt",
        "benchmark",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.21597v1",
      "title": "NGDB-Zoo: Towards Efficient and Scalable Neural Graph Databases Training",
      "summary": "Neural Graph Databases (NGDBs) facilitate complex logical reasoning over incomplete knowledge structures, yet their training efficiency and expressivity are constrained by rigid query-level batching and structure-exclusive embeddings. We present NGDB-Zoo, a unified framework that resolves these bottlenecks by synergizing operator-level training with semantic augmentation. By decoupling logical operators from query topologies, NGDB-Zoo transforms the training loop into a dynamically scheduled data-flow execution, enabling multi-stream parallelism and achieving a $1.8\\times$ - $6.8\\times$ throughput compared to baselines. Furthermore, we formalize a decoupled architecture to integrate high-dimensional semantic priors from Pre-trained Text Encoders (PTEs) without triggering I/O stalls or memo",
      "authors": [
        "Zhongwei Xie",
        "Jiaxin Bai",
        "Shujie Liu",
        "Haoyu Huang",
        "Yufei Li",
        "Yisen Gao",
        "Hong Ting Tsang",
        "Yangqiu Song"
      ],
      "published": "2026-02-25T05:46:42Z",
      "updated": "2026-02-25T05:46:42Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21597v1",
      "abs_url": "https://arxiv.org/abs/2602.21597v1",
      "relevance_score": 8,
      "matched_keywords": [
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.21515v1",
      "title": "Training Generalizable Collaborative Agents via Strategic Risk Aversion",
      "summary": "Many emerging agentic paradigms require agents to collaborate with one another (or people) to achieve shared goals. Unfortunately, existing approaches to learning policies for such collaborative problems produce brittle solutions that fail when paired with new partners. We attribute these failures to a combination of free-riding during training and a lack of strategic robustness. To address these problems, we study the concept of strategic risk aversion and interpret it as a principled inductive bias for generalizable cooperation with unseen partners. While strategically risk-averse players are robust to deviations in their partner's behavior by design, we show that, in collaborative games, they also (1) can have better equilibrium outcomes than those at classical game-theoretic concepts l",
      "authors": [
        "Chengrui Qu",
        "Yizhou Zhang",
        "Nicholas Lanzetti",
        "Eric Mazumdar"
      ],
      "published": "2026-02-25T03:06:59Z",
      "updated": "2026-02-25T03:06:59Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21515v1",
      "abs_url": "https://arxiv.org/abs/2602.21515v1",
      "relevance_score": 8,
      "matched_keywords": [
        "llm",
        "agent",
        "agentic",
        "benchmark"
      ],
      "category": "agents"
    },
    {
      "id": "2602.21480v1",
      "title": "Both Ends Count! Just How Good are LLM Agents at \"Text-to-Big SQL\"?",
      "summary": "Text-to-SQL and Big Data are both extensively benchmarked fields, yet there is limited research that evaluates them jointly. In the real world, Text-to-SQL systems are often embedded with Big Data workflows, such as large-scale data processing or interactive data analytics. We refer to this as \"Text-to-Big SQL\". However, existing text-to-SQL benchmarks remain narrowly scoped and overlook the cost and performance implications that arise at scale. For instance, translation errors that are minor on small datasets lead to substantial cost and latency overheads as data scales, a relevant issue completely ignored by text-to-SQL metrics. In this paper, we overcome this overlooked challenge by introducing novel and representative metrics for evaluating Text-to-Big SQL. Our study focuses on product",
      "authors": [
        "Germ√°n T. Eizaguirre",
        "Lars Tissen",
        "Marc S√°nchez-Artigas"
      ],
      "published": "2026-02-25T01:12:35Z",
      "updated": "2026-02-25T01:12:35Z",
      "categories": [
        "cs.DB",
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21480v1",
      "abs_url": "https://arxiv.org/abs/2602.21480v1",
      "relevance_score": 8,
      "matched_keywords": [
        "llm",
        "agent",
        "benchmark",
        "evaluation"
      ],
      "category": "agents"
    },
    {
      "id": "2602.21442v1",
      "title": "MINAR: Mechanistic Interpretability for Neural Algorithmic Reasoning",
      "summary": "The recent field of neural algorithmic reasoning (NAR) studies the ability of graph neural networks (GNNs) to emulate classical algorithms like Bellman-Ford, a phenomenon known as algorithmic alignment. At the same time, recent advances in large language models (LLMs) have spawned the study of mechanistic interpretability, which aims to identify granular model components like circuits that perform specific computations. In this work, we introduce Mechanistic Interpretability for Neural Algorithmic Reasoning (MINAR), an efficient circuit discovery toolbox that adapts attribution patching methods from mechanistic interpretability to the GNN setting. We show through two case studies that MINAR recovers faithful neuron-level circuits from GNNs trained on algorithmic tasks. Our study sheds new ",
      "authors": [
        "Jesse He",
        "Helen Jenne",
        "Max Vargas",
        "Davis Brown",
        "Gal Mishne",
        "Yusu Wang",
        "Henry Kvinge"
      ],
      "published": "2026-02-24T23:38:06Z",
      "updated": "2026-02-24T23:38:06Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21442v1",
      "abs_url": "https://arxiv.org/abs/2602.21442v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "reasoning",
        "reasoning",
        "alignment"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.21428v1",
      "title": "PSF-Med: Measuring and Explaining Paraphrase Sensitivity in Medical Vision Language Models",
      "summary": "Medical Vision Language Models (VLMs) can change their answers when clinicians rephrase the same question, which raises deployment risks. We introduce Paraphrase Sensitivity Failure (PSF)-Med, a benchmark of 19,748 chest Xray questions paired with about 92,000 meaningpreserving paraphrases across MIMIC-CXR and PadChest. Across six medical VLMs, we measure yes/no flips for the same image and find flip rates from 8% to 58%. However, low flip rate does not imply visual grounding: text-only baselines show that some models stay consistent even when the image is removed, suggesting they rely on language priors. To study mechanisms in one model, we apply GemmaScope 2 Sparse Autoencoders (SAEs) to MedGemma 4B and analyze FlipBank, a curated set of 158 flip cases. We identify a sparse feature at la",
      "authors": [
        "Binesh Sadanandan",
        "Vahid Behzadan"
      ],
      "published": "2026-02-24T23:03:50Z",
      "updated": "2026-02-24T23:03:50Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21428v1",
      "abs_url": "https://arxiv.org/abs/2602.21428v1",
      "relevance_score": 8,
      "matched_keywords": [
        "language model",
        "prompt",
        "benchmark",
        "evaluation",
        "rag",
        "vision language"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.21397v1",
      "title": "MMLoP: Multi-Modal Low-Rank Prompting for Efficient Vision-Language Adaptation",
      "summary": "Prompt learning has become a dominant paradigm for adapting vision-language models (VLMs) such as CLIP to downstream tasks without modifying pretrained weights. While extending prompts to both vision and text encoders across multiple transformer layers significantly boosts performance, it dramatically increases the number of trainable parameters, with state-of-the-art methods requiring millions of parameters and abandoning the parameter efficiency that makes prompt tuning attractive. In this work, we propose \\textbf{MMLoP} (\\textbf{M}ulti-\\textbf{M}odal \\textbf{Lo}w-Rank \\textbf{P}rompting), a framework that achieves deep multi-modal prompting with only \\textbf{11.5K trainable parameters}, comparable to early text-only methods like CoOp. MMLoP parameterizes vision and text prompts at each ",
      "authors": [
        "Sajjad Ghiasvand",
        "Haniyeh Ehsani Oskouie",
        "Mahnoosh Alizadeh",
        "Ramtin Pedarsani"
      ],
      "published": "2026-02-24T22:00:34Z",
      "updated": "2026-02-24T22:00:34Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21397v1",
      "abs_url": "https://arxiv.org/abs/2602.21397v1",
      "relevance_score": 8,
      "matched_keywords": [
        "language model",
        "transformer",
        "prompt",
        "few-shot",
        "benchmark",
        "alignment"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.22207v1",
      "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets",
      "summary": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localizati",
      "authors": [
        "Hanna Yukhymenko",
        "Anton Alexandrov",
        "Martin Vechev"
      ],
      "published": "2026-02-25T18:58:25Z",
      "updated": "2026-02-25T18:58:25Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22207v1",
      "abs_url": "https://arxiv.org/abs/2602.22207v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "evaluation"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.22124v1",
      "title": "SWE-Prot√©g√©: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents",
      "summary": "Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Prot√©g√©, a post-training framework that reframes software repair as an expert-prot√©g√© collaboration problem. In SWE-Prot√©g√©, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We ligh",
      "authors": [
        "Patrick Tser Jern Kon",
        "Archana Pradeep",
        "Ang Chen",
        "Alexander P. Ellis",
        "Warren Hunt",
        "Zijian Wang",
        "John Yang",
        "Samuel Thompson"
      ],
      "published": "2026-02-25T17:11:49Z",
      "updated": "2026-02-25T17:11:49Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22124v1",
      "abs_url": "https://arxiv.org/abs/2602.22124v1",
      "relevance_score": 7,
      "matched_keywords": [
        "language model",
        "agent",
        "agentic",
        "fine-tuning",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.21950v1",
      "title": "MEDSYN: Benchmarking Multi-EviDence SYNthesis in Complex Clinical Cases for Multimodal Large Language Models",
      "summary": "Multimodal large language models (MLLMs) have shown great potential in medical applications, yet existing benchmarks inadequately capture real-world clinical complexity. We introduce MEDSYN, a multilingual, multimodal benchmark of highly complex clinical cases with up to 7 distinct visual clinical evidence (CE) types per case. Mirroring clinical workflow, we evaluate 18 MLLMs on differential diagnosis (DDx) generation and final diagnosis (FDx) selection. While top models often match or even outperform human experts on DDx generation, all MLLMs exhibit a much larger DDx--FDx performance gap compared to expert clinicians, indicating a failure mode in synthesis of heterogeneous CE types. Ablations attribute this failure to (i) overreliance on less discriminative textual CE ($\\it{e.g.}$, medic",
      "authors": [
        "Boqi Chen",
        "Xudong Liu",
        "Jiachuan Peng",
        "Marianne Frey-Marti",
        "Bang Zheng",
        "Kyle Lam",
        "Lin Li",
        "Jianing Qiu"
      ],
      "published": "2026-02-25T14:33:33Z",
      "updated": "2026-02-25T14:33:33Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21950v1",
      "abs_url": "https://arxiv.org/abs/2602.21950v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "multimodal"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.21680v1",
      "title": "Hierarchical Lead Critic based Multi-Agent Reinforcement Learning",
      "summary": "Cooperative Multi-Agent Reinforcement Learning (MARL) solves complex tasks that require coordination from multiple agents, but is often limited to either local (independent learning) or global (centralized learning) perspectives. In this paper, we introduce a novel sequential training scheme and MARL architecture, which learns from multiple perspectives on different hierarchy levels. We propose the Hierarchical Lead Critic (HLC) - inspired by natural emerging distributions in team structures, where following high-level objectives combines with low-level execution. HLC demonstrates that introducing multiple hierarchies, leveraging local and global perspectives, can lead to improved performance with high sample efficiency and robust policies. Experimental results conducted on cooperative, no",
      "authors": [
        "David Eckel",
        "Henri Mee√ü"
      ],
      "published": "2026-02-25T08:33:39Z",
      "updated": "2026-02-25T08:33:39Z",
      "categories": [
        "cs.LG",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21680v1",
      "abs_url": "https://arxiv.org/abs/2602.21680v1",
      "relevance_score": 7,
      "matched_keywords": [
        "agent",
        "benchmark",
        "rag"
      ],
      "category": "agents"
    }
  ],
  "recent_papers": [
    {
      "id": "2602.22188v1",
      "title": "Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach",
      "summary": "Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. As a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. Four of these are reduced-order models (ROM) based on one neural network for compression and another for prediction. The other four are single neural networks wi",
      "authors": [
        "Nathalie C. Pinheiro",
        "Donghu Guo",
        "Hannah P. Menke",
        "Aniket C. Joshi",
        "Claire E. Heaney",
        "Ahmed H. ElSheikh",
        "Christopher C. Pain"
      ],
      "published": "2026-02-25T18:34:03Z",
      "updated": "2026-02-25T18:34:03Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.flu-dyn"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22188v1",
      "abs_url": "https://arxiv.org/abs/2602.22188v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.22149v1",
      "title": "Enhancing Framingham Cardiovascular Risk Score Transparency through Logic-Based XAI",
      "summary": "Cardiovascular disease (CVD) remains one of the leading global health challenges, accounting for more than 19 million deaths worldwide. To address this, several tools that aim to predict CVD risk and support clinical decision making have been developed. In particular, the Framingham Risk Score (FRS) is one of the most widely used and recommended worldwide. However, it does not explain why a patient was assigned to a particular risk category nor how it can be reduced. Due to this lack of transparency, we present a logical explainer for the FRS. Based on first-order logic and explainable artificial intelligence (XAI) fundaments, the explainer is capable of identifying a minimal set of patient attributes that are sufficient to explain a given risk classification. Our explainer also produces a",
      "authors": [
        "Emannuel L. de A. Bezerra",
        "Luiz H. T. Viana",
        "Vin√≠cius P. Chagas",
        "Diogo E. Rolim",
        "Thiago Alves Rocha",
        "Carlos H. L. Cavalcante"
      ],
      "published": "2026-02-25T17:58:11Z",
      "updated": "2026-02-25T17:58:11Z",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22149v1",
      "abs_url": "https://arxiv.org/abs/2602.22149v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.22130v1",
      "title": "Sample Complexity Bounds for Robust Mean Estimation with Mean-Shift Contamination",
      "summary": "We study the basic task of mean estimation in the presence of mean-shift contamination. In the mean-shift contamination model, an adversary is allowed to replace a small constant fraction of the clean samples by samples drawn from arbitrarily shifted versions of the base distribution. Prior work characterized the sample complexity of this task for the special cases of the Gaussian and Laplace distributions. Specifically, it was shown that consistent estimation is possible in these cases, a property that is provably impossible in Huber's contamination model. An open question posed in earlier work was to determine the sample complexity of mean estimation in the mean-shift contamination model for general base distributions. In this work, we study and essentially resolve this open question. Sp",
      "authors": [
        "Ilias Diakonikolas",
        "Giannis Iakovidis",
        "Daniel M. Kane",
        "Sihan Liu"
      ],
      "published": "2026-02-25T17:21:23Z",
      "updated": "2026-02-25T17:21:23Z",
      "categories": [
        "cs.LG",
        "cs.DS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22130v1",
      "abs_url": "https://arxiv.org/abs/2602.22130v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.22122v1",
      "title": "Probing the Geometry of Diffusion Models with the String Method",
      "summary": "Understanding the geometry of learned distributions is fundamental to improving and interpreting diffusion models, yet systematic tools for exploring their landscape remain limited. Standard latent-space interpolations fail to respect the structure of the learned distribution, often traversing low-density regions. We introduce a framework based on the string method that computes continuous paths between samples by evolving curves under the learned score function. Operating on pretrained models without retraining, our approach interpolates between three regimes: pure generative transport, which yields continuous sample paths; gradient-dominated dynamics, which recover minimum energy paths (MEPs); and finite-temperature string dynamics, which compute principal curves -- self-consistent paths",
      "authors": [
        "Elio Moreau",
        "Florentin Coeurdoux",
        "Gr√©goire Ferre",
        "Eric Vanden-Eijnden"
      ],
      "published": "2026-02-25T17:10:59Z",
      "updated": "2026-02-25T17:10:59Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22122v1",
      "abs_url": "https://arxiv.org/abs/2602.22122v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.22101v1",
      "title": "On Imbalanced Regression with Hoeffding Trees",
      "summary": "Many real-world applications provide a continuous stream of data that is subsequently used by machine learning models to solve regression tasks of interest. Hoeffding trees and their variants have a long-standing tradition due to their effectiveness, either alone or as base models in broader ensembles. At the same time a recent line of work in batch learning has shown that kernel density estimation (KDE) is an effective approach for smoothed predictions in imbalanced regression tasks [Yang et al., 2021]. Moreover, another recent line of work for batch learning, called hierarchical shrinkage (HS) [Agarwal et al., 2022], has introduced a post-hoc regularization method for decision trees that does not alter the structure of the learned tree. Using a telescoping argument we cast KDE to streami",
      "authors": [
        "Pantia-Marina Alchirch",
        "Dimitrios I. Diochnos"
      ],
      "published": "2026-02-25T16:48:07Z",
      "updated": "2026-02-25T16:48:07Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22101v1",
      "abs_url": "https://arxiv.org/abs/2602.22101v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.22086v1",
      "title": "MBD-ML: Many-body dispersion from machine learning for molecules and materials",
      "summary": "Van der Waals (vdW) interactions are essential for describing molecules and materials, from drug design and catalysis to battery applications. These omnipresent interactions must also be accurately included in machine-learned force fields. The many-body dispersion (MBD) method stands out as one of the most accurate and transferable approaches to capture vdW interactions, requiring only atomic $C_6$ coefficients and polarizabilities as input. We present MBD-ML, a pretrained message passing neural network that predicts these atomic properties directly from atomic structures. Through seamless integration with libMBD, our method enables the immediate calculation of MBD-inclusive total energies, forces, and stress tensors. By eliminating the need for intermediate electronic structure calculatio",
      "authors": [
        "Evgeny Moerman",
        "Adil Kabylda",
        "Almaz Khabibrakhmanov",
        "Alexandre Tkatchenko"
      ],
      "published": "2026-02-25T16:34:53Z",
      "updated": "2026-02-25T16:34:53Z",
      "categories": [
        "physics.chem-ph",
        "cond-mat.mtrl-sci",
        "cs.LG",
        "physics.comp-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22086v1",
      "abs_url": "https://arxiv.org/abs/2602.22086v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.22061v1",
      "title": "Learning Quantum Data Distribution via Chaotic Quantum Diffusion Model",
      "summary": "Generative models for quantum data pose significant challenges but hold immense potential in fields such as chemoinformatics and quantum physics. Quantum denoising diffusion probabilistic models (QuDDPMs) enable efficient learning of quantum data distributions by progressively scrambling and denoising quantum states; however, existing implementations typically rely on circuit-based random unitary dynamics that can be costly to realize and sensitive to control imperfections, particularly on analog quantum hardware. We propose the chaotic quantum diffusion model, a framework that generates projected ensembles via chaotic Hamiltonian time evolution, providing a flexible and hardware-compatible diffusion mechanism. Requiring only global, time-independent control, our approach substantially red",
      "authors": [
        "Quoc Hoan Tran",
        "Koki Chinzei",
        "Yasuhiro Endo",
        "Hirotaka Oshima"
      ],
      "published": "2026-02-25T16:09:50Z",
      "updated": "2026-02-25T16:09:50Z",
      "categories": [
        "quant-ph",
        "cs.LG",
        "nlin.CD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22061v1",
      "abs_url": "https://arxiv.org/abs/2602.22061v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.22059v1",
      "title": "NESTOR: A Nested MOE-based Neural Operator for Large-Scale PDE Pre-Training",
      "summary": "Neural operators have emerged as an efficient paradigm for solving PDEs, overcoming the limitations of traditional numerical methods and significantly improving computational efficiency. However, due to the diversity and complexity of PDE systems, existing neural operators typically rely on a single network architecture, which limits their capacity to fully capture heterogeneous features and complex system dependencies. This constraint poses a bottleneck for large-scale PDE pre-training based on neural operators. To address these challenges, we propose a large-scale PDE pre-trained neural operator based on a nested Mixture-of-Experts (MoE) framework. In particular, the image-level MoE is designed to capture global dependencies, while the token-level Sub-MoE focuses on local dependencies. O",
      "authors": [
        "Dengdi Sun",
        "Xiaoya Zhou",
        "Xiao Wang",
        "Hao Si",
        "Wanli Lyu",
        "Jin Tang",
        "Bin Luo"
      ],
      "published": "2026-02-25T16:08:46Z",
      "updated": "2026-02-25T16:08:46Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22059v1",
      "abs_url": "https://arxiv.org/abs/2602.22059v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.22056v1",
      "title": "FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation",
      "summary": "Generative manipulation policies can fail catastrophically under deployment-time distribution shift, yet many failures are near-misses: the robot reaches almost-correct poses and would succeed with a small corrective motion. We present FlowCorrect, a deployment-time correction framework that converts near-miss failures into successes using sparse human nudges, without full policy retraining. During execution, a human provides brief corrective pose nudges via a lightweight VR interface. FlowCorrect uses these sparse corrections to locally adapt the policy, improving actions without retraining the backbone while preserving the model performance on previously learned scenarios. We evaluate on a real-world robot across three tabletop tasks: pick-and-place, pouring, and cup uprighting. With a l",
      "authors": [
        "Edgar Welte",
        "Yitian Shi",
        "Rosa Wolf",
        "Maximillian Gilles",
        "Rania Rayyes"
      ],
      "published": "2026-02-25T16:06:49Z",
      "updated": "2026-02-25T16:06:49Z",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22056v1",
      "abs_url": "https://arxiv.org/abs/2602.22056v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.22055v1",
      "title": "Physics-Informed Machine Learning for Vessel Shaft Power and Fuel Consumption Prediction: Interpretable KAN-based Approach",
      "summary": "Accurate prediction of shaft rotational speed, shaft power, and fuel consumption is crucial for enhancing operational efficiency and sustainability in maritime transportation. Conventional physics-based models provide interpretability but struggle with real-world variability, while purely data-driven approaches achieve accuracy at the expense of physical plausibility. This paper introduces a Physics-Informed Kolmogorov-Arnold Network (PI-KAN), a hybrid method that integrates interpretable univariate feature transformations with a physics-informed loss function and a leakage-free chained prediction pipeline. Using operational and environmental data from five cargo vessels, PI-KAN consistently outperforms the traditional polynomial method and neural network baselines. The model achieves the ",
      "authors": [
        "Hamza Haruna Mohammed",
        "Dusica Marijan",
        "Arnbj√∏rn Maressa"
      ],
      "published": "2026-02-25T16:06:28Z",
      "updated": "2026-02-25T16:06:28Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22055v1",
      "abs_url": "https://arxiv.org/abs/2602.22055v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.22045v1",
      "title": "DLT-Corpus: A Large-Scale Text Collection for the Distributed Ledger Technology Domain",
      "summary": "We introduce DLT-Corpus, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: 2.98 billion tokens from 22.12 million documents spanning scientific literature (37,440 publications), United States Patent and Trademark Office (USPTO) patents (49,023 filings), and social media (22 million posts). Existing Natural Language Processing (NLP) resources for DLT focus narrowly on cryptocurrencies price prediction and smart contracts, leaving domain-specific language under explored despite the sector's ~$3 trillion market capitalization and rapid technological evolution. We demonstrate DLT-Corpus' utility by analyzing technology emergence patterns and market-innovation correlations. Findings reveal that technologies originate in scientific literature b",
      "authors": [
        "Walter Hernandez Cruz",
        "Peter Devine",
        "Nikhil Vadgama",
        "Paolo Tasca",
        "Jiahua Xu"
      ],
      "published": "2026-02-25T15:53:41Z",
      "updated": "2026-02-25T15:53:41Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22045v1",
      "abs_url": "https://arxiv.org/abs/2602.22045v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.22039v1",
      "title": "TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition",
      "summary": "Low-resource automatic speech recognition (ASR) continues to pose significant challenges, primarily due to the limited availability of transcribed data for numerous languages. While a wealth of spoken content is accessible in television dramas and online videos, Taiwanese Hokkien exemplifies this issue, with transcriptions often being scarce and the majority of available subtitles provided only in Mandarin. To address this deficiency, we introduce TG-ASR for Taiwanese Hokkien drama speech recognition, a translation-guided ASR framework that utilizes multilingual translation embeddings to enhance recognition performance in low-resource environments. The framework is centered around the parallel gated cross-attention (PGCA) mechanism, which adaptively integrates embeddings from various auxil",
      "authors": [
        "Cheng-Yeh Yang",
        "Chien-Chun Wang",
        "Li-Wei Chen",
        "Hung-Shin Lee",
        "Hsin-Min Wang",
        "Berlin Chen"
      ],
      "published": "2026-02-25T15:47:34Z",
      "updated": "2026-02-25T15:47:34Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22039v1",
      "abs_url": "https://arxiv.org/abs/2602.22039v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.22018v1",
      "title": "Disease Progression and Subtype Modeling for Combined Discrete and Continuous Input Data",
      "summary": "Disease progression modeling provides a robust framework to identify long-term disease trajectories from short-term biomarker data. It is a valuable tool to gain a deeper understanding of diseases with a long disease trajectory, such as Alzheimer's disease. A key limitation of most disease progression models is that they are specific to a single data type (e.g., continuous data), thereby limiting their applicability to heterogeneous, real-world datasets. To address this limitation, we propose the Mixed Events model, a novel disease progression model that handles both discrete and continuous data types. This model is implemented within the Subtype and Stage Inference (SuStaIn) framework, resulting in Mixed-SuStaIn, enabling subtype and progression modeling. We demonstrate the effectiveness ",
      "authors": [
        "Sterre de Jonge",
        "Elisabeth J. Vinke",
        "Meike W. Vernooij",
        "Daniel C. Alexander",
        "Alexandra L. Young",
        "Esther E. Bron"
      ],
      "published": "2026-02-25T15:31:30Z",
      "updated": "2026-02-25T15:31:30Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22018v1",
      "abs_url": "https://arxiv.org/abs/2602.22018v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.22015v1",
      "title": "Function-Space Empirical Bayes Regularisation with Student's t Priors",
      "summary": "Bayesian deep learning (BDL) has emerged as a principled approach to produce reliable uncertainty estimates by integrating deep neural networks with Bayesian inference, and the selection of informative prior distributions remains a significant challenge. Various function-space variational inference (FSVI) regularisation methods have been presented, assigning meaningful priors over model predictions. However, these methods typically rely on a Gaussian prior, which fails to capture the heavy-tailed statistical characteristics inherent in neural network outputs. By contrast, this work proposes a novel function-space empirical Bayes regularisation framework -- termed ST-FS-EB -- which employs heavy-tailed Student's $t$ priors in both parameter and function spaces. Also, we approximate the post",
      "authors": [
        "Pengcheng Hao",
        "Ercan Engin Kuruoglu"
      ],
      "published": "2026-02-25T15:29:44Z",
      "updated": "2026-02-25T15:29:44Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22015v1",
      "abs_url": "https://arxiv.org/abs/2602.22015v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.22003v1",
      "title": "Neural solver for Wasserstein Geodesics and optimal transport dynamics",
      "summary": "In recent years, the machine learning community has increasingly embraced the optimal transport (OT) framework for modeling distributional relationships. In this work, we introduce a sample-based neural solver for computing the Wasserstein geodesic between a source and target distribution, along with the associated velocity field. Building on the dynamical formulation of the optimal transport (OT) problem, we recast the constrained optimization as a minimax problem, using deep neural networks to approximate the relevant functions. This approach not only provides the Wasserstein geodesic but also recovers the OT map, enabling direct sampling from the target distribution. By estimating the OT map, we obtain velocity estimates along particle trajectories, which in turn allow us to learn the f",
      "authors": [
        "Hailiang Liu",
        "Yan-Han Chen"
      ],
      "published": "2026-02-25T15:21:24Z",
      "updated": "2026-02-25T15:21:24Z",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.22003v1",
      "abs_url": "https://arxiv.org/abs/2602.22003v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.21965v1",
      "title": "Compact Circulant Layers with Spectral Priors",
      "summary": "Critical applications in areas such as medicine, robotics and autonomous systems require compact (i.e., memory efficient), uncertainty-aware neural networks suitable for edge and other resource-constrained deployments. We study compact spectral circulant and block-circulant-with-circulant-blocks (BCCB) layers: FFT-diagonalizable circular convolutions whose weights live directly in the real FFT (RFFT) half (1D) or half-plane (2D). Parameterizing filters in the frequency domain lets us impose simple spectral structure, perform structured variational inference in a low-dimensional weight space, and calculate exact layer spectral norms, enabling inexpensive global Lipschitz bounds and margin-based robustness diagnostics. By placing independent complex Gaussians on the Hermitian support we obta",
      "authors": [
        "Joseph Margaryan",
        "Thomas Hamelryck"
      ],
      "published": "2026-02-25T14:48:25Z",
      "updated": "2026-02-25T14:48:25Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21965v1",
      "abs_url": "https://arxiv.org/abs/2602.21965v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.21961v1",
      "title": "Robustness in sparse artificial neural networks trained with adaptive topology",
      "summary": "We investigate the robustness of sparse artificial neural networks trained with adaptive topology. We focus on a simple yet effective architecture consisting of three sparse layers with 99% sparsity followed by a dense layer, applied to image classification tasks such as MNIST and Fashion MNIST. By updating the topology of the sparse layers between each epoch, we achieve competitive accuracy despite the significantly reduced number of weights. Our primary contribution is a detailed analysis of the robustness of these networks, exploring their performance under various perturbations including random link removal, adversarial attack, and link weight shuffling. Through extensive experiments, we demonstrate that adaptive topology not only enhances efficiency but also maintains robustness. This",
      "authors": [
        "Bendeg√∫z Sulyok",
        "Gergely Palla",
        "Filippo Radicchi",
        "Santo Fortunato"
      ],
      "published": "2026-02-25T14:44:15Z",
      "updated": "2026-02-25T14:44:15Z",
      "categories": [
        "cs.LG",
        "physics.soc-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21961v1",
      "abs_url": "https://arxiv.org/abs/2602.21961v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.21959v1",
      "title": "Estimation and Optimization of Ship Fuel Consumption in Maritime: Review, Challenges and Future Directions",
      "summary": "To reduce carbon emissions and minimize shipping costs, improving the fuel efficiency of ships is crucial. Various measures are taken to reduce the total fuel consumption of ships, including optimizing vessel parameters and selecting routes with the lowest fuel consumption. Different estimation methods are proposed for predicting fuel consumption, while various optimization methods are proposed to minimize fuel oil consumption. This paper provides a comprehensive review of methods for estimating and optimizing fuel oil consumption in maritime transport. Our novel contributions include categorizing fuel oil consumption \\&amp; estimation methods into physics-based, machine-learning, and hybrid models, exploring their strengths and limitations. Furthermore, we highlight the importance of data",
      "authors": [
        "Dusica Marijan",
        "Hamza Haruna Mohammed",
        "Bakht Zaman"
      ],
      "published": "2026-02-25T14:41:07Z",
      "updated": "2026-02-25T14:41:07Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21959v1",
      "abs_url": "https://arxiv.org/abs/2602.21959v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.21928v1",
      "title": "Learning Unknown Interdependencies for Decentralized Root Cause Analysis in Nonlinear Dynamical Systems",
      "summary": "Root cause analysis (RCA) in networked industrial systems, such as supply chains and power networks, is notoriously difficult due to unknown and dynamically evolving interdependencies among geographically distributed clients. These clients represent heterogeneous physical processes and industrial assets equipped with sensors that generate large volumes of nonlinear, high-dimensional, and heterogeneous IoT data. Classical RCA methods require partial or full knowledge of the system's dependency graph, which is rarely available in these complex networks. While federated learning (FL) offers a natural framework for decentralized settings, most existing FL methods assume homogeneous feature spaces and retrainable client models. These assumptions are not compatible with our problem setting. Diff",
      "authors": [
        "Ayush Mohanty",
        "Paritosh Ramanan",
        "Nagi Gebraeel"
      ],
      "published": "2026-02-25T14:05:38Z",
      "updated": "2026-02-25T14:05:38Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21928v1",
      "abs_url": "https://arxiv.org/abs/2602.21928v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.21926v1",
      "title": "Bridging Through Absence: How Comeback Researchers Bridge Knowledge Gaps Through Structural Re-emergence",
      "summary": "Understanding the role of researchers who return to academia after prolonged inactivity, termed \"comeback researchers\", is crucial for developing inclusive models of scientific careers. This study investigates the structural and semantic behaviors of comeback researchers, focusing on their role in cross-disciplinary knowledge transfer and network reintegration. Using the AMiner citation dataset, we analyze 113,637 early-career researchers and identify 1,425 comeback cases based on a three-year-or-longer publication gap followed by renewed activity. We find that comeback researchers cite 126% more distinct communities and exhibit 7.6% higher bridging scores compared to dropouts. They also demonstrate 74% higher gap entropy, reflecting more irregular yet strategically impactful publication t",
      "authors": [
        "Somyajit Chakraborty",
        "Angshuman Jana",
        "Avijit Gayen"
      ],
      "published": "2026-02-25T14:04:03Z",
      "updated": "2026-02-25T14:04:03Z",
      "categories": [
        "cs.SI",
        "cs.DL",
        "cs.LG",
        "physics.soc-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.21926v1",
      "abs_url": "https://arxiv.org/abs/2602.21926v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    }
  ]
}