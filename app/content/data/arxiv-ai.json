{
  "scraped_at": "2026-02-18T06:40:06.255Z",
  "categories_searched": [
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "stats": {
    "total_fetched": 200,
    "relevant_count": 50,
    "recent_count": 20,
    "by_category": {
      "agents": 14,
      "code-generation": 15,
      "benchmarks": 17,
      "training": 1,
      "reasoning": 3
    }
  },
  "relevant_papers": [
    {
      "id": "2602.15724v1",
      "title": "Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation",
      "summary": "Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embed",
      "authors": [
        "Shutian Gu",
        "Chengkai Huang",
        "Ruoyu Wang",
        "Lina Yao"
      ],
      "published": "2026-02-17T17:00:11Z",
      "updated": "2026-02-17T17:00:11Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15724v1",
      "abs_url": "https://arxiv.org/abs/2602.15724v1",
      "relevance_score": 15,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "fine-tuning",
        "prompt",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15513v1",
      "title": "Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling",
      "summary": "Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle in non-stationary environments. In this work, we propose a non-parametric memory framework that explicitly disentangles episodic and semantic memory for embodied exploration and question answering. Our retrieval-first, reasoning-assisted paradigm recalls episodic experiences via semantic similarity and verifies them through visual reasoning, enabling robust reuse of past observations without rigid geometric alignment. In parallel, we introduce a program-style rule extraction mechanism that conv",
      "authors": [
        "Ji Li",
        "Jing Xia",
        "Mingyi Li",
        "Shiyan Hu"
      ],
      "published": "2026-02-17T11:41:28Z",
      "updated": "2026-02-17T11:41:28Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15513v1",
      "abs_url": "https://arxiv.org/abs/2602.15513v1",
      "relevance_score": 15,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "benchmark",
        "reasoning",
        "reasoning",
        "multimodal",
        "alignment"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15763v1",
      "title": "GLM-5: from Vibe Coding to Agentic Engineering",
      "summary": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benc",
      "authors": [
        "GLM-5 Team",
        ":",
        "Aohan Zeng",
        "Xin Lv",
        "Zhenyu Hou",
        "Zhengxiao Du",
        "Qinkai Zheng",
        "Bin Chen",
        "Da Yin",
        "Chendi Ge",
        "Chengxing Xie",
        "Cunxiang Wang",
        "Gengzheng Pan",
        "Hao Zeng",
        "Haoke Zhang",
        "Haoran Wang",
        "Huilong Chen",
        "Jiajie Zhang",
        "Jian Jiao",
        "Jiaqi Guo",
        "Jingsen Wang",
        "Jingzhao Du",
        "Jinzhu Wu",
        "Kedong Wang",
        "Lei Li",
        "Lin Fan",
        "Lucen Zhong",
        "Mingdao Liu",
        "Mingming Zhao",
        "Pengfan Du",
        "Qian Dong",
        "Rui Lu",
        "Shuang-Li",
        "Shulin Cao",
        "Song Liu",
        "Ting Jiang",
        "Xiaodong Chen",
        "Xiaohan Zhang",
        "Xuancheng Huang",
        "Xuezhen Dong",
        "Yabo Xu",
        "Yao Wei",
        "Yifan An",
        "Yilin Niu",
        "Yitong Zhu",
        "Yuanhao Wen",
        "Yukuo Cen",
        "Yushi Bai",
        "Zhongpei Qiao",
        "Zihan Wang",
        "Zikang Wang",
        "Zilin Zhu",
        "Ziqiang Liu",
        "Zixuan Li",
        "Bojie Wang",
        "Bosi Wen",
        "Can Huang",
        "Changpeng Cai",
        "Chao Yu",
        "Chen Li",
        "Chen Li",
        "Chenghua Huang",
        "Chengwei Hu",
        "Chenhui Zhang",
        "Chenzheng Zhu",
        "Congfeng Yin",
        "Daoyan Lin",
        "Dayong Yang",
        "Di Wang",
        "Ding Ai",
        "Erle Zhu",
        "Fangzhou Yi",
        "Feiyu Chen",
        "Guohong Wen",
        "Hailong Sun",
        "Haisha Zhao",
        "Haiyi Hu",
        "Hanchen Zhang",
        "Hanrui Liu",
        "Hanyu Zhang",
        "Hao Peng",
        "Hao Tai",
        "Haobo Zhang",
        "He Liu",
        "Hongwei Wang",
        "Hongxi Yan",
        "Hongyu Ge",
        "Huan Liu",
        "Huan Liu",
        "Huanpeng Chu",
        "Jia'ni Zhao",
        "Jiachen Wang",
        "Jiajing Zhao",
        "Jiamin Ren",
        "Jiapeng Wang",
        "Jiaxin Zhang",
        "Jiayi Gui",
        "Jiayue Zhao",
        "Jijie Li",
        "Jing An",
        "Jing Li",
        "Jingwei Yuan",
        "Jinhua Du",
        "Jinxin Liu",
        "Junkai Zhi",
        "Junwen Duan",
        "Kaiyue Zhou",
        "Kangjian Wei",
        "Ke Wang",
        "Keyun Luo",
        "Laiqiang Zhang",
        "Leigang Sha",
        "Liang Xu",
        "Lindong Wu",
        "Lintao Ding",
        "Lu Chen",
        "Minghao Li",
        "Nianyi Lin",
        "Pan Ta",
        "Qiang Zou",
        "Rongjun Song",
        "Ruiqi Yang",
        "Shangqing Tu",
        "Shangtong Yang",
        "Shaoxiang Wu",
        "Shengyan Zhang",
        "Shijie Li",
        "Shuang Li",
        "Shuyi Fan",
        "Wei Qin",
        "Wei Tian",
        "Weining Zhang",
        "Wenbo Yu",
        "Wenjie Liang",
        "Xiang Kuang",
        "Xiangmeng Cheng",
        "Xiangyang Li",
        "Xiaoquan Yan",
        "Xiaowei Hu",
        "Xiaoying Ling",
        "Xing Fan",
        "Xingye Xia",
        "Xinyuan Zhang",
        "Xinze Zhang",
        "Xirui Pan",
        "Xunkai Zhang",
        "Yandong Wu",
        "Yanfu Li",
        "Yidong Wang",
        "Yifan Zhu",
        "Yijun Tan",
        "Yilin Zhou",
        "Yiming Pan",
        "Ying Zhang",
        "Yinpei Su",
        "Yipeng Geng",
        "Yipeng Geng",
        "Yong Yan",
        "Yonglin Tan",
        "Yuean Bi",
        "Yuhan Shen",
        "Yuhao Yang",
        "Yujiang Li",
        "Yunan Liu",
        "Yunqing Wang",
        "Yuntao Li",
        "Yurong Wu",
        "Yutao Zhang",
        "Yuxi Duan",
        "Yuxuan Zhang",
        "Zezhen Liu",
        "Zhengtao Jiang",
        "Zhenhe Yan",
        "Zheyu Zhang",
        "Zhixiang Wei",
        "Zhuo Chen",
        "Zhuoer Feng",
        "Zijun Yao",
        "Ziwei Chai",
        "Ziyuan Wang",
        "Zuzhou Zhang",
        "Bin Xu",
        "Minlie Huang",
        "Hongning Wang",
        "Juanzi Li",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "published": "2026-02-17T17:50:56Z",
      "updated": "2026-02-17T17:50:56Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15763v1",
      "abs_url": "https://arxiv.org/abs/2602.15763v1",
      "relevance_score": 14,
      "matched_keywords": [
        "agent",
        "agentic",
        "benchmark",
        "reasoning",
        "reasoning",
        "alignment"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15449v1",
      "title": "TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models",
      "summary": "Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, ",
      "authors": [
        "Chansung Park",
        "Juyong Jiang",
        "Fan Wang",
        "Sayak Paul",
        "Jiasi Shen",
        "Jing Tang",
        "Jianguo Li"
      ],
      "published": "2026-02-17T09:29:18Z",
      "updated": "2026-02-17T09:29:18Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15449v1",
      "abs_url": "https://arxiv.org/abs/2602.15449v1",
      "relevance_score": 14,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "code generation",
        "fine-tuning",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15313v1",
      "title": "Mnemis: Dual-Route Retrieval on Hierarchical Graphs for Long-Term LLM Memory",
      "summary": "AI Memory, specifically how models organizes and retrieves historical messages, becomes increasingly valuable to Large Language Models (LLMs), yet existing methods (RAG and Graph-RAG) primarily retrieve memory through similarity-based mechanisms. While efficient, such System-1-style retrieval struggles with scenarios that require global reasoning or comprehensive coverage of all relevant information. In this work, We propose Mnemis, a novel memory framework that integrates System-1 similarity search with a complementary System-2 mechanism, termed Global Selection. Mnemis organizes memory into a base graph for similarity retrieval and a hierarchical graph that enables top-down, deliberate traversal over semantic hierarchies. By combining the complementary strength from both retrieval routes",
      "authors": [
        "Zihao Tang",
        "Xin Yu",
        "Ziyu Xiao",
        "Zengxuan Wen",
        "Zelin Li",
        "Jiaxi Zhou",
        "Hualei Wang",
        "Haohua Wang",
        "Haizhen Huang",
        "Weiwei Deng",
        "Feng Sun",
        "Qi Zhang"
      ],
      "published": "2026-02-17T02:44:03Z",
      "updated": "2026-02-17T02:44:03Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15313v1",
      "abs_url": "https://arxiv.org/abs/2602.15313v1",
      "relevance_score": 14,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15212v1",
      "title": "Secure and Energy-Efficient Wireless Agentic AI Networks",
      "summary": "In this paper, we introduce a secure wireless agentic AI network comprising one supervisor AI agent and multiple other AI agents to provision quality of service (QoS) for users' reasoning tasks while ensuring confidentiality of private knowledge and reasoning outcomes. Specifically, the supervisor AI agent can dynamically assign other AI agents to participate in cooperative reasoning, while the unselected AI agents act as friendly jammers to degrade the eavesdropper's interception performance. To extend the service duration of AI agents, an energy minimization problem is formulated that jointly optimizes AI agent selection, base station (BS) beamforming, and AI agent transmission power, subject to latency and reasoning accuracy constraints. To address the formulated problem, we propose two",
      "authors": [
        "Yuanyan Song",
        "Kezhi Wang",
        "Xinmian Xu"
      ],
      "published": "2026-02-16T21:42:33Z",
      "updated": "2026-02-16T21:42:33Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15212v1",
      "abs_url": "https://arxiv.org/abs/2602.15212v1",
      "relevance_score": 14,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "agentic",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15809v1",
      "title": "Decision Quality Evaluation Framework at Pinterest",
      "summary": "Online platforms require robust systems to enforce content safety policies at scale. A critical component of these systems is the ability to evaluate the quality of moderation decisions made by both human agents and Large Language Models (LLMs). However, this evaluation is challenging due to the inherent trade-offs between cost, scale, and trustworthiness, along with the complexity of evolving policies. To address this, we present a comprehensive Decision Quality Evaluation Framework developed and deployed at Pinterest. The framework is centered on a high-trust Golden Set (GDS) curated by subject matter experts (SMEs), which serves as a ground truth benchmark. We introduce an automated intelligent sampling pipeline that uses propensity scores to efficiently expand dataset coverage. We demo",
      "authors": [
        "Yuqi Tian",
        "Robert Paine",
        "Attila Dobi",
        "Kevin O'Sullivan",
        "Aravindh Manickavasagam",
        "Faisal Farooq"
      ],
      "published": "2026-02-17T18:45:55Z",
      "updated": "2026-02-17T18:45:55Z",
      "categories": [
        "stat.AP",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15809v1",
      "abs_url": "https://arxiv.org/abs/2602.15809v1",
      "relevance_score": 13,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "prompt",
        "benchmark",
        "evaluation",
        "rag",
        "safety"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15197v1",
      "title": "OpaqueToolsBench: Learning Nuances of Tool Behavior Through Interaction",
      "summary": "Tool-calling is essential for Large Language Model (LLM) agents to complete real-world tasks. While most existing benchmarks assume simple, perfectly documented tools, real-world tools (e.g., general \"search\" APIs) are often opaque, lacking clear best practices or failure modes. Can LLM agents improve their performance in environments with opaque tools by interacting and subsequently improving documentation? To study this, we create OpaqueToolsBench, a benchmark consisting of three distinct task-oriented environments: general function calling, interactive chess playing, and long-trajectory agentic search. Each environment provides underspecified tools that models must learn to use effectively to complete the task. Results on OpaqueToolsBench suggest existing methods for automatically docum",
      "authors": [
        "Skyler Hallinan",
        "Thejas Venkatesh",
        "Xiang Ren",
        "Sai Praneeth Karimireddy",
        "Ashwin Paranjape",
        "Yuhao Zhang",
        "Jack Hessel"
      ],
      "published": "2026-02-16T21:26:37Z",
      "updated": "2026-02-16T21:26:37Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15197v1",
      "abs_url": "https://arxiv.org/abs/2602.15197v1",
      "relevance_score": 13,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "agentic",
        "function calling",
        "benchmark"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15753v1",
      "title": "Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac",
      "summary": "Low-resource languages pose persistent challenges for Natural Language Processing tasks such as lemmatization and part-of-speech (POS) tagging. This paper investigates the capacity of recent large language models (LLMs), including GPT-4 variants and open-weight Mistral models, to address these tasks in few-shot and zero-shot settings for four historically and linguistically diverse under-resourced languages: Ancient Greek, Classical Armenian, Old Georgian, and Syriac. Using a novel benchmark comprising aligned training and out-of-domain test corpora, we evaluate the performance of foundation models across lemmatization and POS-tagging, and compare them with PIE, a task-specific RNN baseline. Our results demonstrate that LLMs, even without fine-tuning, achieve competitive or superior perfor",
      "authors": [
        "Chahan Vidal-Gor√®ne",
        "Bastien Kindt",
        "Florian Cafiero"
      ],
      "published": "2026-02-17T17:34:32Z",
      "updated": "2026-02-17T17:34:32Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15753v1",
      "abs_url": "https://arxiv.org/abs/2602.15753v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "mistral",
        "fine-tuning",
        "few-shot",
        "benchmark"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15620v1",
      "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
      "summary": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\\%, which we term \\emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, lea",
      "authors": [
        "Shiqi Liu",
        "Zeyu He",
        "Guojian Zhan",
        "Letian Tao",
        "Zhilong Zheng",
        "Jiang Wu",
        "Yinuo Wang",
        "Yang Guan",
        "Kehua Sheng",
        "Bo Zhang",
        "Keqiang Li",
        "Jingliang Duan",
        "Shengbo Eben Li"
      ],
      "published": "2026-02-17T14:46:48Z",
      "updated": "2026-02-17T14:46:48Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15620v1",
      "abs_url": "https://arxiv.org/abs/2602.15620v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "fine-tuning",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15481v1",
      "title": "LLM-as-Judge on a Budget",
      "summary": "LLM-as-a-judge has emerged as a cornerstone technique for evaluating large language models by leveraging LLM reasoning to score prompt-response pairs. Since LLM judgments are stochastic, practitioners commonly query each pair multiple times to estimate mean scores accurately. This raises a critical challenge: given a fixed computational budget $B$, how to optimally allocate queries across $K$ prompt-response pairs to minimize estimation error? % We present a principled variance-adaptive approach leveraging multi-armed bandit theory and concentration inequalities. Our method dynamically allocates queries based on estimated score variances, concentrating resources where uncertainty is highest. Further, our algorithm is shown to achieve a worst-case score-estimation error of $\\tilde{O}\\left(\\",
      "authors": [
        "Aadirupa Saha",
        "Aniket Wagde",
        "Branislav Kveton"
      ],
      "published": "2026-02-17T10:35:41Z",
      "updated": "2026-02-17T10:35:41Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15481v1",
      "abs_url": "https://arxiv.org/abs/2602.15481v1",
      "relevance_score": 12,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "prompt",
        "evaluation",
        "reasoning",
        "reasoning",
        "rag",
        "safety",
        "alignment"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15294v1",
      "title": "EAA: Automating materials characterization with vision language model agents",
      "summary": "We present Experiment Automation Agents (EAA), a vision-language-model-driven agentic system designed to automate complex experimental microscopy workflows. EAA integrates multimodal reasoning, tool-augmented action, and optional long-term memory to support both autonomous procedures and interactive user-guided measurements. Built on a flexible task-manager architecture, the system enables workflows ranging from fully agent-driven automation to logic-defined routines that embed localized LLM queries. EAA further provides a modern tool ecosystem with two-way compatibility for Model Context Protocol (MCP), allowing instrument-control tools to be consumed or served across applications. We demonstrate EAA at an imaging beamline at the Advanced Photon Source, including automated zone plate focu",
      "authors": [
        "Ming Du",
        "Yanqi Luo",
        "Srutarshi Banerjee",
        "Michael Wojcik",
        "Jelena Popovic",
        "Mathew J. Cherukara"
      ],
      "published": "2026-02-17T01:34:05Z",
      "updated": "2026-02-17T01:34:05Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15294v1",
      "abs_url": "https://arxiv.org/abs/2602.15294v1",
      "relevance_score": 12,
      "matched_keywords": [
        "llm",
        "language model",
        "agent",
        "agentic",
        "reasoning",
        "reasoning",
        "multimodal",
        "vision language"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15725v1",
      "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models",
      "summary": "Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is ",
      "authors": [
        "Sarim Chaudhry"
      ],
      "published": "2026-02-17T17:01:42Z",
      "updated": "2026-02-17T17:01:42Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15725v1",
      "abs_url": "https://arxiv.org/abs/2602.15725v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "language model",
        "mistral",
        "prompt",
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15485v1",
      "title": "SecCodeBench-V2 Technical Report",
      "summary": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and Node.js. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. ",
      "authors": [
        "Longfei Chen",
        "Ji Zhao",
        "Lanxiao Cui",
        "Tong Su",
        "Xingbo Pan",
        "Ziyang Li",
        "Yongxing Wu",
        "Qijiang Cao",
        "Qiyao Cai",
        "Jing Zhang",
        "Yuandong Ni",
        "Junyao He",
        "Zeyu Zhang",
        "Chao Ge",
        "Xuhuai Lu",
        "Zeyu Gao",
        "Yuxin Cui",
        "Weisen Chen",
        "Yuxuan Peng",
        "Shengping Wang",
        "Qi Li",
        "Yukai Huang",
        "Yukun Liu",
        "Tuo Zhou",
        "Terry Yue Zhuo",
        "Junyang Lin",
        "Chao Zhang"
      ],
      "published": "2026-02-17T10:47:06Z",
      "updated": "2026-02-17T10:47:06Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15485v1",
      "abs_url": "https://arxiv.org/abs/2602.15485v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "coding assistant",
        "benchmark",
        "evaluation",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15382v1",
      "title": "The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems",
      "summary": "Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning tra",
      "authors": [
        "Xiaoze Liu",
        "Ruowang Zhang",
        "Weichen Yu",
        "Siheng Xiong",
        "Liu He",
        "Feijie Wu",
        "Hoin Jung",
        "Matt Fredrikson",
        "Xiaoqian Wang",
        "Jing Gao"
      ],
      "published": "2026-02-17T06:31:53Z",
      "updated": "2026-02-17T06:31:53Z",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15382v1",
      "abs_url": "https://arxiv.org/abs/2602.15382v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "language model",
        "agent",
        "reasoning",
        "reasoning",
        "rag",
        "alignment"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15325v1",
      "title": "AgriWorld:A World Tools Protocol Framework for Verifiable Agricultural Reasoning with Code-Executing LLM Agents",
      "summary": "Foundation models for agriculture are increasingly trained on massive spatiotemporal data (e.g., multi-spectral remote sensing, soil grids, and field-level management logs) and achieve strong performance on forecasting and monitoring. However, these models lack language-based reasoning and interactive capabilities, limiting their usefulness in real-world agronomic workflows. Meanwhile, large language models (LLMs) excel at interpreting and generating text, but cannot directly reason over high-dimensional, heterogeneous agricultural datasets. We bridge this gap with an agentic framework for agricultural science. It provides a Python execution environment, AgriWorld, exposing unified tools for geospatial queries over field parcels, remote-sensing time-series analytics, crop growth simulation",
      "authors": [
        "Zhixing Zhang",
        "Jesen Zhang",
        "Hao Liu",
        "Qinhan Lv",
        "Jing Yang",
        "Kaitong Cai",
        "Keze Wang"
      ],
      "published": "2026-02-17T03:12:57Z",
      "updated": "2026-02-17T03:12:57Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15325v1",
      "abs_url": "https://arxiv.org/abs/2602.15325v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "agentic",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15173v1",
      "title": "Mind the (DH) Gap! A Contrast in Risky Choices Between Reasoning and Conversational LLMs",
      "summary": "The use of large language models either as decision support systems, or in agentic workflows, is rapidly transforming the digital ecosystem. However, the understanding of LLM decision-making under uncertainty remains limited. We initiate a comparative study of LLM risky choices along two dimensions: (1) prospect representation (explicit vs. experience based) and (2) decision rationale (explanation). Our study, which involves 20 frontier and open LLMs, is complemented by a matched human subjects experiment, which provides one reference point, while an expected payoff maximizing rational agent model provides another. We find that LLMs cluster into two categories: reasoning models (RMs) and conversational models (CMs). RMs tend towards rational behavior, are insensitive to the order of prospe",
      "authors": [
        "Luise Ge",
        "Yongyan Zhang",
        "Yevgeniy Vorobeychik"
      ],
      "published": "2026-02-16T20:24:54Z",
      "updated": "2026-02-16T20:24:54Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15173v1",
      "abs_url": "https://arxiv.org/abs/2602.15173v1",
      "relevance_score": 11,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "agentic",
        "reasoning",
        "reasoning"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15677v1",
      "title": "CAMEL: An ECG Language Model for Forecasting Cardiac Events",
      "summary": "Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classi",
      "authors": [
        "Neelay Velingker",
        "Alaia Solko-Breslin",
        "Mayank Keoliya",
        "Seewon Choi",
        "Jiayi Xin",
        "Anika Marathe",
        "Alireza Oraii",
        "Rajat Deo",
        "Sameed Khatana",
        "Rajeev Alur",
        "Mayur Naik",
        "Eric Wong"
      ],
      "published": "2026-02-17T16:02:52Z",
      "updated": "2026-02-17T16:02:52Z",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15677v1",
      "abs_url": "https://arxiv.org/abs/2602.15677v1",
      "relevance_score": 10,
      "matched_keywords": [
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15460v1",
      "title": "On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks",
      "summary": "Integrating reasoning in large language models and large vision-language models has recently led to significant improvement of their capabilities. However, the generalization of reasoning models is still vaguely defined and poorly understood. In this work, we present an evaluation framework to rigorously examine how well chain-of-thought (CoT) approaches generalize on a simple planning task. Specifically, we consider a grid-based navigation task in which a model is provided with a map and must output a sequence of moves that guides a player from a start position to a goal while avoiding obstacles. The versatility of the task and its data allows us to fine-tune model variants using different input representations (visual and textual) and CoT reasoning strategies, and systematically evaluate",
      "authors": [
        "Yannic Neuhaus",
        "Nicolas Flammarion",
        "Matthias Hein",
        "Francesco Croce"
      ],
      "published": "2026-02-17T09:51:40Z",
      "updated": "2026-02-17T09:51:40Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15460v1",
      "abs_url": "https://arxiv.org/abs/2602.15460v1",
      "relevance_score": 10,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "evaluation",
        "reasoning",
        "cot",
        "reasoning",
        "multimodal"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15378v1",
      "title": "Making Large Language Models Speak Tulu: Structured Prompting for an Extremely Low-Resource Language",
      "summary": "Can large language models converse in languages virtually absent from their training data? We investigate this question through a case study on Tulu, a Dravidian language with over 2 million speakers but minimal digital presence. Rather than fine-tuning an LLM, we examine whether structured prompts alone can elicit basic conversational ability under controlled prompting. We systematically tackle various challenges posed by absence of training data for Tulu by combining explicit grammar documentation, negative constraints to suppress high-probability tokens from related languages, romanization standardization, and quality-controlled synthetic data generation via self-play. Evaluated on a manually curated held-out set across three LLMs (Gemini 2.0 Flash, GPT-4o, Llama 3.1 70B) and validated ",
      "authors": [
        "Prathamesh Devadiga",
        "Paras Chopra"
      ],
      "published": "2026-02-17T06:20:09Z",
      "updated": "2026-02-17T06:20:09Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15378v1",
      "abs_url": "https://arxiv.org/abs/2602.15378v1",
      "relevance_score": 10,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "gemini",
        "llama",
        "fine-tuning",
        "prompt"
      ],
      "category": "training"
    },
    {
      "id": "2602.15353v1",
      "title": "NeuroSymActive: Differentiable Neural-Symbolic Reasoning with Active Exploration for Knowledge Graph Question Answering",
      "summary": "Large pretrained language models and neural reasoning systems have advanced many natural language tasks, yet they remain challenged by knowledge-intensive queries that require precise, structured multi-hop inference. Knowledge graphs provide a compact symbolic substrate for factual grounding, but integrating graph structure with neural models is nontrivial: naively embedding graph facts into prompts leads to inefficiency and fragility, while purely symbolic or search-heavy approaches can be costly in retrievals and lack gradient-based refinement. We introduce NeuroSymActive, a modular framework that combines a differentiable neural-symbolic reasoning layer with an active, value-guided exploration controller for Knowledge Graph Question Answering. The method couples soft-unification style s",
      "authors": [
        "Rong Fu",
        "Yang Li",
        "Zeyu Zhang",
        "Jiekai Wu",
        "Yaohua Liu",
        "Shuaishuai Cao",
        "Yangchen Zeng",
        "Yuhang Zhang",
        "Xiaojing Du",
        "Chuang Zhao",
        "Kangning Cui",
        "Simon Fong"
      ],
      "published": "2026-02-17T04:47:29Z",
      "updated": "2026-02-17T04:47:29Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15353v1",
      "abs_url": "https://arxiv.org/abs/2602.15353v1",
      "relevance_score": 10,
      "matched_keywords": [
        "language model",
        "prompt",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15183v1",
      "title": "Seeing to Generalize: How Visual Data Corrects Binding Shortcuts",
      "summary": "Vision Language Models (VLMs) are designed to extend Large Language Models (LLMs) with visual capabilities, yet in this work we observe a surprising phenomenon: VLMs can outperform their underlying LLMs on purely text-only tasks, particularly in long-context information retrieval. To investigate this effect, we build a controlled synthetic retrieval task and find that a transformer trained only on text achieves perfect in-distribution accuracy but fails to generalize out of distribution, while subsequent training on an image-tokenized version of the same task nearly doubles text-only OOD performance. Mechanistic interpretability reveals that visual training changes the model's internal binding strategy: text-only training encourages positional shortcuts, whereas image-based training disrup",
      "authors": [
        "Nicolas Buzeta",
        "Felipe del Rio",
        "Cristian Hinostroza",
        "Denis Parra",
        "Hans Lobel",
        "Rodrigo Toro Icarte"
      ],
      "published": "2026-02-16T20:43:12Z",
      "updated": "2026-02-16T20:43:12Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15183v1",
      "abs_url": "https://arxiv.org/abs/2602.15183v1",
      "relevance_score": 10,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "transformer",
        "reasoning",
        "reasoning",
        "rag",
        "vision language"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15156v1",
      "title": "Panini: Continual Learning in Token Space via Structured Memory",
      "summary": "Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself c",
      "authors": [
        "Shreyas Rajesh",
        "Pavan Holur",
        "Mehmet Yigit Turali",
        "Chenda Duan",
        "Vwani Roychowdhury"
      ],
      "published": "2026-02-16T19:58:03Z",
      "updated": "2026-02-16T19:58:03Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15156v1",
      "abs_url": "https://arxiv.org/abs/2602.15156v1",
      "relevance_score": 10,
      "matched_keywords": [
        "llm",
        "language model",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15564v1",
      "title": "Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL",
      "summary": "Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs' reasoning capability in adaptive workflow ",
      "authors": [
        "Yihan Wang",
        "Peiyu Liu",
        "Runyu Chen",
        "Wei Xu"
      ],
      "published": "2026-02-17T13:24:56Z",
      "updated": "2026-02-17T13:24:56Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15564v1",
      "abs_url": "https://arxiv.org/abs/2602.15564v1",
      "relevance_score": 9,
      "matched_keywords": [
        "llm",
        "benchmark",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15531v1",
      "title": "GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway",
      "summary": "This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: f",
      "authors": [
        "Javier Irigoyen",
        "Roberto Daza",
        "Aythami Morales",
        "Julian Fierrez",
        "Francisco Jurado",
        "Alvaro Ortigosa",
        "Ruben Tolosana"
      ],
      "published": "2026-02-17T12:11:49Z",
      "updated": "2026-02-17T12:11:49Z",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15531v1",
      "abs_url": "https://arxiv.org/abs/2602.15531v1",
      "relevance_score": 9,
      "matched_keywords": [
        "llm",
        "gemini",
        "llama",
        "fine-tuning",
        "prompt",
        "benchmark",
        "evaluation"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15327v1",
      "title": "Prescriptive Scaling Reveals the Evolution of Language Model Capabilities",
      "summary": "For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning",
      "authors": [
        "Hanlin Zhang",
        "Jikai Jin",
        "Vasilis Syrgkanis",
        "Sham Kakade"
      ],
      "published": "2026-02-17T03:13:51Z",
      "updated": "2026-02-17T03:13:51Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15327v1",
      "abs_url": "https://arxiv.org/abs/2602.15327v1",
      "relevance_score": 9,
      "matched_keywords": [
        "language model",
        "benchmark",
        "evaluation",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15791v1",
      "title": "Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings",
      "summary": "Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Variou",
      "authors": [
        "Suhyung Jang",
        "Ghang Lee",
        "Jaekun Lee",
        "Hyunjun Lee"
      ],
      "published": "2026-02-17T18:26:36Z",
      "updated": "2026-02-17T18:26:36Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15791v1",
      "abs_url": "https://arxiv.org/abs/2602.15791v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "gpt",
        "llama",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15758v1",
      "title": "ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models",
      "summary": "While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-controlled modification chains and a rigorously human-verified subset. Unlike prior one-shot benchmarks, ChartEditBench evaluates sustained, context-aware editing. We further propose a robust evaluation framework that mitigates limitations of LLM-as-a-Judge metrics by integrating execution-based fidelity",
      "authors": [
        "Manav Nitin Kapadnis",
        "Lawanya Baghel",
        "Atharva Naik",
        "Carolyn Ros√©"
      ],
      "published": "2026-02-17T17:45:34Z",
      "updated": "2026-02-17T17:45:34Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15758v1",
      "abs_url": "https://arxiv.org/abs/2602.15758v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "evaluation",
        "multimodal"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15669v1",
      "title": "PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra",
      "summary": "Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for",
      "authors": [
        "Xiachong Feng",
        "Liang Zhao",
        "Weihong Zhong",
        "Yichong Huang",
        "Yuxuan Gu",
        "Lingpeng Kong",
        "Xiaocheng Feng",
        "Bing Qin"
      ],
      "published": "2026-02-17T15:47:58Z",
      "updated": "2026-02-17T15:47:58Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15669v1",
      "abs_url": "https://arxiv.org/abs/2602.15669v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "fine-tuning",
        "prompt",
        "benchmark"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15195v1",
      "title": "Weight space Detection of Backdoors in LoRA Adapters",
      "summary": "LoRA adapters let users fine-tune large language models (LLMs) efficiently. However, LoRA adapters are shared through open repositories like Hugging Face Hub \\citep{huggingface_hub_docs}, making them vulnerable to backdoor attacks. Current detection methods require running the model with test input data -- making them impractical for screening thousands of adapters where the trigger for backdoor behavior is unknown. We detect poisoned adapters by analyzing their weight matrices directly, without running the model -- making our method data-agnostic. Our method extracts simple statistics -- how concentrated the singular values are, their entropy, and the distribution shape -- and flags adapters that deviate from normal patterns. We evaluate the method on 500 LoRA adapters -- 400 clean, and 1",
      "authors": [
        "David Puertolas Merenciano",
        "Ekaterina Vasyagina",
        "Raghav Dixit",
        "Kevin Zhu",
        "Ruizhe Li",
        "Javier Ferrando",
        "Maheep Chaudhary"
      ],
      "published": "2026-02-16T21:20:47Z",
      "updated": "2026-02-16T21:20:47Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15195v1",
      "abs_url": "https://arxiv.org/abs/2602.15195v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "llama",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15189v1",
      "title": "ScrapeGraphAI-100k: A Large-Scale Dataset for LLM-Based Web Information Extraction",
      "summary": "The use of large language models for web information extraction is becoming increasingly fundamental to modern web information retrieval pipelines. However, existing datasets tend to be small, synthetic or text-only, failing to capture the structural context of the web. We introduce ScrapeGraphAI-100k, a large-scale dataset comprising real-world LLM extraction events, collected via opt-in ScrapeGraphAI telemetry during Q2 and Q3 of 2025. Starting from 9M events, we deduplicate and balance by schema to produce 93,695 examples spanning diverse domains and languages. Each instance includes Markdown content, a prompt, a JSON schema, the LLM response, and complexity/validation metadata. We characterize the datasets structural diversity and its failure modes as schema complexity increases. We al",
      "authors": [
        "William Brach",
        "Francesco Zuppichini",
        "Marco Vinciguerra",
        "Lorenzo Padoan"
      ],
      "published": "2026-02-16T20:56:59Z",
      "updated": "2026-02-16T20:56:59Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15189v1",
      "abs_url": "https://arxiv.org/abs/2602.15189v1",
      "relevance_score": 8,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "fine-tuning",
        "prompt",
        "benchmark"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15829v1",
      "title": "Operationalising the Superficial Alignment Hypothesis via Task Complexity",
      "summary": "The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SAH simply claims that pre-trained models drastically reduce the complexity of achieving high performance on many tasks. Our definition unifies prior arguments supporting the SAH, interpreting them as different strategies to find such short programs. Experimentally, we estimate the task complexity of ",
      "authors": [
        "Tom√°s Vergara-Browne",
        "Darshan Patil",
        "Ivan Titov",
        "Siva Reddy",
        "Tiago Pimentel",
        "Marius Mosbach"
      ],
      "published": "2026-02-17T18:59:39Z",
      "updated": "2026-02-17T18:59:39Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15829v1",
      "abs_url": "https://arxiv.org/abs/2602.15829v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "language model",
        "reasoning",
        "reasoning",
        "alignment"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15823v1",
      "title": "CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing",
      "summary": "A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exact",
      "authors": [
        "Zarif Ikram",
        "Arad Firouzkouhi",
        "Stephen Tu",
        "Mahdi Soltanolkotabi",
        "Paria Rashidinejad"
      ],
      "published": "2026-02-17T18:58:04Z",
      "updated": "2026-02-17T18:58:04Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15823v1",
      "abs_url": "https://arxiv.org/abs/2602.15823v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "rag"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15532v1",
      "title": "Quantifying construct validity in large language model evaluations",
      "summary": "The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance. Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the cap",
      "authors": [
        "Ryan Othniel Kearns"
      ],
      "published": "2026-02-17T12:15:57Z",
      "updated": "2026-02-17T12:15:57Z",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15532v1",
      "abs_url": "https://arxiv.org/abs/2602.15532v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "benchmark",
        "evaluation"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15456v1",
      "title": "In Agents We Trust, but Who Do Agents Trust? Latent Source Preferences Steer LLM Generations",
      "summary": "Agents based on Large Language Models (LLMs) are increasingly being deployed as interfaces to information on online platforms. These agents filter, prioritize, and synthesize information retrieved from the platforms' back-end databases or via web search. In these scenarios, LLM agents govern the information users receive, by drawing users' attention to particular instances of retrieved information at the expense of others. While much prior work has focused on biases in the information LLMs themselves generate, less attention has been paid to the factors that influence what information LLMs select and present to users. We hypothesize that when information is attributed to specific sources (e.g., particular publishers, journals, or platforms), current LLMs exhibit systematic latent source pr",
      "authors": [
        "Mohammad Aflah Khan",
        "Mahsa Amani",
        "Soumi Das",
        "Bishwamittra Ghosh",
        "Qinyuan Wu",
        "Krishna P. Gummadi",
        "Manish Gupta",
        "Abhilasha Ravichander"
      ],
      "published": "2026-02-17T09:45:22Z",
      "updated": "2026-02-17T09:45:22Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15456v1",
      "abs_url": "https://arxiv.org/abs/2602.15456v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent",
        "prompt"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15344v1",
      "title": "ER-MIA: Black-Box Adversarial Memory Injection Attacks on Long-Term Memory-Augmented Large Language Models",
      "summary": "Large language models (LLMs) are increasingly augmented with long-term memory systems to overcome finite context windows and enable persistent reasoning across interactions. However, recent research finds that LLMs become more vulnerable because memory provides extra attack surfaces. In this paper, we present the first systematic study of black-box adversarial memory injection attacks that target the similarity-based retrieval mechanism in long-term memory-augmented LLMs. We introduce ER-MIA, a unified framework that exposes this vulnerability and formalizes two realistic attack settings: content-based attacks and question-targeted attacks. In these settings, ER-MIA includes an arsenal of composable attack primitives and ensemble attacks that achieve high success rates under minimal attack",
      "authors": [
        "Mitchell Piehl",
        "Zhaohan Xi",
        "Zuobin Xiong",
        "Pan He",
        "Muchao Ye"
      ],
      "published": "2026-02-17T04:19:45Z",
      "updated": "2026-02-17T04:19:45Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15344v1",
      "abs_url": "https://arxiv.org/abs/2602.15344v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "reasoning",
        "reasoning"
      ],
      "category": "reasoning"
    },
    {
      "id": "2602.15312v1",
      "title": "Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement",
      "summary": "Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions pr",
      "authors": [
        "Stephan Ludwig",
        "Peter J. Danaher",
        "Xiaohao Yang",
        "Yu-Ting Lin",
        "Ehsan Abedin",
        "Dhruv Grewal",
        "Lan Du"
      ],
      "published": "2026-02-17T02:33:51Z",
      "updated": "2026-02-17T02:33:51Z",
      "categories": [
        "cs.CL",
        "econ.EM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15312v1",
      "abs_url": "https://arxiv.org/abs/2602.15312v1",
      "relevance_score": 7,
      "matched_keywords": [
        "large language model",
        "language model",
        "gpt",
        "evaluation",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15260v1",
      "title": "Fast and Effective On-policy Distillation from Reasoning Prefixes",
      "summary": "On-policy distillation (OPD), which samples trajectories from the student model and supervises them with a teacher at the token level, avoids relying solely on verifiable terminal rewards and can yield better generalization than off-policy distillation. However, OPD requires expensive on-the-fly sampling of the student policy during training, which substantially increases training cost, especially for long responses. Our initial analysis shows that, during OPD, training signals are often concentrated in the prefix of each output, and that even a short teacher-generated prefix can significantly help the student produce the correct answer. Motivated by these observations, we propose a simple yet effective modification of OPD: we apply the distillation objective only to prefixes of student-ge",
      "authors": [
        "Dongxu Zhang",
        "Zhichao Yang",
        "Sepehr Janghorbani",
        "Jun Han",
        "Andrew Ressler",
        "Qian Qian",
        "Gregory D. Lyng",
        "Sanjit Singh Batra",
        "Robert E. Tillman"
      ],
      "published": "2026-02-16T23:28:54Z",
      "updated": "2026-02-16T23:28:54Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15260v1",
      "abs_url": "https://arxiv.org/abs/2602.15260v1",
      "relevance_score": 7,
      "matched_keywords": [
        "benchmark",
        "reasoning",
        "reasoning"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15252v1",
      "title": "Decision Making under Imperfect Recall: Algorithms and Benchmarks",
      "summary": "In game theory, imperfect-recall decision problems model situations in which an agent forgets information it held before. They encompass games such as the ``absentminded driver'' and team games with limited communication. In this paper, we introduce the first benchmark suite for imperfect-recall decision problems. Our benchmarks capture a variety of problem types, including ones concerning privacy in AI systems that elicit sensitive information, and AI safety via testing of agents in simulation. Across 61 problem instances generated using this suite, we evaluate the performance of different algorithms for finding first-order optimal strategies in such problems. In particular, we introduce the family of regret matching (RM) algorithms for nonlinear constrained optimization. This class of pa",
      "authors": [
        "Emanuel Tewolde",
        "Brian Hu Zhang",
        "Ioannis Anagnostides",
        "Tuomas Sandholm",
        "Vincent Conitzer"
      ],
      "published": "2026-02-16T23:19:01Z",
      "updated": "2026-02-16T23:19:01Z",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15252v1",
      "abs_url": "https://arxiv.org/abs/2602.15252v1",
      "relevance_score": 7,
      "matched_keywords": [
        "agent",
        "benchmark",
        "safety"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15241v1",
      "title": "GenAI for Systems: Recurring Challenges and Design Principles from Software to Silicon",
      "summary": "Generative AI is reshaping how computing systems are designed, optimized, and built, yet research remains fragmented across software, architecture, and chip design communities. This paper takes a cross-stack perspective, examining how generative models are being applied from code generation and distributed runtimes through hardware design space exploration to RTL synthesis, physical layout, and verification. Rather than reviewing each layer in isolation, we analyze how the same structural difficulties and effective responses recur across the stack. Our central finding is one of convergence. Despite the diversity of domains and tools, the field keeps encountering five recurring challenges (the feedback loop crisis, the tacit knowledge problem, trust and validation, co-design across boundari",
      "authors": [
        "Arya Tschand",
        "Chenyu Wang",
        "Zishen Wan",
        "Andrew Cheng",
        "Ioana Cristescu",
        "Kevin He",
        "Howard Huang",
        "Alexander Ingare",
        "Akseli Kangaslahti",
        "Sara Kangaslahti",
        "Theo Lebryk",
        "Hongjin Lin",
        "Jeffrey Jian Ma",
        "Alexandru Meterez",
        "Clara Mohri",
        "Depen Morwani",
        "Sunny Qin",
        "Roy Rinberg",
        "Paula Rodriguez-Diaz",
        "Alyssa Mia Taliotis",
        "Pernille Undrum Fathi",
        "Rosie Zhao",
        "Todd Zhou",
        "Vijay Janapa Reddi"
      ],
      "published": "2026-02-16T22:45:33Z",
      "updated": "2026-02-16T22:45:33Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15241v1",
      "abs_url": "https://arxiv.org/abs/2602.15241v1",
      "relevance_score": 7,
      "matched_keywords": [
        "code generation",
        "benchmark",
        "rag"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15143v1",
      "title": "Protecting Language Models Against Unauthorized Distillation through Trace Rewriting",
      "summary": "Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \\emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \\emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, ",
      "authors": [
        "Xinhang Ma",
        "William Yeoh",
        "Ning Zhang",
        "Yevgeniy Vorobeychik"
      ],
      "published": "2026-02-16T19:40:07Z",
      "updated": "2026-02-16T19:40:07Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15143v1",
      "abs_url": "https://arxiv.org/abs/2602.15143v1",
      "relevance_score": 7,
      "matched_keywords": [
        "llm",
        "language model",
        "reasoning",
        "reasoning",
        "rag"
      ],
      "category": "reasoning"
    },
    {
      "id": "2602.15740v1",
      "title": "MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis",
      "summary": "Alzheimer's disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of ",
      "authors": [
        "Fatemeh Khalvandi",
        "Saadat Izadi",
        "Abdolah Chalechale"
      ],
      "published": "2026-02-17T17:15:32Z",
      "updated": "2026-02-17T17:15:32Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15740v1",
      "abs_url": "https://arxiv.org/abs/2602.15740v1",
      "relevance_score": 6,
      "matched_keywords": [
        "attention mechanism",
        "dpo",
        "prompt",
        "evaluation",
        "multimodal",
        "alignment"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15689v1",
      "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models",
      "summary": "Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework char",
      "authors": [
        "Meirav Segal",
        "Noa Linder",
        "Omer Antverg",
        "Gil Gekker",
        "Tomer Fichman",
        "Omri Bodenheimer",
        "Edan Maor",
        "Omer Nevo"
      ],
      "published": "2026-02-17T16:12:21Z",
      "updated": "2026-02-17T16:12:21Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15689v1",
      "abs_url": "https://arxiv.org/abs/2602.15689v1",
      "relevance_score": 6,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "agent"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15645v1",
      "title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving",
      "summary": "Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reaso",
      "authors": [
        "Lucas Elbert Suryana",
        "Farah Bierenga",
        "Sanne van Buuren",
        "Pepijn Kooij",
        "Elsefien Tulleners",
        "Federico Scari",
        "Simeon Calvert",
        "Bart van Arem",
        "Arkady Zgonnikov"
      ],
      "published": "2026-02-17T15:13:36Z",
      "updated": "2026-02-17T15:13:36Z",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15645v1",
      "abs_url": "https://arxiv.org/abs/2602.15645v1",
      "relevance_score": 6,
      "matched_keywords": [
        "language model",
        "prompt",
        "evaluation",
        "vision language",
        "safety",
        "alignment"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15637v1",
      "title": "The Stationarity Bias: Stratified Stress-Testing for Time-Series Imputation in Regulated Dynamical Systems",
      "summary": "Time-series imputation benchmarks employ uniform random masking and shape-agnostic metrics (MSE, RMSE), implicitly weighting evaluation by regime prevalence. In systems with a dominant attractor -- homeostatic physiology, nominal industrial operation, stable network traffic -- this creates a systematic \\emph{Stationarity Bias}: simple methods appear superior because the benchmark predominantly samples the easy, low-entropy regime where they trivially succeed. We formalize this bias and propose a \\emph{Stratified Stress-Test} that partitions evaluation into Stationary and Transient regimes. Using Continuous Glucose Monitoring (CGM) as a testbed -- chosen for its rigorous ground-truth forcing functions (meals, insulin) that enable precise regime identification -- we establish three findings ",
      "authors": [
        "Amirreza Dolatpour Fathkouhi",
        "Alireza Namazi",
        "Heman Shakeri"
      ],
      "published": "2026-02-17T15:05:56Z",
      "updated": "2026-02-17T15:05:56Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15637v1",
      "abs_url": "https://arxiv.org/abs/2602.15637v1",
      "relevance_score": 6,
      "matched_keywords": [
        "benchmark",
        "evaluation",
        "rag",
        "safety"
      ],
      "category": "benchmarks"
    },
    {
      "id": "2602.15580v1",
      "title": "How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning",
      "summary": "When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \\emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \\",
      "authors": [
        "Hongxuan Wu",
        "Yukun Zhang",
        "Xueqing Zhou"
      ],
      "published": "2026-02-17T13:49:49Z",
      "updated": "2026-02-17T13:49:49Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15580v1",
      "abs_url": "https://arxiv.org/abs/2602.15580v1",
      "relevance_score": 6,
      "matched_keywords": [
        "transformer",
        "reasoning",
        "reasoning",
        "multimodal"
      ],
      "category": "reasoning"
    },
    {
      "id": "2602.15457v1",
      "title": "Benchmarking IoT Time-Series AD with Event-Level Augmentations",
      "summary": "Anomaly detection (AD) for safety-critical IoT time series should be judged at the event level: reliability and earliness under realistic perturbations. Yet many studies still emphasize point-level results on curated base datasets, limiting value for model selection in practice. We introduce an evaluation protocol with unified event-level augmentations that simulate real-world issues: calibrated sensor dropout, linear and log drift, additive noise, and window shifts. We also perform sensor-level probing via mask-as-missing zeroing with per-channel influence estimation to support root-cause analysis. We evaluate 14 representative models on five public anomaly datasets (SWaT, WADI, SMD, SKAB, TEP) and two industrial datasets (steam turbine, nuclear turbogenerator) using unified splits and ev",
      "authors": [
        "Dmitry Zhevnenko",
        "Ilya Makarov",
        "Aleksandr Kovalenko",
        "Fedor Meshchaninov",
        "Anton Kozhukhov",
        "Vladislav Travnikov",
        "Makar Ippolitov",
        "Kirill Yashunin",
        "Iurii Katser"
      ],
      "published": "2026-02-17T09:45:44Z",
      "updated": "2026-02-17T09:45:44Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15457v1",
      "abs_url": "https://arxiv.org/abs/2602.15457v1",
      "relevance_score": 6,
      "matched_keywords": [
        "benchmark",
        "evaluation",
        "rag",
        "safety"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15397v1",
      "title": "ActionCodec: What Makes for Good Action Tokenizers",
      "summary": "Vision-Language-Action (VLA) models leveraging the native autoregressive paradigm of Vision-Language Models (VLMs) have demonstrated superior instruction-following and training efficiency. Central to this paradigm is action tokenization, yet its design has primarily focused on reconstruction fidelity, failing to address its direct impact on VLA optimization. Consequently, the fundamental question of \\textit{what makes for good action tokenizers} remains unanswered. In this paper, we bridge this gap by establishing design principles specifically from the perspective of VLA optimization. We identify a set of best practices based on information-theoretic insights, including maximized temporal token overlap, minimized vocabulary redundancy, enhanced multimodal mutual information, and token ind",
      "authors": [
        "Zibin Dong",
        "Yicheng Liu",
        "Shiduo Zhang",
        "Baijun Ye",
        "Yifu Yuan",
        "Fei Ni",
        "Jingjing Gong",
        "Xipeng Qiu",
        "Hang Zhao",
        "Yinchuan Li",
        "Jianye Hao"
      ],
      "published": "2026-02-17T07:07:15Z",
      "updated": "2026-02-17T07:07:15Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15397v1",
      "abs_url": "https://arxiv.org/abs/2602.15397v1",
      "relevance_score": 6,
      "matched_keywords": [
        "language model",
        "benchmark",
        "rag",
        "multimodal"
      ],
      "category": "code-generation"
    },
    {
      "id": "2602.15384v1",
      "title": "World-Model-Augmented Web Agents with Action Correction",
      "summary": "Web agents based on large language models have demonstrated promising capability in automating web tasks. However, current web agents struggle to reason out sensible actions due to the limitations of predicting environment changes, and might not possess comprehensive awareness of execution risks, prematurely performing risky actions that cause losses and lead to task failure. To address these challenges, we propose WAC, a web agent that integrates model collaboration, consequence simulation, and feedback-driven action refinement. To overcome the cognitive isolation of individual models, we introduce a multi-agent collaboration process that enables an action model to consult a world model as a web-environment expert for strategic guidance; the action model then grounds these suggestions int",
      "authors": [
        "Zhouzhou Shen",
        "Xueyu Hu",
        "Xiyun Li",
        "Tianqing Fang",
        "Juncheng Li",
        "Shengyu Zhang"
      ],
      "published": "2026-02-17T06:37:31Z",
      "updated": "2026-02-17T06:37:31Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15384v1",
      "abs_url": "https://arxiv.org/abs/2602.15384v1",
      "relevance_score": 6,
      "matched_keywords": [
        "large language model",
        "language model",
        "agent",
        "rag"
      ],
      "category": "agents"
    },
    {
      "id": "2602.15318v1",
      "title": "Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs",
      "summary": "Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to th",
      "authors": [
        "Libo Zhang",
        "Zhaoning Zhang",
        "Wangyang Hong",
        "Peng Qiao",
        "Dongsheng Li"
      ],
      "published": "2026-02-17T02:51:36Z",
      "updated": "2026-02-17T02:51:36Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15318v1",
      "abs_url": "https://arxiv.org/abs/2602.15318v1",
      "relevance_score": 6,
      "matched_keywords": [
        "large language model",
        "llm",
        "language model",
        "rag"
      ],
      "category": "code-generation"
    }
  ],
  "recent_papers": [
    {
      "id": "2602.15756v1",
      "title": "A Note on Non-Composability of Layerwise Approximate Verification for Neural Inference",
      "summary": "A natural and informal approach to verifiable (or zero-knowledge) ML inference over floating-point data is: ``prove that each layer was computed correctly up to tolerance $Œ¥$; therefore the final output is a reasonable inference result''. This short note gives a simple counterexample showing that this inference is false in general: for any neural network, we can construct a functionally equivalent network for which adversarially chosen approximation-magnitude errors in individual layer computations suffice to steer the final output arbitrarily (within a prescribed bounded range).",
      "authors": [
        "Or Zamir"
      ],
      "published": "2026-02-17T17:41:59Z",
      "updated": "2026-02-17T17:41:59Z",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15756v1",
      "abs_url": "https://arxiv.org/abs/2602.15756v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.15751v1",
      "title": "Enabling Low-Latency Machine learning on Radiation-Hard FPGAs with hls4ml",
      "summary": "This paper presents the first demonstration of a viable, ultra-fast, radiation-hard machine learning (ML) application on FPGAs, which could be used in future high-energy physics experiments. We present a three-fold contribution, with the PicoCal calorimeter, planned for the LHCb Upgrade II experiment, used as a test case. First, we develop a lightweight autoencoder to compress a 32-sample timing readout, representative of that of the PicoCal, into a two-dimensional latent space. Second, we introduce a systematic, hardware-aware quantization strategy and show that the model can be reduced to 10-bit weights with minimal performance loss. Third, as a barrier to the adoption of on-detector ML is the lack of support for radiation-hard FPGAs in the High-Energy Physics community's standard ML syn",
      "authors": [
        "Katya Govorkova",
        "Julian Garcia Pardinas",
        "Vladimir Loncar",
        "Victoria Nguyen",
        "Sebastian Schmitt",
        "Marco Pizzichemi",
        "Loris Martinazzoli",
        "Eluned Anne Smith"
      ],
      "published": "2026-02-17T17:30:28Z",
      "updated": "2026-02-17T17:30:28Z",
      "categories": [
        "hep-ex",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15751v1",
      "abs_url": "https://arxiv.org/abs/2602.15751v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.15750v1",
      "title": "UrbanVerse: Learning Urban Region Representation Across Cities and Tasks",
      "summary": "Recent advances in urban region representation learning have enabled a wide range of applications in urban analytics, yet existing methods remain limited in their capabilities to generalize across cities and analytic tasks. We aim to generalize urban representation learning beyond city- and task-specific settings, towards a foundation-style model for urban analytics. To this end, we propose UrbanVerse, a model for cross-city urban representation learning and cross-task urban analytics. For cross-city generalization, UrbanVerse focuses on features local to the target regions and structural features of the nearby regions rather than the entire city. We model regions as nodes on a graph, which enables a random walk-based procedure to form \"sequences of regions\" that reflect both local and nei",
      "authors": [
        "Fengze Sun",
        "Egemen Tanin",
        "Shanika Karunasekera",
        "Zuqing Li",
        "Flora D. Salim",
        "Jianzhong Qi"
      ],
      "published": "2026-02-17T17:28:48Z",
      "updated": "2026-02-17T17:28:48Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15750v1",
      "abs_url": "https://arxiv.org/abs/2602.15750v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.15712v1",
      "title": "Criteria-first, semantics-later: reproducible structure discovery in image-based sciences",
      "summary": "Across the natural and life sciences, images have become a primary measurement modality, yet the dominant analytic paradigm remains semantics-first. Structure is recovered by predicting or enforcing domain-specific labels. This paradigm fails systematically under the conditions that make image-based science most valuable, including open-ended scientific discovery, cross-sensor and cross-site comparability, and long-term monitoring in which domain ontologies and associated label sets drift culturally, institutionally, and ecologically. A deductive inversion is proposed in the form of criteria-first and semantics-later. A unified framework for criteria-first structure discovery is introduced. It separates criterion-defined, semantics-free structure extraction from downstream semantic mapping",
      "authors": [
        "Jan Bumberger"
      ],
      "published": "2026-02-17T16:45:49Z",
      "updated": "2026-02-17T16:45:49Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15712v1",
      "abs_url": "https://arxiv.org/abs/2602.15712v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.15711v1",
      "title": "Random Wavelet Features for Graph Kernel Machines",
      "summary": "Node embeddings map graph vertices into low-dimensional Euclidean spaces while preserving structural information. They are central to tasks such as node classification, link prediction, and signal reconstruction. A key goal is to design node embeddings whose dot products capture meaningful notions of node similarity induced by the graph. Graph kernels offer a principled way to define such similarities, but their direct computation is often prohibitive for large networks. Inspired by random feature methods for kernel approximation in Euclidean spaces, we introduce randomized spectral node embeddings whose dot products estimate a low-rank approximation of any specific graph kernel. We provide theoretical and empirical results showing that our embeddings achieve more accurate kernel approxima",
      "authors": [
        "Valentin de Bassompierre",
        "Jean-Charles Delvenne",
        "Laurent Jacques"
      ],
      "published": "2026-02-17T16:45:15Z",
      "updated": "2026-02-17T16:45:15Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15711v1",
      "abs_url": "https://arxiv.org/abs/2602.15711v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.15708v1",
      "title": "Outer Diversity of Structured Domains",
      "summary": "An ordinal preference domain is a subset of preference orders that the voters are allowed to cast in an election. We introduce and study the notion of outer diversity of a domain and evaluate its value for a number of well-known structured domains, such as the single-peaked, single-crossing, group-separable, and Euclidean ones.",
      "authors": [
        "Piotr Faliszewski",
        "Krzysztof Sornat",
        "Stanis≈Çaw Szufa",
        "Tomasz WƒÖs"
      ],
      "published": "2026-02-17T16:42:05Z",
      "updated": "2026-02-17T16:42:05Z",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15708v1",
      "abs_url": "https://arxiv.org/abs/2602.15708v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.15648v1",
      "title": "Guided Diffusion by Optimized Loss Functions on Relaxed Parameters for Inverse Material Design",
      "summary": "Inverse design problems are common in engineering and materials science. The forward direction, i.e., computing output quantities from design parameters, typically requires running a numerical simulation, such as a FEM, as an intermediate step, which is an optimization problem by itself. In many scenarios, several design parameters can lead to the same or similar output values. For such cases, multi-modal probabilistic approaches are advantageous to obtain diverse solutions. A major difficulty in inverse design stems from the structure of the design space, since discrete parameters or further constraints disallow the direct use of gradient-based optimization. To tackle this problem, we propose a novel inverse design method based on diffusion models. Our approach relaxes the original design",
      "authors": [
        "Jens U. Kreber",
        "Christian Wei√üenfels",
        "Joerg Stueckler"
      ],
      "published": "2026-02-17T15:15:28Z",
      "updated": "2026-02-17T15:15:28Z",
      "categories": [
        "cs.LG",
        "cs.CE",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15648v1",
      "abs_url": "https://arxiv.org/abs/2602.15648v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.15632v1",
      "title": "Neural-POD: A Plug-and-Play Neural Operator Framework for Infinite-Dimensional Functional Nonlinear Proper Orthogonal Decomposition",
      "summary": "The rapid development of AI for Science is often hindered by the \"discretization\", where learned representations remain restricted to the specific grids or resolutions used during training. We propose the Neural Proper Orthogonal Decomposition (Neural-POD), a plug-and-play neural operator framework that constructs nonlinear, orthogonal basis functions in infinite-dimensional space using neural networks. Unlike the classical Proper Orthogonal Decomposition (POD), which is limited to linear subspace approximations obtained through singular value decomposition (SVD), Neural-POD formulates basis construction as a sequence of residual minimization problems solved through neural network training. Each basis function is obtained by learning to represent the remaining structure in the data, follow",
      "authors": [
        "Changhong Mou",
        "Binghang Lu",
        "Guang Lin"
      ],
      "published": "2026-02-17T15:01:40Z",
      "updated": "2026-02-17T15:01:40Z",
      "categories": [
        "physics.comp-ph",
        "cs.LG",
        "math.NA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15632v1",
      "abs_url": "https://arxiv.org/abs/2602.15632v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.15602v1",
      "title": "Certified Per-Instance Unlearning Using Individual Sensitivity Bounds",
      "summary": "Certified machine unlearning can be achieved via noise injection leading to differential privacy guarantees, where noise is calibrated to worst-case sensitivity. Such conservative calibration often results in performance degradation, limiting practical applicability. In this work, we investigate an alternative approach based on adaptive per-instance noise calibration tailored to the individual contribution of each data point to the learned solution. This raises the following challenge: how can one establish formal unlearning guarantees when the mechanism depends on the specific point to be removed? To define individual data point sensitivities in noisy gradient dynamics, we consider the use of per-instance differential privacy. For ridge regression trained via Langevin dynamics, we derive ",
      "authors": [
        "Hanna Benarroch",
        "Jamal Atif",
        "Olivier Capp√©"
      ],
      "published": "2026-02-17T14:18:47Z",
      "updated": "2026-02-17T14:18:47Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15602v1",
      "abs_url": "https://arxiv.org/abs/2602.15602v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.15593v1",
      "title": "A unified theory of feature learning in RNNs and DNNs",
      "summary": "Recurrent and deep neural networks (RNNs/DNNs) are cornerstone architectures in machine learning. Remarkably, RNNs differ from DNNs only by weight sharing, as can be shown through unrolling in time. How does this structural similarity fit with the distinct functional properties these networks exhibit? To address this question, we here develop a unified mean-field theory for RNNs and DNNs in terms of representational kernels, describing fully trained networks in the feature learning ($Œº$P) regime. This theory casts training as Bayesian inference over sequences and patterns, directly revealing the functional implications induced by the RNNs' weight sharing. In DNN-typical tasks, we identify a phase transition when the learning signal overcomes the noise due to randomness in the weights: belo",
      "authors": [
        "Jan P. Bauer",
        "Kirsten Fischer",
        "Moritz Helias",
        "Agostina Palmigiano"
      ],
      "published": "2026-02-17T14:06:34Z",
      "updated": "2026-02-17T14:06:34Z",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15593v1",
      "abs_url": "https://arxiv.org/abs/2602.15593v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.15586v1",
      "title": "Uniform error bounds for quantized dynamical models",
      "summary": "This paper provides statistical guarantees on the accuracy of dynamical models learned from dependent data sequences. Specifically, we develop uniform error bounds that apply to quantized models and imperfect optimization algorithms commonly used in practical contexts for system identification, and in particular hybrid system identification. Two families of bounds are obtained: slow-rate bounds via a block decomposition and fast-rate, variance-adaptive, bounds via a novel spaced-point strategy. The bounds scale with the number of bits required to encode the model and thus translate hardware constraints into interpretable statistical complexities.",
      "authors": [
        "Abdelkader Metakalard",
        "Fabien Lauer",
        "Kevin Colin",
        "Marion Gilson"
      ],
      "published": "2026-02-17T13:56:04Z",
      "updated": "2026-02-17T13:56:04Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15586v1",
      "abs_url": "https://arxiv.org/abs/2602.15586v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.15579v1",
      "title": "Intracoronary Optical Coherence Tomography Image Processing and Vessel Classification Using Machine Learning",
      "summary": "Intracoronary Optical Coherence Tomography (OCT) enables high-resolution visualization of coronary vessel anatomy but presents challenges due to noise, imaging artifacts, and complex tissue structures. This paper proposes a fully automated pipeline for vessel segmentation and classification in OCT images using machine learning techniques. The proposed method integrates image preprocessing, guidewire artifact removal, polar-to-Cartesian transformation, unsupervised K-means clustering, and local feature extraction. These features are used to train Logistic Regression and Support Vector Machine classifiers for pixel-wise vessel classification. Experimental results demonstrate excellent performance, achieving precision, recall, and F1-score values up to 1.00 and overall classification accuracy",
      "authors": [
        "Amal Lahchim",
        "Lambros Athanasiou"
      ],
      "published": "2026-02-17T13:47:27Z",
      "updated": "2026-02-17T13:47:27Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15579v1",
      "abs_url": "https://arxiv.org/abs/2602.15579v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.15552v1",
      "title": "Latent Regularization in Generative Test Input Generation",
      "summary": "This study investigates the impact of regularization of latent spaces through truncation on the quality of generated test inputs for deep learning classifiers. We evaluate this effect using style-based GANs, a state-of-the-art generative approach, and assess quality along three dimensions: validity, diversity, and fault detection. We evaluate our approach on the boundary testing of deep learning image classifiers across three datasets, MNIST, Fashion MNIST, and CIFAR-10. We compare two truncation strategies: latent code mixing with binary search optimization and random latent truncation for generative exploration. Our experiments show that the latent code-mixing approach yields a higher fault detection rate than random truncation, while also improving both diversity and validity.",
      "authors": [
        "Giorgi Merabishvili",
        "Oliver Wei√ül",
        "Andrea Stocco"
      ],
      "published": "2026-02-17T12:57:17Z",
      "updated": "2026-02-17T12:57:17Z",
      "categories": [
        "cs.SE",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15552v1",
      "abs_url": "https://arxiv.org/abs/2602.15552v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.15539v1",
      "title": "Dynamic Training-Free Fusion of Subject and Style LoRAs",
      "summary": "Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by ",
      "authors": [
        "Qinglong Cao",
        "Yuntian Chen",
        "Chao Ma",
        "Xiaokang Yang"
      ],
      "published": "2026-02-17T12:42:30Z",
      "updated": "2026-02-17T12:42:30Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15539v1",
      "abs_url": "https://arxiv.org/abs/2602.15539v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.15510v1",
      "title": "On the Geometric Coherence of Global Aggregation in Federated GNN",
      "summary": "Federated Learning (FL) enables distributed training across multiple clients without centralized data sharing, while Graph Neural Networks (GNNs) model relational data through message passing. In federated GNN settings, client graphs often exhibit heterogeneous structural and propagation characteristics. When standard aggregation mechanisms are applied to such heterogeneous updates, the global model may converge numerically while exhibiting degraded relational behavior.Our work identifies a geometric failure mode of global aggregation in Cross- Domain Federated GNNs. Although GNN parameters are numerically represented as vectors, they encode relational transformations that govern the direction, strength, and sensitivity of information flow across graph neighborhoods. Aggregating updates or",
      "authors": [
        "Chethana Prasad Kabgere",
        "Shylaja SS"
      ],
      "published": "2026-02-17T11:34:04Z",
      "updated": "2026-02-17T11:34:04Z",
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15510v1",
      "abs_url": "https://arxiv.org/abs/2602.15510v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "code-generation"
    },
    {
      "id": "2602.15472v1",
      "title": "Fluids You Can Trust: Property-Preserving Operator Learning for Incompressible Flows",
      "summary": "We present a novel property-preserving kernel-based operator learning method for incompressible flows governed by the incompressible Navier-Stokes equations. Traditional numerical solvers incur significant computational costs to respect incompressibility. Operator learning offers efficient surrogate models, but current neural operators fail to exactly enforce physical properties such as incompressibility, periodicity, and turbulence. Our method maps input functions to expansion coefficients of output functions in a property-preserving kernel basis, ensuring that predicted velocity fields analytically and simultaneously preserve the aforementioned physical properties. We evaluate the method on challenging 2D and 3D, laminar and turbulent, incompressible flow problems. Our method achieves up",
      "authors": [
        "Ramansh Sharma",
        "Matthew Lowery",
        "Houman Owhadi",
        "Varun Shankar"
      ],
      "published": "2026-02-17T10:20:46Z",
      "updated": "2026-02-17T10:20:46Z",
      "categories": [
        "physics.flu-dyn",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15472v1",
      "abs_url": "https://arxiv.org/abs/2602.15472v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    },
    {
      "id": "2602.15306v1",
      "title": "Sparse Additive Model Pruning for Order-Based Causal Structure Learning",
      "summary": "Causal structure learning, also known as causal discovery, aims to estimate causal relationships between variables as a form of a causal directed acyclic graph (DAG) from observational data. One of the major frameworks is the order-based approach that first estimates a topological order of the underlying DAG and then prunes spurious edges from the fully-connected DAG induced by the estimated topological order. Previous studies often focus on the former ordering step because it can dramatically reduce the search space of DAGs. In practice, the latter pruning step is equally crucial for ensuring both computational efficiency and estimation accuracy. Most existing methods employ a pruning technique based on generalized additive models and hypothesis testing, commonly known as CAM-pruning. How",
      "authors": [
        "Kentaro Kanamori",
        "Hirofumi Suzuki",
        "Takuya Takagi"
      ],
      "published": "2026-02-17T02:06:42Z",
      "updated": "2026-02-17T02:06:42Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15306v1",
      "abs_url": "https://arxiv.org/abs/2602.15306v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.15304v1",
      "title": "Hybrid Federated and Split Learning for Privacy Preserving Clinical Prediction and Treatment Optimization",
      "summary": "Collaborative clinical decision support is often constrained by governance and privacy rules that prevent pooling patient-level records across institutions. We present a hybrid privacy-preserving framework that combines Federated Learning (FL) and Split Learning (SL) to support decision-oriented healthcare modeling without raw-data sharing. The approach keeps feature-extraction trunks on clients while hosting prediction heads on a coordinating server, enabling shared representation learning and exposing an explicit collaboration boundary where privacy controls can be applied. Rather than assuming distributed training is inherently private, we audit leakage empirically using membership inference on cut-layer representations and study lightweight defenses based on activation clipping and add",
      "authors": [
        "Farzana Akter",
        "Rakib Hossain",
        "Deb Kanna Roy Toushi",
        "Mahmood Menon Khan",
        "Sultana Amin",
        "Lisan Al Amin"
      ],
      "published": "2026-02-17T01:57:27Z",
      "updated": "2026-02-17T01:57:27Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15304v1",
      "abs_url": "https://arxiv.org/abs/2602.15304v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.15298v1",
      "title": "X-MAP: eXplainable Misclassification Analysis and Profiling for Spam and Phishing Detection",
      "summary": "Misclassifications in spam and phishing detection are very harmful, as false negatives expose users to attacks while false positives degrade trust. Existing uncertainty-based detectors can flag potential errors, but possibly be deceived and offer limited interpretability. This paper presents X-MAP, an eXplainable Misclassification Analysis and Profilling framework that reveals topic-level semantic patterns behind model failures. X-MAP combines SHAP-based feature attributions with non-negative matrix factorization to build interpretable topic profiles for reliably classified spam/phishing and legitimate messages, and measures each message's deviation from these profiles using Jensen-Shannon divergence. Experiments on SMS and phishing datasets show that misclassified messages exhibit at leas",
      "authors": [
        "Qi Zhang",
        "Dian Chen",
        "Lance M. Kaplan",
        "Audun J√∏sang",
        "Dong Hyun Jeong",
        "Feng Chen",
        "Jin-Hee Cho"
      ],
      "published": "2026-02-17T01:46:08Z",
      "updated": "2026-02-17T01:46:08Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15298v1",
      "abs_url": "https://arxiv.org/abs/2602.15298v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "benchmarks"
    },
    {
      "id": "2602.15281v1",
      "title": "High-Fidelity Network Management for Federated AI-as-a-Service: Cross-Domain Orchestration",
      "summary": "To support the emergence of AI-as-a-Service (AIaaS), communication service providers (CSPs) are on the verge of a radical transformation-from pure connectivity providers to AIaaS a managed network service (control-and-orchestration plane that exposes AI models). In this model, the CSP is responsible not only for transport/communications, but also for intent-to-model resolution and joint network-compute orchestration, i.e., reliable and timely end-to-end delivery. The resulting end-to-end AIaaS service thus becomes governed by communications impairments (delay, loss) and inference impairments (latency, error). A central open problem is an operational AIaaS control-and-orchestration framework that enforces high fidelity, particularly under multi-domain federation. This paper introduces an as",
      "authors": [
        "Merve Saimler",
        "Mohaned Chraiti",
        "Ozgur Ercetin"
      ],
      "published": "2026-02-17T00:40:04Z",
      "updated": "2026-02-17T00:40:04Z",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15281v1",
      "abs_url": "https://arxiv.org/abs/2602.15281v1",
      "relevance_score": 0,
      "matched_keywords": [],
      "category": "general"
    }
  ]
}