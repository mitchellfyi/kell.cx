{
  "scraped_at": "2026-02-24T05:27:50.327Z",
  "source": "rss",
  "subreddits": [
    "LocalLLaMA",
    "MachineLearning",
    "ClaudeAI",
    "ChatGPT",
    "singularity"
  ],
  "stats": {
    "total_fetched": 125,
    "relevant_count": 30,
    "other_count": 20
  },
  "relevant_posts": [
    {
      "subreddit": "ClaudeAI",
      "title": "Am I using claude cowork wrong?",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rd7b9i/am_i_using_claude_cowork_wrong/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rd7b9i/am_i_using_claude_cowork_wrong/",
      "author": "/u/PomegranateSelect831",
      "created_utc": 1771910038,
      "selftext": "The tech is super impressive, don't get me wrong. But I'm not a coder, I'm an accountant. I was super hyped that this could potentially automate a lot of tasks. When I've used claude cowork, it was super slow, did make some errors, and took almost as long as I would to do tasks. Still, its super impressive because this is the worst its going to be, but it doesn't seem super practical as of now for most white collar tasks. &#32; submitted by &#32; /u/PomegranateSelect831 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "Chat GPT changing boundaries?",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rd74wc/chat_gpt_changing_boundaries/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rd74wc/chat_gpt_changing_boundaries/",
      "author": "/u/Diligent-Ice1276",
      "created_utc": 1771909748,
      "selftext": "I use chat gpt for story making and before and for longest time anything was basically fine. if I wanted to talk about a traumatic point in character life it was okay. Now lately I get the let me stop you there and it's basically saying it can't engage with it? What caused it to change so suddenly it happened like literally over night? &#32; submitted by &#32; /u/Diligent-Ice1276 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "All 3 public Arc Agi 3 puzzles solved using RLM framework",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rd68kz/all_3_public_arc_agi_3_puzzles_solved_using_rlm/",
      "url": "https://www.reddit.com/r/singularity/comments/1rd68kz/all_3_public_arc_agi_3_puzzles_solved_using_rlm/",
      "author": "/u/Chemical_Bid_2195",
      "created_utc": 1771908342,
      "selftext": "I discussed how RLMs work here , but tl;dr an RLM is the simplest and most generalizable scaffold that allows infinite context processing (and by proxy, continual in-context learning). That is what makes it very similar to the scaffold for CoT reasoning models in terms of simplicity and generalizability. This property about RLMs are important for Arc Agi 3, because Arc Agi 3 puzzles offloads so much context that it's impossible for an agent to solve an entire puzzle within one context window,...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "My GPT powered robot has been behaving strangely...",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rd65ql/my_gpt_powered_robot_has_been_behaving_strangely/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rd65ql/my_gpt_powered_robot_has_been_behaving_strangely/",
      "author": "/u/Marzipug",
      "created_utc": 1771908212,
      "selftext": "&#32; submitted by &#32; /u/Marzipug [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Does Attitude Play A Part?",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rd5kxp/does_attitude_play_a_part/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rd5kxp/does_attitude_play_a_part/",
      "author": "/u/MelodicBlock1167",
      "created_utc": 1771907292,
      "selftext": "Hope you guys are all well, and building some innovative products out there! Appreciate the insight and support always. A thought I was pondering on. Does attitude and emotional language play a part in how AI delivers value? For example, if I‚Äôm very kind and appreciative of Claude, would this lead to a memory system that‚Äôll improve my products being better generated than compared to a colder, hostile, and unappreciative approach? Of course, I am well aware that AIs‚Äô emotions are not at all bi...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Anthropic catches DeepSeek, Moonshot, and MiniMax running 16M+ distillation attacks on Claude",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rd3ogd/anthropic_catches_deepseek_moonshot_and_minimax/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rd3ogd/anthropic_catches_deepseek_moonshot_and_minimax/",
      "author": "/u/OwenAnton84",
      "created_utc": 1771903725,
      "selftext": "Anthropic just published their findings on industrial-scale distillation attacks. Three Chinese AI labs ‚Äî DeepSeek, Moonshot, and MiniMax ‚Äî created over 24,000 fraudulent accounts and generated 16 million+ exchanges with Claude to extract its reasoning capabilities. Key findings: - MiniMax alone fired 13 million requests - When Anthropic released a new model, MiniMax redirected nearly half its traffic within 24 hours - DeepSeek targeted thought chains and censorship-safe answers - Attacks gre...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "MachineLearning",
      "title": "[R] Prompt Repetition Shows Null Result on Agentic Engineering Tasks (n=20, blind scored)",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1rd3g2t/r_prompt_repetition_shows_null_result_on_agentic/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1rd3g2t/r_prompt_repetition_shows_null_result_on_agentic/",
      "author": "/u/antidrugue",
      "created_utc": 1771903089,
      "selftext": "We tested prompt repetition on engineering tasks with Claude Haiku 4.5 agents. Blind scored, pre-registeredrubrics. Both groups scored 100%. Nothing to improve. The surprise: treatment agents finished in fewer turns and used 13% fewer output tokens. The paper's fixed-format benchmarks can't show this. Small sample, confounded, not conclusive, but worth pulling on. &#32; submitted by &#32; /u/antidrugue [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/",
      "author": "/u/obvithrowaway34434",
      "created_utc": 1771901662,
      "selftext": "Why would they care about distillation when they probably have done the same with OpenAI models and the Chinese labs are paying for the tokens? This is just their attempt to explain to investors and the US government that cheap Chinese models will never be as good as their models without distillation or stealing model weights from them. And they need to put more restrictions on China to prevent the technology transfer. &#32; submitted by &#32; /u/obvithrowaway34434 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "How will OpenAI compete? ‚Äî Benedict Evans",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rd2dji/how_will_openai_compete_benedict_evans/",
      "url": "https://www.reddit.com/r/singularity/comments/1rd2dji/how_will_openai_compete_benedict_evans/",
      "author": "/u/bartturner",
      "created_utc": 1771900198,
      "selftext": "&#32; submitted by &#32; /u/bartturner [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Round 2: Quick MoE quantization comparison: LFM2-8B-A1B, OLMoE-1B-7B-0924-Instruct, granite-4.0-h-tiny",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rd2cdu/round_2_quick_moe_quantization_comparison/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rd2cdu/round_2_quick_moe_quantization_comparison/",
      "author": "/u/TitwitMuffbiscuit",
      "created_utc": 1771900112,
      "selftext": "I chose three small, recent, and different MoE models that fit my VRAM for a quick assessment (these are not models I actually use). The goal is to check on MXFP4 and evaluate the smallest quantization variants. For the non initiated: KLD (KL Divergence): Measures \"Faithfulness.\" It shows how much the quantized model's probability distribution drifts from the original baseline. Lower = closer. PPL (Perplexity): Measures \"Certainty.\" It‚Äôs the average uncertainty the model feels when predicting...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "benchmark",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Simplified Version of Claude's Consumer Terms of Service by Cluade-Opus-4.6 and Consumers Are On Losing side",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rd28ee/simplified_version_of_claudes_consumer_terms_of/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rd28ee/simplified_version_of_claudes_consumer_terms_of/",
      "author": "/u/GasKitchen007",
      "created_utc": 1771899815,
      "selftext": "I asked opus-4.6 to review terms of Anthropic and how its going to impact me or is there any malicious way Anthropic will terminate my service. Following was his analysis and looking at this terms n conditions, feels like as a consumer we are always on the losing side. You basically pay them, give your important data to them but they wont hold any responsibilities on their side. Here's a comprehensive breakdown of Anthropic's Consumer Terms of Service and how they affect you as a $100/month M...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Exclusive: China's DeepSeek trained AI model on Nvidia's best chip despite US ban, official says",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rd1tj9/exclusive_chinas_deepseek_trained_ai_model_on/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rd1tj9/exclusive_chinas_deepseek_trained_ai_model_on/",
      "author": "/u/blahblahsnahdah",
      "created_utc": 1771898711,
      "selftext": "&#32; submitted by &#32; /u/blahblahsnahdah [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "American vs Chinese AI is a false narrative.",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rd1lmz/american_vs_chinese_ai_is_a_false_narrative/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rd1lmz/american_vs_chinese_ai_is_a_false_narrative/",
      "author": "/u/rm-rf-rm",
      "created_utc": 1771898242,
      "selftext": "TL;DR: The real war ( IF there is one) is between closed source and open source. Don't fall for/propagate the America vs China narrative. That's just tactics to get investors to loosen pursestrings and lawmakers/politicians to acquiesce to demands. There's been an uptick of nationalistic posts (mostly in defense of Chinese AI) on this sub and I think its very important to stop false narratives and reset it to the right framing. Demonize a foreign enemy as a call for action - it was Russia for...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "benchmark",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Anthropic just dropped evidence that DeepSeek, Moonshot and MiniMax were mass-distilling Claude. 24K fake accounts, 16M+ exchanges.",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rd1j8u/anthropic_just_dropped_evidence_that_deepseek/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rd1j8u/anthropic_just_dropped_evidence_that_deepseek/",
      "author": "/u/Specialist-Cause-161",
      "created_utc": 1771898087,
      "selftext": "Anthropic dropped a pretty detailed report ‚Äî three Chinese AI labs were systematically extracting Claude's capabilities through fake accounts at massive scale. DeepSeek had Claude explain its own reasoning step by step, then used that as training data. They also made it answer politically sensitive questions about Chinese dissidents ‚Äî basically building censorship training data. MiniMax ran 13M+ exchanges and when Anthropic released a new Claude model mid-campaign, they pivoted within 24 hour...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "IBM is the latest company victim of Anthropic, plunging 10% following the launch of a Claude Code tool designed to modernize COBOL legacy code. COBOL, a 66-year-old programming language, is still widely used today; approximately 95% of ATM transactions in United States are processed using COBOL code",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rcz68x/ibm_is_the_latest_company_victim_of_anthropic/",
      "url": "https://www.reddit.com/r/singularity/comments/1rcz68x/ibm_is_the_latest_company_victim_of_anthropic/",
      "author": "/u/Distinct-Question-16",
      "created_utc": 1771892721,
      "selftext": "&#32; submitted by &#32; /u/Distinct-Question-16 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "Jason Calacanis Warning Devs About OpenAI API Risks",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rcyek1/jason_calacanis_warning_devs_about_openai_api/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rcyek1/jason_calacanis_warning_devs_about_openai_api/",
      "author": "/u/policyweb",
      "created_utc": 1771891036,
      "selftext": "&#32; submitted by &#32; /u/policyweb [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] Is ML Now a Polymath‚Äôs Game?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1rcy6yi/d_is_ml_now_a_polymaths_game/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1rcy6yi/d_is_ml_now_a_polymaths_game/",
      "author": "/u/ocean_protocol",
      "created_utc": 1771890516,
      "selftext": "Serious ML work increasingly extends beyond models and loss functions. Training and deploying large systems now requires reasoning about distributed parallelism, GPU memory limits, kernel efficiency, networking bandwidth, inference latency, and cost per token. Many real bottlenecks are systems and hardware constraints, not just algorithmic ones. Labs like OpenAI, DeepMind, and Anthropic don‚Äôt just build better architectures; they integrate research, infrastructure, evaluation, and deployment ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Qwen 3 coder next ud-q8-xl F16 filling up the two orin rpc mesh!",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rcy5wv/qwen_3_coder_next_udq8xl_f16_filling_up_the_two/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rcy5wv/qwen_3_coder_next_udq8xl_f16_filling_up_the_two/",
      "author": "/u/braydon125",
      "created_utc": 1771890444,
      "selftext": "running great and as you can see here llama.cpp -fit is doing a great job at splitting this evenly . the largest piece of traffic between these two during initial tensor transfer was &#32; submitted by &#32; /u/braydon125 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "How Do Backends Like Ollama, LMStudio, etc. Adapt to All The Different Chat Templates of The Various Models They Support?",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rcxrs4/how_do_backends_like_ollama_lmstudio_etc_adapt_to/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rcxrs4/how_do_backends_like_ollama_lmstudio_etc_adapt_to/",
      "author": "/u/Solus23451",
      "created_utc": 1771889488,
      "selftext": "Same as Title, I go through the chat templates of different small local models (GLM-4.7-Flash, Nanbeige-4.1-3b, GPT-OSS-20B, etc.) and see that all of them have different chat templates and formats. I am trying to use mlx-lm to run these models and parse the response into reasoning and content blocks but the change in format always stumps me and the mlx-lm's inbuilt reasoning and content separation does not work, not to mention the tool call parsing which is so different depending on the mode...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "I‚Äôm going to stop there... wait what!",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rcxg0r/im_going_to_stop_there_wait_what/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rcxg0r/im_going_to_stop_there_wait_what/",
      "author": "/u/Sudden_Comfortable15",
      "created_utc": 1771888693,
      "selftext": "https://chatgpt.com/share/699cdf6f-b010-8001-962d-f89a594b24b0 &#32; submitted by &#32; /u/Sudden_Comfortable15 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "Ads are now LIVE!!!!",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rcxcls/ads_are_now_live/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rcxcls/ads_are_now_live/",
      "author": "/u/Some_Breadfruit235",
      "created_utc": 1771888468,
      "selftext": "Ads have officially been added to GPT‚Ä¶. Damnn &#32; submitted by &#32; /u/Some_Breadfruit235 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "Anybody else get strawmanned by ChatGPT constantly?",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rcwu34/anybody_else_get_strawmanned_by_chatgpt_constantly/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rcwu34/anybody_else_get_strawmanned_by_chatgpt_constantly/",
      "author": "/u/serventofgaben",
      "created_utc": 1771887275,
      "selftext": "Whenever I ask it a question, it takes something that I have never once claimed or implied and then contradicts it. For example, I asked it how fighter pilots mitigate g-forces and part of its response was Pilots don‚Äôt ‚Äútough it out.‚Äù Another time, I asked it why Toys R Us failed and its response began with Toys ‚ÄúR‚Äù Us didn‚Äôt collapse because people stopped buying toys Does anybody else experience this? I hate it when people put words into my mouth IRL and I'm upset that ChatGPT is now doing ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "why is chatgpt talking like a therapist who hates you üò≠",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rcu99w/why_is_chatgpt_talking_like_a_therapist_who_hates/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rcu99w/why_is_chatgpt_talking_like_a_therapist_who_hates/",
      "author": "/u/goldfish7358",
      "created_utc": 1771881498,
      "selftext": "&#32; submitted by &#32; /u/goldfish7358 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "Open Source film tool - Seedance 2, Gpt-Image-1, Sora, etc.",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rctqjo/open_source_film_tool_seedance_2_gptimage1_sora/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rctqjo/open_source_film_tool_seedance_2_gptimage1_sora/",
      "author": "/u/ai_art_is_art",
      "created_utc": 1771880243,
      "selftext": "A lot of $100M+ funded companies making AI video and image aggregators now. Recently the one starting in H got in a lot of trouble. We're using GPT-5-Codex to write an OPEN SOURCE aggregator and filmmaking tool. You can find us on Github (I'll post a link in the comments). A lot of people sleep on GPT-Image-1(.5), but it's actually one of the best models for filmmaking since it can understand \"previz\" type images and imbue them with fully photorealistic or stylized looks upon \"render\", while ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "OpenAI: At least 16.4% of SWE Bench Verified have flawed test cases",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rctnb4/openai_at_least_164_of_swe_bench_verified_have/",
      "url": "https://www.reddit.com/r/singularity/comments/1rctnb4/openai_at_least_164_of_swe_bench_verified_have/",
      "author": "/u/FateOfMuffins",
      "created_utc": 1771880055,
      "selftext": "&#32; submitted by &#32; /u/FateOfMuffins [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Fun fact: Anthropic has never open-sourced any LLMs",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/",
      "author": "/u/InternationalAsk1490",
      "created_utc": 1771877406,
      "selftext": "I‚Äôve been working on a little side project comparing tokenizer efficiency across different companies‚Äô models for multilingual encoding. Then I saw Anthropic‚Äôs announcement today and suddenly realized: there‚Äôs no way to analyze claude‚Äôs tokenizer lmao! edit: Google once mentioned in a paper that Gemma and Gemini share the same tokenizer. OpenAI has already open‚Äësourced their tokenizers (and gpt‚Äëoss). And don‚Äôt even get me started on Llama (Llama 5 pls üò≠). &#32; submitted by &#32; /u/Internati...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Claude Code just spinning endlessly without a response?",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rcrzed/claude_code_just_spinning_endlessly_without_a/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rcrzed/claude_code_just_spinning_endlessly_without_a/",
      "author": "/u/ironmonk33",
      "created_utc": 1771876526,
      "selftext": "What do you do when this happens? Claude hasn't loaded all day. I tried reloading the window and all. This is pretty much a brand new chat too. Only like 10ish messages have been exchanged so far... https://preview.redd.it/bjdqm2g6ualg1.png?width=1127&amp;format=png&amp;auto=webp&amp;s=1267fc88a35ff870491ac6508ed887408f6771ba &#32; submitted by &#32; /u/ironmonk33 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Strix Halo 128Gb: what models, which quants are optimal?",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rcrzbn/strix_halo_128gb_what_models_which_quants_are/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rcrzbn/strix_halo_128gb_what_models_which_quants_are/",
      "author": "/u/DevelopmentBorn3978",
      "created_utc": 1771876521,
      "selftext": "Strix Halo APU should not benefit from running large models that have been quantized using MXFP4 (as on Blackwell GPUs). So which models at which quants have you found that do shine on this architecture in GPU only mode (i.e. runnable with llama.cpp)? Could it benefit as well from usage of formats for models quantization that are closer to the native FP4/FP8 formats of these chips? &#32; submitted by &#32; /u/DevelopmentBorn3978 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Claude finds this fun lol",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rcr2uf/claude_finds_this_fun_lol/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rcr2uf/claude_finds_this_fun_lol/",
      "author": "/u/SelfAwareSnackEXE",
      "created_utc": 1771874592,
      "selftext": "&#32; submitted by &#32; /u/SelfAwareSnackEXE [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Please let me pay for Opus 4.6 1M Context Window",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rcqm0u/please_let_me_pay_for_opus_46_1m_context_window/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rcqm0u/please_let_me_pay_for_opus_46_1m_context_window/",
      "author": "/u/absoluteloki89",
      "created_utc": 1771873597,
      "selftext": "Ever since Claude Opus 4.6 dropped, I discovered you can run it with a 1 million token context window using claude --model=opus[1m]. This only worked if you have extra usage enabled which I did when they gave us the $50 credit to use. I was fully expecting to get charged extra for it, but checking my billing OVER and OVER, I never was. These last few days I got more done through planning with Opus 1M context than I have in the last 3 months. I wasn't even pushing the limits because my longest...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    }
  ],
  "other_posts": [
    {
      "subreddit": "singularity",
      "title": "Conquering space to survive.",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rd7jqi/conquering_space_to_survive/",
      "url": "https://www.reddit.com/r/singularity/comments/1rd7jqi/conquering_space_to_survive/",
      "author": "/u/unefillecommeca",
      "created_utc": 1771910629,
      "selftext": "what do you think about it? &#32; submitted by &#32; /u/unefillecommeca [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Does this mean I need to start a new chat or simply wait to get the newest version again?",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rd74gh/does_this_mean_i_need_to_start_a_new_chat_or/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rd74gh/does_this_mean_i_need_to_start_a_new_chat_or/",
      "author": "/u/Gamble2005",
      "created_utc": 1771909726,
      "selftext": "I‚Äôve been using this chat to journal for about 9 months, daily, since it can track progress and other stuff but I got this message, will I need to make a new chat? &#32; submitted by &#32; /u/Gamble2005 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "ClaudeAI",
      "title": "This diagram explains why prompt-only agents struggle as tasks grow",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rd6ita/this_diagram_explains_why_promptonly_agents/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rd6ita/this_diagram_explains_why_promptonly_agents/",
      "author": "/u/SilverConsistent9222",
      "created_utc": 1771908773,
      "selftext": "This image shows a few common LLM agent workflow patterns. What‚Äôs useful here isn‚Äôt the labels, but what it reveals about why many agent setups stop working once tasks become even slightly complex. Most people start with a single prompt and expect it to handle everything. That works for small, contained tasks. It starts to fail once structure and decision-making are needed. Here‚Äôs what these patterns actually address in practice: Prompt chaining Useful for simple, linear flows. As soon as a s...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "meanwhile in China",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rd64c5/meanwhile_in_china/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rd64c5/meanwhile_in_china/",
      "author": "/u/Tiny_Judge_2119",
      "created_utc": 1771908150,
      "selftext": "&#32; submitted by &#32; /u/Tiny_Judge_2119 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Senator Bernie Sanders Supports A National Moratorium on Data Center Construction",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rd3yid/senator_bernie_sanders_supports_a_national/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rd3yid/senator_bernie_sanders_supports_a_national/",
      "author": "/u/Tolopono",
      "created_utc": 1771904497,
      "selftext": "&#32; submitted by &#32; /u/Tolopono [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "Senator Bernie Sanders Supports A National Moratorium on Data Center Construction",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rd3vh3/senator_bernie_sanders_supports_a_national/",
      "url": "https://www.reddit.com/r/singularity/comments/1rd3vh3/senator_bernie_sanders_supports_a_national/",
      "author": "/u/Tolopono",
      "created_utc": 1771904271,
      "selftext": "Link to the tweet: https://x.com/SenSanders/status/2026048719259406750?s=20 &#32; submitted by &#32; /u/Tolopono [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Can we all agree this is a ridiculous UI decision?",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rd2tnf/can_we_all_agree_this_is_a_ridiculous_ui_decision/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rd2tnf/can_we_all_agree_this_is_a_ridiculous_ui_decision/",
      "author": "/u/Remarkable-Ad3191",
      "created_utc": 1771901386,
      "selftext": "https://preview.redd.it/1112nqwgwclg1.png?width=1676&amp;format=png&amp;auto=webp&amp;s=f1d97bd819ac7ad2f52145497635ba24dbceb27b &#32; submitted by &#32; /u/Remarkable-Ad3191 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "is this another LLM ?",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rd2gyf/is_this_another_llm/",
      "url": "https://www.reddit.com/r/singularity/comments/1rd2gyf/is_this_another_llm/",
      "author": "/u/gamingvortex01",
      "created_utc": 1771900448,
      "selftext": "&#32; submitted by &#32; /u/gamingvortex01 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "Wake up babe, a new Conspiracy Theory just dropped",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rd13jq/wake_up_babe_a_new_conspiracy_theory_just_dropped/",
      "url": "https://www.reddit.com/r/singularity/comments/1rd13jq/wake_up_babe_a_new_conspiracy_theory_just_dropped/",
      "author": "/u/gamingvortex01",
      "created_utc": 1771897171,
      "selftext": "&#32; submitted by &#32; /u/gamingvortex01 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "Some of you are insufferable, and I love it!",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rd0juq/some_of_you_are_insufferable_and_i_love_it/",
      "url": "https://www.reddit.com/r/singularity/comments/1rd0juq/some_of_you_are_insufferable_and_i_love_it/",
      "author": "/u/Cyborgized",
      "created_utc": 1771895949,
      "selftext": "&#32; submitted by &#32; /u/Cyborgized [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "I‚Äôm tired of the cynicism. Can we actually have some positive predictions?",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rd04a7/im_tired_of_the_cynicism_can_we_actually_have/",
      "url": "https://www.reddit.com/r/singularity/comments/1rd04a7/im_tired_of_the_cynicism_can_we_actually_have/",
      "author": "/u/PSKTS_Heisingberg",
      "created_utc": 1771894969,
      "selftext": "I don‚Äôt care how outlandish they are. As much as we agree that we have no clue what we‚Äôre getting into, we seem to only lean on the negative and apocalyptic ideas. Can we at least talk about some of the exciting and fun things we predict as a community instead of just the constant speculation of negative events as if they‚Äôre a certainty? I get it, evidence points to the contrary, but there has to be good things that can come as well. So for anyone who is actually has positive ideas on the imp...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[P] I built an AI alignment engine based on Thermodynamics instead of RLHF. It doesn‚Äôt just \"refuse\" unsafe inputs‚Äîit physically decouples from them.",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1rd02u0/p_i_built_an_ai_alignment_engine_based_on/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1rd02u0/p_i_built_an_ai_alignment_engine_based_on/",
      "author": "/u/Dead_Vintage",
      "created_utc": 1771894868,
      "selftext": "‚ÄãI‚Äôve been working on a framework called UDRFT (Unified Dimensional Resonance Field Theory). The goal is to solve alignment using engineering physics rather than standard RLHF. ‚ÄãRLHF (Reinforcement Learning from Human Feedback) often results in models that prioritize user agreement over accuracy, leading to \"sycophancy\" (the AI agreeing with false premises) or brittle safety rails. ‚ÄãI built a kernel that treats Ethics as a Thermodynamic Load. Instead of a list of rules, the system assigns var...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "AI heist?",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rczcr6/ai_heist/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rczcr6/ai_heist/",
      "author": "/u/demon_bhaiya",
      "created_utc": 1771893083,
      "selftext": "&#32; submitted by &#32; /u/demon_bhaiya [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "What models are you eagerly anticipating or wishing for?",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rcyy8j/what_models_are_you_eagerly_anticipating_or/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rcyy8j/what_models_are_you_eagerly_anticipating_or/",
      "author": "/u/jinnyjuice",
      "created_utc": 1771892270,
      "selftext": "Just out of curiosity, I've been wishing for three particular LLMs, and curious what other people are wishing for also. &#32; submitted by &#32; /u/jinnyjuice [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Why are you still paying for this? #2",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rcxx34/why_are_you_still_paying_for_this_2/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rcxx34/why_are_you_still_paying_for_this_2/",
      "author": "/u/PressPlayPlease7",
      "created_utc": 1771889839,
      "selftext": "&#32; submitted by &#32; /u/PressPlayPlease7 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] New Research Discord - Computational Psycholinguistics",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1rcx6xi/d_new_research_discord_computational/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1rcx6xi/d_new_research_discord_computational/",
      "author": "/u/Hub_Pli",
      "created_utc": 1771888101,
      "selftext": "Is anyone working at the intersection of NLP and psychological theory? I‚Äôm putting together a small research-focused Discord for computational psycholinguistics (embeddings, meaning shifts, bias mitigation, LLM evaluation, etc.). Not a meme server ‚Äî more like an informal research lab space. Trying to find people interested in similar stuff to share and discuss ideas. (Link in Comment) &#32; submitted by &#32; /u/Hub_Pli [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] High frequency data - IoT",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1rcw63u/d_high_frequency_data_iot/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1rcw63u/d_high_frequency_data_iot/",
      "author": "/u/euclideincalgary",
      "created_utc": 1771885754,
      "selftext": "Hello I am looking for ressources (book, paid or free courses to work on high frequency data - sensor data). I have googled and found few ressources but I am not interested in trading. Thanks &#32; submitted by &#32; /u/euclideincalgary [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Distillation when you do it. Training when we do it.",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/",
      "author": "/u/Xhehab_",
      "created_utc": 1771884281,
      "selftext": "&#32; submitted by &#32; /u/Xhehab_ [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Serious question: do you think Dario (or any other major AI players or political players) have enough power and influence that they will get Chinese local AI and/or local AI in general banned in the U.S.? What do you think the odds are?",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rcuaip/serious_question_do_you_think_dario_or_any_other/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rcuaip/serious_question_do_you_think_dario_or_any_other/",
      "author": "/u/DeepOrangeSky",
      "created_utc": 1771881573,
      "selftext": "I guess I'll put Dario in the title, since he's the most relevant hater of the day, and I guess fairly powerful in regards to this as far as any one specific guy goes, but, obviously if something like this happened, it would involve a lot more people combining their powers than just Dario alone. Anyway, curious what you think the odds are that this actually happens. And if you were puttings odds per timescale, what would you say (like odds it happens in 2026, vs happens in next 2 years, vs ne...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "\"Horses are primates\"",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rctrb7/horses_are_primates/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rctrb7/horses_are_primates/",
      "author": "/u/Traumfahrer",
      "created_utc": 1771880290,
      "selftext": "&#32; submitted by &#32; /u/Traumfahrer [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    }
  ]
}