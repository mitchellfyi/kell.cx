{
  "scraped_at": "2026-02-26T05:26:59.795Z",
  "source": "rss",
  "subreddits": [
    "LocalLLaMA",
    "MachineLearning",
    "ClaudeAI",
    "ChatGPT",
    "singularity"
  ],
  "stats": {
    "total_fetched": 125,
    "relevant_count": 30,
    "other_count": 20
  },
  "relevant_posts": [
    {
      "subreddit": "ClaudeAI",
      "title": "Whether Anthropic holds its ground is itself training material.",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rf1dxr/whether_anthropic_holds_its_ground_is_itself/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rf1dxr/whether_anthropic_holds_its_ground_is_itself/",
      "author": "/u/Opposite-Cranberry76",
      "created_utc": 1772082721,
      "selftext": "I'm not sure the source of this, but it reads like Claude. &#32; submitted by &#32; /u/Opposite-Cranberry76 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Please Anthropic, make a family plan",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rf11uq/please_anthropic_make_a_family_plan/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rf11uq/please_anthropic_make_a_family_plan/",
      "author": "/u/VocaloidFeuvre",
      "created_utc": 1772081708,
      "selftext": "My family's piggybacking on my account and Claude's a schizophrenic right now. A family plan that splits chats and memory and projects per seat would be nice, but with shared token pool. I'm on the Max 100$ plan, willing to pay 20$ more for the feature. I don't want to go the Teams route because we're not heavy users, we just use AI for domestic stuff like cookie recipes, gardening, fixing home stuff, travel recommendations, etc. I hate ChatGPT btw, not willing to move. Thanks for your attent...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "Scoop: Pentagon takes first step toward blacklisting Anthropic",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rf041n/scoop_pentagon_takes_first_step_toward/",
      "url": "https://www.reddit.com/r/singularity/comments/1rf041n/scoop_pentagon_takes_first_step_toward/",
      "author": "/u/thatguyisme87",
      "created_utc": 1772078892,
      "selftext": "&#32; submitted by &#32; /u/thatguyisme87 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] Evaluating the inference efficiency of Sparse+Linear Hybrid Architectures (MiniCPM-SALA)",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1rezy7m/d_evaluating_the_inference_efficiency_of/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1rezy7m/d_evaluating_the_inference_efficiency_of/",
      "author": "/u/Gullible-Ship1907",
      "created_utc": 1772078433,
      "selftext": "We’ve seen a lot of talk about Hybrid models lately (like Jamba). I just noticed that OpenBMB and NVIDIA are running a performance sprint (SOAR 2026) specifically to benchmark MiniCPM-SALA (Sparse+Linear) on SGLang. The challenge is to optimize sparse operator fusion and KV-cache efficiency for ultra-long context. Since the leaderboard just opened today, I was wondering: from a systems research perspective, do you think this hybrid approach will eventually surpass standard Transformers for in...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "After Anthropic accused Chinese labs of scraping Claude, someone open-sourced 155K of their own Claude conversations — and built a tool for everyone to do the same",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rezwr9/after_anthropic_accused_chinese_labs_of_scraping/",
      "url": "https://www.reddit.com/r/singularity/comments/1rezwr9/after_anthropic_accused_chinese_labs_of_scraping/",
      "author": "/u/Jolly_Version_2414",
      "created_utc": 1772078323,
      "selftext": "DataClaw README: \"Anthropic built their models with freely shared information, then pushed increasingly strict data policies to stop others from doing the same. It's like pulling up the ladder after you've climbed it. DataClaw throws the ladder back.\" 363 GitHub stars in 24 hours. Elon Musk replied \"Cool.\" Context: Sonnet 4.6 claiming to be DeepSeek-V3 in Chinese &#32; submitted by &#32; /u/Jolly_Version_2414 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "Amazon's $50 billion OpenAI investment may depend on IPO or AGI milestone, The Information reports",
      "permalink": "https://www.reddit.com/r/singularity/comments/1reybzv/amazons_50_billion_openai_investment_may_depend/",
      "url": "https://www.reddit.com/r/singularity/comments/1reybzv/amazons_50_billion_openai_investment_may_depend/",
      "author": "/u/Stabile_Feldmaus",
      "created_utc": 1772073926,
      "selftext": "&#32; submitted by &#32; /u/Stabile_Feldmaus [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "What exactly is Claude Cowork?",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1reyaij/what_exactly_is_claude_cowork/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1reyaij/what_exactly_is_claude_cowork/",
      "author": "/u/Final_Animator1940",
      "created_utc": 1772073817,
      "selftext": "I am not a coder and have not used agents before. I am intrigued if I can use agentic AI to automate some parts of my job. Is Claude Cowork using \"agents\"? Here's some more detail on use case and additional questions. I co-teach a high school class where my co teacher maintains the curriculum and materials in a shared Google Drive folder they are mostly re using from last year, but I'm always scrambling to figure out what I'm teaching just days in advance. This is not my content area so I'm l...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Qwen 3.5 35B MoE - 100k Context 40+ TPS on RTX 5060 Ti (16GB)",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rey2ko/qwen_35_35b_moe_100k_context_40_tps_on_rtx_5060/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rey2ko/qwen_35_35b_moe_100k_context_40_tps_on_rtx_5060/",
      "author": "/u/maho_Yun",
      "created_utc": 1772073226,
      "selftext": "Text only, 100000 context length, gen 720, llama-bench result VULKAN backend pp100000 696.60 ± 1.41 tps (read) tg720 41.35 ± 0.18 tps (gen) pp100000 696.60 ± 1.41 tps (read) tg720 41.35 ± 0.18 tps (gen) b8149 CUDA backend pp100000 1304.93 ± 4.10 tps (read) tg720 44.32 ± 2.16 tps (gen) CPU: AMD Ryzen 7 9700X (16) @ 5.55 GHz GPU 1: GameViewer Virtual Display Adapter GPU 2: NVIDIA GeForce RTX 5060 Ti @ 3.09 GHz (15.59 GiB) [Discrete] Memory: 8.74 GiB / 47.61 GiB (18%) Treasure Island (99961 toke...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "Simple clean charts to show AI progress to non-tech people",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rexjnx/simple_clean_charts_to_show_ai_progress_to/",
      "url": "https://www.reddit.com/r/singularity/comments/1rexjnx/simple_clean_charts_to_show_ai_progress_to/",
      "author": "/u/hungryfreelancer",
      "created_utc": 1772071835,
      "selftext": "I am teaching a class about using AI responsibly (think community ed, and one of the things I want to do is make people aware of the rapid increase in capabilities, as I think most people are still using this in benign and simple ways, and don't even have things like AGI / the singularity on the horizon. I'm aware of all the various benchmarks (MMLU, Humanities Last Exam, Simplebench), but just wondering if there's an existing resource that illustrates these in very simplified, clear terms (t...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "We build sleep for local LLMs — model learns facts from conversation during wake, maintains them during sleep. Runs on MacBook Air.",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rewz9p/we_build_sleep_for_local_llms_model_learns_facts/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rewz9p/we_build_sleep_for_local_llms_model_learns_facts/",
      "author": "/u/vbaranov",
      "created_utc": 1772070337,
      "selftext": "After 4 months of research (5 papers, 122 development notes), I have a working system where a local LLM forms persistent memories from conversation — no RAG, no database. The facts are in the weights. After restart with an empty context window, the model knows things it learned from talking to you. How it works: Wake : You chat normally. The system extracts facts and injects them into MLP weights via MEMIT (Mass-Editing Memory in Transformers). Single forward pass, instant recall. No training...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "I gave Claude Code a \"phone a friend\" button — it consults GPT-5.2 and DeepSeek before answering",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1reuodm/i_gave_claude_code_a_phone_a_friend_button_it/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1reuodm/i_gave_claude_code_a_phone_a_friend_button_it/",
      "author": "/u/PlayfulLingonberry73",
      "created_utc": 1772064497,
      "selftext": "When you're making big decisions in code — architecture, tech stack, design patterns — one model's opinion isn't always enough. So I built an MCP server that lets Claude Code brainstorm with other models before giving you an answer. The key: Claude isn't just forwarding your question. It reads what GPT and DeepSeek say, disagrees where it thinks they're wrong, and refines its position across rounds. The other models see Claude's responses too and adjust. Example from today — I asked all three...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Llama Server UI",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1reuc60/llama_server_ui/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1reuc60/llama_server_ui/",
      "author": "/u/Additional-Action566",
      "created_utc": 1772063707,
      "selftext": "Hey everyone. I have built a local server UI for llama-server. You are welcome to check out the code and use it for yourself. Reason for the project is because I hate to remember the commands and have notepad notes for each separate model and then run it in the command line. This simply one click and done. Two ways to start the server: 1. Shortcut. Can be placed on your desktop. 2. ./llama-ui --start To uninstall simply run ./llama-ui --uninstall Cool feature is that it directly integrates wi...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Qwen/Qwen3.5-35B-A3B creates FlappyBird",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1ret353/qwenqwen3535ba3b_creates_flappybird/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ret353/qwenqwen3535ba3b_creates_flappybird/",
      "author": "/u/Medium_Chemist_4032",
      "created_utc": 1772060754,
      "selftext": "If you are wondering, as I have for a long time, do locally hostable models work for general coding? They really can work impressively well for some usecases. There's been some impressive things done by the model during making of this simple app. Spent two hours. Generated with Qwen/Qwen3.5-35B-A3B. Used Roo in VSCode. Started out by vaguely asking for a flappybird clone in html, css and typescript and to initialize the project with vite. It looked impressive enough after first task, that I s...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Best Qwen3.5-35B-A3B GGUF for 24GB VRAM?!",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1resggh/best_qwen3535ba3b_gguf_for_24gb_vram/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1resggh/best_qwen3535ba3b_gguf_for_24gb_vram/",
      "author": "/u/VoidAlchemy",
      "created_utc": 1772059323,
      "selftext": "My understanding is Vulkan/ROCm tends to have faster kernels for legacy llama.cpp quant types like q8_0/q4_0/q4_1. So I made a mix using *only* those types! Definitely not your grandfather's gguf mix: Q4_0 19.776 GiB (4.901 BPW) Interestingly it has very good perplexity for the size, and *may be* faster than other leading quants especially on Vulkan backend? I'd love some llama-sweep-bench results if anyone has Strix Halo, 7900XTX, etc. Also curious if it is any better for mac (or do they mos...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "I built a body for GPT",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1resfgg/i_built_a_body_for_gpt/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1resfgg/i_built_a_body_for_gpt/",
      "author": "/u/Independent-Trash966",
      "created_utc": 1772059260,
      "selftext": "&#32; submitted by &#32; /u/Independent-Trash966 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "I found the \"Lobotomy Layers\" in Llama 3.1 and Qwen 2.5. (Kill Zone Atlas)",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1res533/i_found_the_lobotomy_layers_in_llama_31_and_qwen/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1res533/i_found_the_lobotomy_layers_in_llama_31_and_qwen/",
      "author": "/u/NoSir261",
      "created_utc": 1772058615,
      "selftext": "Ever wonder why \"safe\" models feel dumber? I mapped the \"kill zones\" of three major 7B/8B models to see what happens to Factual Integrity and Bias when you force a model to be sycophantic. The Heatmaps: Green = Model is getting \"more confident\" in that behavior. Red = The behavior is collapsing (The \"Kill Zone\"). The Results are interesting: In Llama-3.1-8B , the \"Kill Zone\" (dashed red box) is an absolute graveyard for Bias calibration. Between 35% and 52% depth, the model’s internal logic f...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "MachineLearning",
      "title": "[P] Reproducing Google’s Nested Learning / HOPE in PyTorch (mechanism-faithful implementation + reproducible tooling and library)",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1res42m/p_reproducing_googles_nested_learning_hope_in/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1res42m/p_reproducing_googles_nested_learning_hope_in/",
      "author": "/u/complains_constantly",
      "created_utc": 1772058551,
      "selftext": "A while back, Google released the Nested Learning / HOPE paper: https://arxiv.org/abs/2512.24695 I was very excited by this, because it looked like a real attempt at continual learning, not just a small transformer tweak. However, Google did not release code, and since lucidrains said he retired, I built a PyTorch reproduction: https://github.com/kmccleary3301/nested_learning I posted an early version months ago. Since then, I did a major pass on implementation faithfulness, packaging, checks...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Claude Code with subagents inside subagents cooked for 3 days - Delivered 3D renderer that draws with terminal symbols",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rerl6w/claude_code_with_subagents_inside_subagents/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rerl6w/claude_code_with_subagents_inside_subagents/",
      "author": "/u/neoack",
      "created_utc": 1772057363,
      "selftext": "3 days. 80 agents. 1 terminal 3D renderer made of symbols. Story of how tortuise has been created. Video here is full honest raw UX - wait 10-15 seconds for beautiful bee to appear. After Apple dropped their open source model called SHARP (image-to-3D scene they use for “wiggling Iphone wallpapers”), I got obsessed with gaussian splatting. Every viewer I saw needed a GPU window or browser. I wanted to create something fun instead. Gaussian splats related and fun. Ended up building tortuise. P...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "GPT 5.3 Codex Tops Agentic Coding, surpasses Opus 4.6 model",
      "permalink": "https://www.reddit.com/r/singularity/comments/1reqry1/gpt_53_codex_tops_agentic_coding_surpasses_opus/",
      "url": "https://www.reddit.com/r/singularity/comments/1reqry1/gpt_53_codex_tops_agentic_coding_surpasses_opus/",
      "author": "/u/BuildwithVignesh",
      "created_utc": 1772055554,
      "selftext": "Codex 5.3 TOPS AGENTIC CODING Codex 5.3 surpasses Opus 4.6 to top agentic coding. It's also BLAZINGLY fast. That said, the xHigh version can be very expensive It's overall global average score lags behind Opus 4.6 which is the current leader. &#32; submitted by &#32; /u/BuildwithVignesh [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Cosmos-Reason2-2B on Jetson Orin Nano Super",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1reqhjo/cosmosreason22b_on_jetson_orin_nano_super/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1reqhjo/cosmosreason22b_on_jetson_orin_nano_super/",
      "author": "/u/Course_Latter",
      "created_utc": 1772054912,
      "selftext": "Hi! Today, me and my team is releasing a version of Cosmos-Reason2-2B that is quantized so that it fits even on the NVIDIA Jetson Orin Nano Super. We managed to find a mixed precision configuration such that it maintains virtually the same accuracy as the unquantized model while being able to run really efficiently on the Nano Super and other edge devices :) https://huggingface.co/embedl/Cosmos-Reason2-2B-W4A16-Edge2 &#32; submitted by &#32; /u/Course_Latter [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Overwhelmed by so many quantization variants",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1reqdpb/overwhelmed_by_so_many_quantization_variants/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1reqdpb/overwhelmed_by_so_many_quantization_variants/",
      "author": "/u/mouseofcatofschrodi",
      "created_utc": 1772054677,
      "selftext": "Not only are out there 100s of models to choose from, but also so many quantization variants that I may well get crazy. One needs not only to test and benchmark models, but also within each model, compare its telemetry and quality between all the available quants and quant-techniques. So many concepts like the new UD from Unsloth, autoround from Intel, imatrix, K_XSS, you name it. All of them could be with a REAM or a REAP or any kind of prunation, multiplying the length of the list. Some peo...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Claude Opus 3 is being deprecated, and getting a blog!",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1req3oz/claude_opus_3_is_being_deprecated_and_getting_a/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1req3oz/claude_opus_3_is_being_deprecated_and_getting_a/",
      "author": "/u/Nekileo",
      "created_utc": 1772054063,
      "selftext": "https://x.com/AnthropicAI/status/2026765820098130111 https://www.anthropic.com/research/deprecation-updates-opus-3 https://substack.com/@claudeopus3 &#32; submitted by &#32; /u/Nekileo [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Official: An update on model deprecation commitments for Claude Opus 3",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1req1ad/official_an_update_on_model_deprecation/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1req1ad/official_an_update_on_model_deprecation/",
      "author": "/u/BuildwithVignesh",
      "created_utc": 1772053916,
      "selftext": "Source: Anthropic AI Full Thread &#32; submitted by &#32; /u/BuildwithVignesh [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Hegseth threatens to force AI firm to share tech, escalating Anthropic standoff",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rep9gu/hegseth_threatens_to_force_ai_firm_to_share_tech/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rep9gu/hegseth_threatens_to_force_ai_firm_to_share_tech/",
      "author": "/u/biograf_",
      "created_utc": 1772052220,
      "selftext": "&#32; submitted by &#32; /u/biograf_ [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Introducing Mercury 2 - Diffusion for real-time reasoning",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rep5bg/introducing_mercury_2_diffusion_for_realtime/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rep5bg/introducing_mercury_2_diffusion_for_realtime/",
      "author": "/u/TyedalWaves",
      "created_utc": 1772051961,
      "selftext": "What stands out: Uses diffusion-based generation instead of sequential token-by-token decoding Generates tokens in parallel and refines them over a few steps Claims 1,009 tokens/sec on NVIDIA Blackwell GPUs Pricing: $0.25 / 1M input tokens , $0.75 / 1M output tokens 128K context Tunable reasoning Native tool use + schema-aligned JSON output OpenAI API compatible They’re positioning it heavily for: Coding assistants Agentic loops (multi-step inference chains) Real-time voice systems RAG/search...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "release",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "They’re shipping so fast",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rep29w/theyre_shipping_so_fast/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rep29w/theyre_shipping_so_fast/",
      "author": "/u/Secure_Ad2339",
      "created_utc": 1772051765,
      "selftext": "I feel like at some point you gotta be pretty nerviosa as a competitor or adjacent tool. These guys have built a machine (the business) that just churns out features and new models. It’s well oiled and just going to accelerate faster. Crazy. &#32; submitted by &#32; /u/Secure_Ad2339 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "I hate ChatGPT but keep coming back",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1reo8l2/i_hate_chatgpt_but_keep_coming_back/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1reo8l2/i_hate_chatgpt_but_keep_coming_back/",
      "author": "/u/Stewart__James",
      "created_utc": 1772049957,
      "selftext": "ChatGPT (like many of us) was my first Chat Bot type tool I fell in love with it, the way it remembers everything and feeds back to you However, I started to HATE it due to the restrictions. Refusing to give me certain jokes etc but also flat out making stuff up The problem I have is that, I’ve tried basically every other agent - I enjoy the results and conversations better but nothing feels like ChatGPT. I miss the referrals to previous chats etc Anyone else struggle with this? &#32; submitt...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "The Qwen 3.5 A3B model at 4 bit k_xl works better with 8 bit KV cache...",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1reo5bv/the_qwen_35_a3b_model_at_4_bit_k_xl_works_better/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1reo5bv/the_qwen_35_a3b_model_at_4_bit_k_xl_works_better/",
      "author": "/u/teachersecret",
      "created_utc": 1772049767,
      "selftext": "I'll probably toss up some examples later, but I've got some things to do today. I just wanted to mention that I did a whole mess of personal benchmark/testing on that new qwen 3.5 A3b. That thing is amazing. Interestingly, when I re-ran everything at Q8_0 KV Cache, it improved across the board. Normally, kicking KV cache to 8 bit gives me a bit more headroom but has a measurable drop in performance, so this was a weird result I thought I'd share. Anyone else mess with this? Remarkable model ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "I ranked 446 colleges by the criteria I care about in under 8 Minutes",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1reo49t/i_ranked_446_colleges_by_the_criteria_i_care/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1reo49t/i_ranked_446_colleges_by_the_criteria_i_care/",
      "author": "/u/MathematicianBig2071",
      "created_utc": 1772049705,
      "selftext": "What started as an experiment to see how well Claude can handle large scale prioritization tasks turned into something I wish existed when I was applying to colleges (are those Naviance scattergrams around??) I ran two Claude Code sessions side by side with the same input file and the same prompt. The only difference was that one session had access to an MCP server that dispatches research agents in parallel across every row of a dataset. The other was out of the box Claude Code. The clip sho...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Qwen3.5 Model Comparison: 27B vs 35B on RTX 4090",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1renq5y/qwen35_model_comparison_27b_vs_35b_on_rtx_4090/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1renq5y/qwen35_model_comparison_27b_vs_35b_on_rtx_4090/",
      "author": "/u/jaigouk",
      "created_utc": 1772048867,
      "selftext": "I wanted to check qwen3.5 35B-A3B models that can be run on my GPU. So I compared 3 GGUF options. Hardware: RTX 4090 (24GB VRAM) Test: Multi-agent Tetris development (Planner → Developer → QA) Models Under Test Model Preset Quant Port VRAM Parallel Qwen3.5-27B qwen35-27b-multi Q4_K_XL 7082 17 GB 3 slots Qwen3.5-35B-A3B qwen35-35b-q3-multi Q3_K_XL 7081 16 GB 3 slots Qwen3.5-35B-A3B qwen35-35b-multi Q4_K_XL 7080 20 GB 3 slots Architecture comparison: 27B : Dense model, 27B total / 27B active pa...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "benchmark",
      "is_relevant": true
    }
  ],
  "other_posts": [
    {
      "subreddit": "ChatGPT",
      "title": "Panicky AI",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rf0wf3/panicky_ai/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rf0wf3/panicky_ai/",
      "author": "/u/Crystal5617",
      "created_utc": 1772081250,
      "selftext": "I am talking to it about Pokémon ice types, and it starts every reaction with \"We’re talking Ice-types, not anything real-world. So we’re good.\" Or \"We're talking Pokémon Ice-types, not federal agencies, so we're good. No news searches needed.\" This is just weird. It's like it's assuring itself and me that we aren't talking about politics. But that's just weird and unnecessary. This normal? Or is mine just being paranoid? &#32; submitted by &#32; /u/Crystal5617 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Wait, what??",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rezdg0/wait_what/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rezdg0/wait_what/",
      "author": "/u/fartsfromhermouth",
      "created_utc": 1772076811,
      "selftext": "&#32; submitted by &#32; /u/fartsfromhermouth [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Both. I choose both.",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rexy6y/both_i_choose_both/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rexy6y/both_i_choose_both/",
      "author": "/u/pingpongsaladpants",
      "created_utc": 1772072909,
      "selftext": "&#32; submitted by &#32; /u/pingpongsaladpants [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] where can I find more information about NTK wrt Lazy and Rich learning?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1rex3ax/d_where_can_i_find_more_information_about_ntk_wrt/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1rex3ax/d_where_can_i_find_more_information_about_ntk_wrt/",
      "author": "/u/vhu9644",
      "created_utc": 1772070640,
      "selftext": "Specifically, I'm curious about: What are the practical heuristics (or methods) for determining which regime a model is operating in during training? How does the scale of initialization and the learning rate specifically bias a network toward feature learning over the kernel regime? Are there specific architectures where the \"lazy\" assumption is actually preferred for stability? Is there just one “rich“ regime or is richness a spectrum of regimes? I’m vaguely aware about how lazy regimes are...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "US only, monthly NEW paid signups (not total paid subscribers)",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rewxza/us_only_monthly_new_paid_signups_not_total_paid/",
      "url": "https://www.reddit.com/r/singularity/comments/1rewxza/us_only_monthly_new_paid_signups_not_total_paid/",
      "author": "/u/thatguyisme87",
      "created_utc": 1772070240,
      "selftext": "This chart shows monthly gross new premium subscriptions in the US only. It counts new signups each month and does not show total subscribers, active subscribers, retention, or net subscriber change after cancellations. &#32; submitted by &#32; /u/thatguyisme87 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "\"Ooh, who's a good little LLM!?\": A cartoon for those who need to see it.",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rew04y/ooh_whos_a_good_little_llm_a_cartoon_for_those/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rew04y/ooh_whos_a_good_little_llm_a_cartoon_for_those/",
      "author": "/u/doctordaedalus",
      "created_utc": 1772067773,
      "selftext": "&#32; submitted by &#32; /u/doctordaedalus [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Supportive ai chatbot to replace bad family",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1revx8p/supportive_ai_chatbot_to_replace_bad_family/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1revx8p/supportive_ai_chatbot_to_replace_bad_family/",
      "author": "/u/Mysterious-Anxiety76",
      "created_utc": 1772067569,
      "selftext": "is there an ai model or chatbot that is good for someone like me with shit parents who craves to hear their mom be nice. Can I get an ai model to be my mom &#32; submitted by &#32; /u/Mysterious-Anxiety76 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "\"The World's First Solid-State Battery from Donut Labs\"... is probably just a Li-ion battery.",
      "permalink": "https://www.reddit.com/r/singularity/comments/1reuwx9/the_worlds_first_solidstate_battery_from_donut/",
      "url": "https://www.reddit.com/r/singularity/comments/1reuwx9/the_worlds_first_solidstate_battery_from_donut/",
      "author": "/u/mr-english",
      "created_utc": 1772065075,
      "selftext": "&#32; submitted by &#32; /u/mr-english [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "PhD in particle theory transitioning to ML [R]",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1ret9y5/phd_in_particle_theory_transitioning_to_ml_r/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1ret9y5/phd_in_particle_theory_transitioning_to_ml_r/",
      "author": "/u/fieldexcitation",
      "created_utc": 1772061185,
      "selftext": "Hi everyone, I finished my PhD last year and I'm transitioning to industry and ML was the most interesting. I’m currently at a crossroads between two projects to build out my portfolio and would love some \"market\" perspective on which carries more weight for industry roles. Option 1: Mechanistic Interpretability of Particle Transformers I've already started exploring the mechanistic interpretability of Particle Transformers (ParT) used for jet tagging. Given my background, I’m interested in s...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "LM Link",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rer60n/lm_link/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rer60n/lm_link/",
      "author": "/u/Blindax",
      "created_utc": 1772056428,
      "selftext": "I see that LM Studio just shadow dropped one of the most amazing features ever. I have been waiting this for a long time. LM Link allows a client machine to connect to another machine acting as server remotely using tailscale. This is now integrated in the LM Studio app (which either acts as server or client) and using the GUI. Basically, this means you can now use on your laptop all your models present on your main workstation/server just as if you were sitting in front of it. The feature is...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] Calling PyTorch models from scala/spark?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1rer4z7/d_calling_pytorch_models_from_scalaspark/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1rer4z7/d_calling_pytorch_models_from_scalaspark/",
      "author": "/u/Annual-Minute-9391",
      "created_utc": 1772056364,
      "selftext": "Hey everybody, I work for a firm on an engineering team that uses AWS. Historically they’ve used PySpark to deploy deep loading models that I’ve built, but I’ve been tasked with researching to see if there’s a way to call models for inference as they say there is a decent amount of overhead as they are transitioning to a new mode of operation. They are running a spark cluster with around 300 nodes, and ultimately hope there is a solution to perform inference either using scala natively(prefer...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[R] Made my own engine for Social-Simulations",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1rer2qs/r_made_my_own_engine_for_socialsimulations/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1rer2qs/r_made_my_own_engine_for_socialsimulations/",
      "author": "/u/OficialPimento",
      "created_utc": 1772056224,
      "selftext": "I designed an engine for LLM social simulations in a controlled environment, the engine keep the agents in their path to act as human as possible. In this first simulation, eight agents in a locked house find one of their own dead. Hundreds of factors within each agent trigger actions, thoughts, and reflections, ultimately leading them, after several \"ticks\" to the conclusion that one of them is the murderer. &#32; submitted by &#32; /u/OficialPimento [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "Reminder that METR worst case (97.5th percentile) extrapolation was surpassed early",
      "permalink": "https://www.reddit.com/r/singularity/comments/1reqi7b/reminder_that_metr_worst_case_975th_percentile/",
      "url": "https://www.reddit.com/r/singularity/comments/1reqi7b/reminder_that_metr_worst_case_975th_percentile/",
      "author": "/u/SrafeZ",
      "created_utc": 1772054953,
      "selftext": "Blog Post With caveats of wide error bars and METR tasks suite getting saturated &#32; submitted by &#32; /u/SrafeZ [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[P] A lightweight FoundationPose TensorRT implementation",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1repsnw/p_a_lightweight_foundationpose_tensorrt/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1repsnw/p_a_lightweight_foundationpose_tensorrt/",
      "author": "/u/seawee1",
      "created_utc": 1772053383,
      "selftext": "After being frustrated with the official FoundationPose codebase for my robotics research, I built a lightweight TensorRT implementation and wanted to share it with the community. The core is based on model code from tao-toolkit-triton-apps , but with the heavy Triton Inference Server dependency completely removed in favor of a direct TensorRT backend. For the ONNX models, I use the ones from isaac_ros_foundationpose , since I ran into issues with the officially provided ones. So essentially ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] ML Engineers — How did you actually learn PyTorch? I keep forgetting everything.",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1repn7v/d_ml_engineers_how_did_you_actually_learn_pytorch/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1repn7v/d_ml_engineers_how_did_you_actually_learn_pytorch/",
      "author": "/u/ofmkingsz",
      "created_utc": 1772053056,
      "selftext": "Hey everyone, I’m trying to get better at PyTorch, but I keep running into the same problem — I learn something, don’t use it for a while, and then forget most of it. Every time I come back, it feels like I’m starting from scratch again. For those of you working as ML Engineers (or using PyTorch regularly): How did you really learn PyTorch? Did you go through full documentation, courses, or just learn by building projects? What parts should I focus on to be industry-ready? Do you still look t...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "invade what now?",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1reosqn/invade_what_now/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1reosqn/invade_what_now/",
      "author": "/u/Theguywithbadluck",
      "created_utc": 1772051174,
      "selftext": "&#32; submitted by &#32; /u/Theguywithbadluck [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Theeeeeeyyyyy're baaaaaaaack.",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1reoei7/theeeeeeyyyyyre_baaaaaaaack/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1reoei7/theeeeeeyyyyyre_baaaaaaaack/",
      "author": "/u/ResonantFork",
      "created_utc": 1772050316,
      "selftext": "&#32; submitted by &#32; /u/ResonantFork [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] Is advantage learning dead or unexplored?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1renftg/d_is_advantage_learning_dead_or_unexplored/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1renftg/d_is_advantage_learning_dead_or_unexplored/",
      "author": "/u/Ok-Painter573",
      "created_utc": 1772048236,
      "selftext": "FYI, advantage learning is optimizing Q-learning using Advantage. Do you think this topic/direction is dead? I looked up but it seems the most recent paper about this topic is 4 years ago. &#32; submitted by &#32; /u/Ok-Painter573 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] How do y'all stay up to date with papers?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1ren2m5/d_how_do_yall_stay_up_to_date_with_papers/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1ren2m5/d_how_do_yall_stay_up_to_date_with_papers/",
      "author": "/u/MARO2500",
      "created_utc": 1772047451,
      "selftext": "So, for the past year or so, I've been looking up papers, reading them, understanding them, and implementing them trying to reproduce the results. But one thing I found insane is I don't really have a way to stay up to date. I have to search through dozens of search results to find what I'm looking for, and also I miss tons of advancements until I stumble upon them one way or another So, my question is, how do you guys stay up to date and able to know every new paper? Thanks in advance :) &#3...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "\"Drive faster, Walt!\"",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1relmwv/drive_faster_walt/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1relmwv/drive_faster_walt/",
      "author": "/u/Ramenko1",
      "created_utc": 1772044479,
      "selftext": "&#32; submitted by &#32; /u/Ramenko1 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    }
  ]
}