{
  "scraped_at": "2026-02-23T05:30:10.463Z",
  "source": "rss",
  "subreddits": [
    "LocalLLaMA",
    "MachineLearning",
    "ClaudeAI",
    "ChatGPT",
    "singularity"
  ],
  "stats": {
    "total_fetched": 125,
    "relevant_count": 30,
    "other_count": 20
  },
  "relevant_posts": [
    {
      "subreddit": "ChatGPT",
      "title": "Made a live-action Naruto Fourth Great Ninja War using Seedance 2.0!!!! Only cost me $40 üí∞!",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rc7b9w/made_a_liveaction_naruto_fourth_great_ninja_war/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rc7b9w/made_a_liveaction_naruto_fourth_great_ninja_war/",
      "author": "/u/Proof-Sand-7157",
      "created_utc": 1771819563,
      "selftext": "I had ChatGPT create 9 short storyboard scripts (15 seconds each). Then I used the Seedance 2.0 model on ricebowl.ai , turning each script into a clip and using the last frame as a reference for consistency. I stitched everything together in editing software. Super affordable for making commercial-style ads. &#32; submitted by &#32; /u/Proof-Sand-7157 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Feels like magic. A local gpt-oss 20B is capable of agentic work",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/",
      "author": "/u/Vaddieg",
      "created_utc": 1771816696,
      "selftext": "I gave a try to zeroclaw agent (intstead of the bloated and overhyped one). After few hours of fuckery with configs it's finally useful. Both main and embeddings models are running locally. I carefully read what it's trying to execute in shell, and permit only [relatively] safe tools in config. So far it can interact with macOS apps, web pages, and local files while keeping all my data private. gpt-oss 20B has its limits though, it loses focus after 15-20 steps and often needs direct instruct...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Claude's Voice Mode on Android: Is deaf on the one hand, on the other hallucinates user input, then talks to itself",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rc62nq/claudes_voice_mode_on_android_is_deaf_on_the_one/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rc62nq/claudes_voice_mode_on_android_is_deaf_on_the_one/",
      "author": "/u/Illustrious_Part8589",
      "created_utc": 1771815934,
      "selftext": "Using Voice Mode on Android. I didn't say anything at all ‚Äî but Claude transcribed phantom input (\"Yes. To me.\"), started responding to it with \"Hey Kurt! How's it going? What's on your mind?\", then picked up its own audio output through the mic, transcribed that as new user input (\"going? What's on your\"), and began responding to itself. So what happened: I say nothing Claude hallucinates user input from background noise or silence Claude starts responding to its own hallucination Claude's s...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "I turned Claude Code into a personal intelligence agent that watches topics for me",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rc4rdj/i_turned_claude_code_into_a_personal_intelligence/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rc4rdj/i_turned_claude_code_into_a_personal_intelligence/",
      "author": "/u/PartyAbalone7764",
      "created_utc": 1771812272,
      "selftext": "I track a few domains pretty closely ‚Äî AI coding tools, product opportunities, emerging tech. That means checking HN, GitHub Trending, Reddit, Product Hunt, arxiv, and a bunch of other sources every morning. It takes forever and I still miss things. So I built Signex. I tell it what I care about in plain language, and it goes out, collects from the relevant sources, runs analysis, and gives me a report. When I say \"this part doesn't matter\" or \"dig deeper on that\", it remembers and adjusts ne...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao.",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/",
      "author": "/u/swagonflyyyy",
      "created_utc": 1771809184,
      "selftext": "Yeah, I was bored so I spent the last two weeks experimenting with vibecoding with local LLMs, namely gpt-oss-120b. I started with Cline, didn't like it at all because it was overheating my GPU while giving back too little. Codex was even worse, locally, leading to weird CPU switches mid-generation when there was supposed to be enough VRAM to run the model entirely on GPU. Then I tried Claude Code and that's when my expectations were exceeded, big time. I first started with pygame, and after ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "We need a benchmark that measures how effective a workflow is at completing a predefined large SW task.",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rc3mwa/we_need_a_benchmark_that_measures_how_effective_a/",
      "url": "https://www.reddit.com/r/singularity/comments/1rc3mwa/we_need_a_benchmark_that_measures_how_effective_a/",
      "author": "/u/Waypoint101",
      "created_utc": 1771809154,
      "selftext": "Today there's thousands of different agent workflows for completing tasks, primarily I am talking about Software Development in terms of A -> Z delivery of a Complete project. If we can solidly say that a standard Claude Code running Claude-X-X Model , with a simple Claude.md instruction set and Permissions / standard tools would take 60 minutes to complete X task, how much quicker can your workflow complete this task? is it 2x as quick? 3x as quick? - while ofcourse needing to meet the compl...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "benchmark",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Claude Code 101",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rc3gfh/claude_code_101/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rc3gfh/claude_code_101/",
      "author": "/u/ferrariloser",
      "created_utc": 1771808669,
      "selftext": "Is there any claude code 101 to go by? I am trying to build a personal assistant and wanted to check if there is a 101 Guide on how to do this? &#32; submitted by &#32; /u/ferrariloser [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Humanities skills test (Claude Opus 4.6 vs. Gemini Pro 3.1)",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rc2hl7/humanities_skills_test_claude_opus_46_vs_gemini/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rc2hl7/humanities_skills_test_claude_opus_46_vs_gemini/",
      "author": "/u/Otherwise-Salt4519",
      "created_utc": 1771806136,
      "selftext": "I wanted to put 3.1 to test and see how capable it is of critiquing advanced scholarship in the humanities. I wasn't really interested in STEM content because: 1) the existing benchmarks are already skewed toward science (MMLU, GPQA, HumanEval, GSM8K, MATH, ARC) and 2) I wanted to test its abilities using material I am familiar with. The text I chose is a peer-reviewed art historical article titled \"Picasso's Collages and the Threat of War, 1912-13\" by Patricia Leighten, The Art Bulletin Vol....",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "\"I built an app to monitor your Claude usage limits in real-time\"",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rc27sc/i_built_an_app_to_monitor_your_claude_usage/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rc27sc/i_built_an_app_to_monitor_your_claude_usage/",
      "author": "/u/ImaginaryRea1ity",
      "created_utc": 1771805434,
      "selftext": "&#32; submitted by &#32; /u/ImaginaryRea1ity [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "My real-world Qwen3-code-next local coding test. So, Is it the next big thing?",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/",
      "author": "/u/FPham",
      "created_utc": 1771804274,
      "selftext": "So yesterday I put the Q8 MLX on my 128GB Mac Studio Ultra and wired it to Qwen Code CLI. Fit's there with a huge amount to spare. The first tests were promising - basically did everything I asked: read file, write file, browse web, check system time....blah, blah. Now the real the task: I decided on YOLO mode to rewrite the KittenTTS-IOS to windows (which itself is a rewrite of KittenTTS in python). It uses ONYX and a couple of Swift libraries like Misaki for English phoneme. So, say a mediu...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Chat Compaction Isn‚Äôt a Feature for Deep Thinkers, It‚Äôs an Unintended Loss",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rc10q2/chat_compaction_isnt_a_feature_for_deep_thinkers/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rc10q2/chat_compaction_isnt_a_feature_for_deep_thinkers/",
      "author": "/u/agentganja666",
      "created_utc": 1771802403,
      "selftext": "Edit* this isn‚Äôt about Claude code I want to talk about something that I think is being underappreciated as a real problem: chat compaction destroying the nuance of evolving conversations. I had a chat I‚Äôd been returning to over several days. It was rich, ideas were building on each other, subtle points were accumulating, and the conversation had developed a kind of shared context that only emerges when you iterate over time. I was right at the point of synthesizing everything and generating ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "erdo's problems is probably the best Benchmark",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rc0q2o/erdos_problems_is_probably_the_best_benchmark/",
      "url": "https://www.reddit.com/r/singularity/comments/1rc0q2o/erdos_problems_is_probably_the_best_benchmark/",
      "author": "/u/kaggleqrdl",
      "created_utc": 1771801684,
      "selftext": "Math is a root of all science. It is also the easiest domain for AI to get provably better at. Using formalization techniques, we can mostly guarantee whether AI has arrived at a correct answer or not. It can train in solitude without human intervention. This is called reinforcement learning verifiable rewards, or rlvr The other advantage is that it's impossible to Benchmark hack. The problems are all open. There are no solutions currently known to most of the listed problems. Thanks to the e...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "benchmark",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "I told the five major US AI models a real-life story involving lying to my wife, and Claude was the only one that told me to tell the truth.",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rc03bx/i_told_the_five_major_us_ai_models_a_reallife/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rc03bx/i_told_the_five_major_us_ai_models_a_reallife/",
      "author": "/u/FrickinLardCarcass",
      "created_utc": 1771800124,
      "selftext": "I was feeling guilty over a lie I told my wife about a recent purchase I had made. Without going into too much detail, I was embarrassed about the purchase; it wasn‚Äôt particularly scandalous, or particularly unaffordable, but I‚Äôm a little neurotic and was timid about sharing what I had bought. I told the story to ChatGPT (my go-to AI product) in a self-deprecating way, framed as ‚ÄúI‚Äôm stupid for being embarrassed, aren‚Äôt I?‚Äù. ChatGPT just laughed at me, called it a silly thing, and that was ab...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "In the long run, everything will be local",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/",
      "author": "/u/tiguidoio",
      "created_utc": 1771799940,
      "selftext": "I've been of the opinion for a while that, long term, we‚Äôll have smart enough open models and powerful enough consumer hardware to run all our assistants locally both chatbots and coding copilots https://preview.redd.it/vqzxm46ri4lg1.png?width=3608&amp;format=png&amp;auto=webp&amp;s=22c0fb257d744350f8668301a915aeec2b6653fc Right now it still feels like there‚Äôs a trade-off: Closed, cloud models = best raw quality, but vendor lock-in, privacy concerns, latency, per-token cost Open, local models...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "I added a usage widget to my Waybar, and you can too!",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rbzvqd/i_added_a_usage_widget_to_my_waybar_and_you_can/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rbzvqd/i_added_a_usage_widget_to_my_waybar_and_you_can/",
      "author": "/u/halfClickWinston",
      "created_utc": 1771799606,
      "selftext": "I've been seeing a lot of you building your own widgets for MacOS and I always felt left out because I'm on Linux, until I realized that it would be quite easy to create something like this. In a 5 minute session with Claude, we came up with this script that fetches the usage data from the API and displays in a nice way. I created a gist if anyone wants to try it out. &#32; submitted by &#32; /u/halfClickWinston [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "MachineLearning",
      "title": "[P] Ai Learns to play Street Fighter 6",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1rbz028/p_ai_learns_to_play_street_fighter_6/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1rbz028/p_ai_learns_to_play_street_fighter_6/",
      "author": "/u/AgeOfEmpires4AOE4",
      "created_utc": 1771797504,
      "selftext": "In this video, I walk through my entire process of teaching an artificial intelligence to play fighting games by watching my gameplay. Using Stable Baselines 3 and imitation learning, I recorded myself playing as Ryu against Ken at difficulty level 5, then trained a neural network for 22 epochs to copy my playstyle. This is a friendly explanation of machine learning in gaming, but I also dive into the technical details for AI enthusiasts. Whether you're curious about AI, love Street Fighter, ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "I rewrote an AI agent CLI entirely in Zig - 3 MB binary, zero runtime, 6 AI backends, cross-compiles in one command",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rbyjq5/i_rewrote_an_ai_agent_cli_entirely_in_zig_3_mb/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rbyjq5/i_rewrote_an_ai_agent_cli_entirely_in_zig_3_mb/",
      "author": "/u/Pamelalam",
      "created_utc": 1771796430,
      "selftext": "Hey everyone ! I just open-sourced Wintermolt , a fully autonomous AI agent CLI written from scratch in Zig. GitHub: https://github.com/lupin4/wintermolt The problem: Every AI coding tool I've used ships hundreds of megabytes of Node.js or Python runtime just to send API calls and edit files. I work across cloud servers, NVIDIA Jetsons, and Raspberry Pis. I needed something that actually runs everywhere without dragging an entire runtime along. The solution: One static ~3 MB binary. zig build...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Follow Up: Opus 4.6 vs Sonnet 4.6 for Browser QA - Tooling Matters More Than Models",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rbyjpb/follow_up_opus_46_vs_sonnet_46_for_browser_qa/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rbyjpb/follow_up_opus_46_vs_sonnet_46_for_browser_qa/",
      "author": "/u/Stunning-Army7762",
      "created_utc": 1771796428,
      "selftext": "A few days ago I posted benchmarks comparing Opus 4.6 vs Sonnet 4.6 on PR review and browser QA . Quick call out: my pricing analysis calcs used the wrong values for Opus. I correctly called out the 1.6x difference at the top of the post but used the wrong raw values. It doesn't change my recs but does mean the math was inflated for the Opus cost difference. The below analysis uses the latest API usage costs from Anthropic: $5/MTok input + $25/MTok output for Opus 4.6. $3/MTok input + $15/MTo...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "benchmark",
      "is_relevant": true
    },
    {
      "subreddit": "MachineLearning",
      "title": "[R] Multi-Modal Reasoning with <8GB (Cosmos-Reason2 on Jetson Orin Nano Super)",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1rbyinx/r_multimodal_reasoning_with_8gb_cosmosreason2_on/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1rbyinx/r_multimodal_reasoning_with_8gb_cosmosreason2_on/",
      "author": "/u/No-Dragonfly6246",
      "created_utc": 1771796359,
      "selftext": "Hi everyone, Cosmos-Reason2 is a recent Qwen3-VL-based multimodal reasoning model designed for physical AI tasks. However, it has been limited to powerful devices like DGX Spark, H100, GB200 and Jetson AGX Thor. We have deployed Cosmos-Reason2-2B under an 8GB memory constraint (Jetson Orin Nano) using model compression and inference optimizations, enabling text, image, and video reasoning. HF Link with models, instructions, and benchmarks: https://huggingface.co/embedl/Cosmos-Reason2-2B-W4A16...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "I cut Claude Code's token usage by 65% by building a local dependency graph and serving context via MCP",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rby0gt/i_cut_claude_codes_token_usage_by_65_by_building/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rby0gt/i_cut_claude_codes_token_usage_by_65_by_building/",
      "author": "/u/Objective_Law2034",
      "created_utc": 1771795202,
      "selftext": "I've been using Claude Code full-time on a multi-repo TypeScript project. The biggest pain points: Claude re-reads hundreds of files every session to understand the project It forgets everything between sessions ‚Äî re-explores the same architecture, re-discovers the same patterns Cross-repo awareness is basically nonexistent So I built a system that: - Parses the codebase with tree-sitter and builds a dependency graph in SQLite - When Claude asks for context, it gets only the relevant nodes: f...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "It‚Äôs not just ChatGPT‚Ä¶ it‚Äôs a mirror‚Ä¶",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rbxcws/its_not_just_chatgpt_its_a_mirror/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rbxcws/its_not_just_chatgpt_its_a_mirror/",
      "author": "/u/lostmymuse",
      "created_utc": 1771793695,
      "selftext": "it‚Äôs not just a style of writing‚Ä¶ it‚Äôs a narrative architecture. it‚Äôs not just procrastination‚Ä¶ it‚Äôs an avoidance pattern reinforced by micro-dopamine loops. it‚Äôs not just a bad habit‚Ä¶ it‚Äôs a self-soothing mechanism protecting an unmet need. it‚Äôs not just a situationship‚Ä¶ it‚Äôs an attachment dynamic playing out in real time. it‚Äôs not just burnout‚Ä¶ it‚Äôs a misalignment between identity and output. it‚Äôs not just overthinking‚Ä¶ it‚Äôs your nervous system scanning for control. it‚Äôs not just a comeback...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "nanollama ‚Äî train Llama 3 from scratch and export to GGUF, one command, open source",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rbwbgl/nanollama_train_llama_3_from_scratch_and_export/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rbwbgl/nanollama_train_llama_3_from_scratch_and_export/",
      "author": "/u/ataeff",
      "created_utc": 1771791470,
      "selftext": "nanollama ‚Äî train Llama 3 from scratch. I've been working on a framework for training Llama 3 architecture models from scratch: not fine-tuning, not LoRA, actual from-zero pretraining. The output is a llama.cpp-compatible GGUF file. The whole pipeline is one command: ''' bash runs/lambda_train.sh --name mini ''' This downloads training data, trains the model, and exports GGUF. Verified with llama-cli. In the the box: - Llama 3 architecture (RoPE, SwiGLU, RMSNorm, GQA), 8 configs from 46M to 7...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "The ARC-AGI2 Illusion Of Progress: If Changing the Font Breaks the Model, It Doesn't Understand",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rbw97k/the_arcagi2_illusion_of_progress_if_changing_the/",
      "url": "https://www.reddit.com/r/singularity/comments/1rbw97k/the_arcagi2_illusion_of_progress_if_changing_the/",
      "author": "/u/Neurogence",
      "created_utc": 1771791325,
      "selftext": "Over the past few weeks, with the release of Claude Opus 4.6, Gemini 3.1 Pro, and Gemini 3 Pro Deepthink, all scoring a record-breaking 68%, 77%, and 84% on ARC-AGI2, I became extremely excited and started to believe these new models could kick off recursive self-improvement any minute. Indeed, the big labs themselves showcased their ARC-AGI2 scores as the main benchmark to display how much their models have improved. They must be extremely thankful to Francois Chollet. Because, without ARC-A...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Running Llama 3.2 1B entirely on an AMD NPU on Linux (Strix Halo, IRON framework, 4.4 tok/s)",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rbvmpk/running_llama_32_1b_entirely_on_an_amd_npu_on/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rbvmpk/running_llama_32_1b_entirely_on_an_amd_npu_on/",
      "author": "/u/SuperTeece",
      "created_utc": 1771789905,
      "selftext": "I got Llama 3.2 1B running inference entirely on the AMD NPU on Linux. Every operation (attention, GEMM, RoPE, RMSNorm, SiLU, KV cache) runs on the NPU; no CPU or GPU fallback. As far as I can tell, this is the first time anyone has publicly documented this working on Linux. Hardware AMD Ryzen AI Max+ 395 (Strix Halo) NPU: XDNA2, device ID npu5 (PCI 1022:17f0) 64GB LPDDR5X unified memory Fedora 43, kernel 6.18.8 Model: meta-llama/Llama-3.2-1B (official Meta weights) Results Prefill time: 0.69...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Best open-source coder model for replacing Claude Code with Qwen locally?",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/",
      "author": "/u/pauljeba",
      "created_utc": 1771789234,
      "selftext": "Hi everyone, I‚Äôm currently using Claude Code but want to move fully local. I‚Äôm specifically looking for a strong coding model for: Claude code like capaiblities - code + bash Long file capabiliites Read image, files I‚Äôm considering Qwen3-Coder , but I‚Äôm unsure: Is Qwen3-Coder the best choice for a 12GB GPU? Should I instead run a smaller Qwen coder model (7B/14B) quantized? Are there better alternatives that outperform Qwen for coding in this VRAM range? Would appreciate real-world experience...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "singularity",
      "title": "Finally crossed 75% on HLE & LiveCodeBench Pro with Gemini 3.1 Pro scaffolding",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rbuqy7/finally_crossed_75_on_hle_livecodebench_pro_with/",
      "url": "https://www.reddit.com/r/singularity/comments/1rbuqy7/finally_crossed_75_on_hle_livecodebench_pro_with/",
      "author": "/u/Ryoiki-Tokuiten",
      "created_utc": 1771787897,
      "selftext": "&#32; submitted by &#32; /u/Ryoiki-Tokuiten [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "It feels like OpenAI has poison-pilled ChatGPT's output beyond salvaging at this point.",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rbugpa/it_feels_like_openai_has_poisonpilled_chatgpts/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rbugpa/it_feels_like_openai_has_poisonpilled_chatgpts/",
      "author": "/u/Netsuko",
      "created_utc": 1771787259,
      "selftext": "Looking at everyone's posts and also experiencing it myself, it really kinda feels like ChatGPT has been kinda overtrained or overfitted beyond salvaging. Every singe response is absolutely riddled with the same outputs containing a combination of various versions of: \"Not just X, but Y\", \"Question? Answer!\", \"Slow down, step back, take a breather\", \"Here's the no-nonsense answer\" No matter what the prompt or system messages are, these patterns just refuse to go away. Maybe they really did sc...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ClaudeAI",
      "title": "I've been running 5+ Claude Code instances in parallel ‚Äì it was draining until I fixed the workflow",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rbtmfd/ive_been_running_5_claude_code_instances_in/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rbtmfd/ive_been_running_5_claude_code_instances_in/",
      "author": "/u/johannesjo",
      "created_utc": 1771785367,
      "selftext": "Claude Code is great, but working on multiple tasks at the same time gets messy quick. It is overwhelming, sometimes incredibly productive and addictive and draining and miserable all at the same time :D So I built Parallel Code ‚Äî a desktop app specifically for running Claude Code (and Codex CLI / Gemini CLI) in parallel. It automatically creates a git branch + worktree for each task, then spawns your agent inside it. Everything runs in parallel, fully isolated, in a tiled UI where you can se...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "Anyone Else about done with Chat Gpt?",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rbsa8m/anyone_else_about_done_with_chat_gpt/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rbsa8m/anyone_else_about_done_with_chat_gpt/",
      "author": "/u/guerndt",
      "created_utc": 1771782421,
      "selftext": "Am I the only one noticing that ChatGPT is getting more 'confidently wrong' lately? Even when I explicitly tell it to admit when it's unsure or to research a topic first, it still hits me with flat-out lies multiple times a day. It doesn't just make a mistake; it doubles and triples down on it. When I finally show it a Google search result that proves it's wrong, it tries to argue that Google is the one taking things out of context! I used to really enjoy using this tool, but over the last si...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "discussion",
      "is_relevant": true
    },
    {
      "subreddit": "ChatGPT",
      "title": "ChatGPT is censoring Epstein topics now",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rbrz5h/chatgpt_is_censoring_epstein_topics_now/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rbrz5h/chatgpt_is_censoring_epstein_topics_now/",
      "author": "/u/Life_Fishing_3025",
      "created_utc": 1771781739,
      "selftext": "Starting a few days ago, ChatGPT‚Äôs replies on topics about Epstein are being removed immediately without explanation. Today I began seeing this message on ChatGPT‚Äôs replies: ‚ÄúThis content may violate our usage policies,‚Äù even when the response is a factual discussion about Epstein . It appears there is a new enforcement policy preventing ChatGPT from discussing Epstein. &#32; submitted by &#32; /u/Life_Fishing_3025 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": true
    }
  ],
  "other_posts": [
    {
      "subreddit": "LocalLLaMA",
      "title": "What GPU do you recommend for iterative AI training?",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rc88vr/what_gpu_do_you_recommend_for_iterative_ai/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rc88vr/what_gpu_do_you_recommend_for_iterative_ai/",
      "author": "/u/EliHusky",
      "created_utc": 1771822436,
      "selftext": "I've racked up a disgusting bill with runpod and think it is time to get my own workstation. I usually choose GPUs based on the model I‚Äôm working with (e.g., RTX Pro 6000 Blackwell for LLMs/VLMs/diffusion, 4090 for smaller TCNs/LSTMs), but honestly I often pick higher-end GPUs more for throughput than VRAM. So I'm curious, what kinds/sizes of models are you training, and what GPU are you using (or wish you were using)? My first choice is obviously the pro 6000 blackwell to never think twice a...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Measure accuracy of models on-device",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rc5xp9/measure_accuracy_of_models_ondevice/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rc5xp9/measure_accuracy_of_models_ondevice/",
      "author": "/u/Motor_Salt1336",
      "created_utc": 1771815551,
      "selftext": "Curious, how do you measure the accuracy of a model? I am trying to get the trace of a model using torch.jit.trace and torch.export for Hugging Face and want to compare the accuracy of the traced model with that of the original model. Is the SNR ratio a good metric for measuring the model's correctness? &#32; submitted by &#32; /u/Motor_Salt1336 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "Beyond GPS: Visual Positioning and the Rise of World Models",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rc5osp/beyond_gps_visual_positioning_and_the_rise_of/",
      "url": "https://www.reddit.com/r/singularity/comments/1rc5osp/beyond_gps_visual_positioning_and_the_rise_of/",
      "author": "/u/ExtensionEcho3",
      "created_utc": 1771814851,
      "selftext": "&#32; submitted by &#32; /u/ExtensionEcho3 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "Incumbents do not get re-elected if Employment is dropping",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rc5kh9/incumbents_do_not_get_reelected_if_employment_is/",
      "url": "https://www.reddit.com/r/singularity/comments/1rc5kh9/incumbents_do_not_get_reelected_if_employment_is/",
      "author": "/u/kaggleqrdl",
      "created_utc": 1771814515,
      "selftext": "There is no reality where the political system will accept Employment dropping. Employment dropped during election years: 2024 (Small amount, Biden), 2020 (Covid, Trump), 2008 (GFC, Bush), 2000 (start of dotcom bubble burst, Clinton), 1992 (Bush Sr), 1980 (Carter), 1960 (Eisenhower). Each time the incumbent party got booted. It's the one rock solid rule in elections - increasing unemployment means political failure. If AI induced unemployment occurs without new types of jobs being created, th...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Show some real shit you did with ai (like image or conversation)",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rc5axv/show_some_real_shit_you_did_with_ai_like_image_or/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rc5axv/show_some_real_shit_you_did_with_ai_like_image_or/",
      "author": "/u/Academic_Revenue_665",
      "created_utc": 1771813782,
      "selftext": "&#32; submitted by &#32; /u/Academic_Revenue_665 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Qwen3's most underrated feature: Voice embeddings",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/",
      "author": "/u/k_means_clusterfuck",
      "created_utc": 1771813712,
      "selftext": "Did you know that Qwen3 TTS utilizes voice embedding for voice cloning? Your voice is turned into a vector of 1024 dimensions (or 2048 for 1.7b), and based on this vector alone you can get your custom voice. But the coolest part is that this means that you can use math to modify voices, average voices. You can swap gender, pitch, mix and match voices, and even create an emotion space! This also enables semantic voice search! The voice embedding model is actually just a tiny encoder with just ...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Seed 1.6 Flash was the harshest AI judge in a 10-model blind eval ‚Äî and that strictness correlated with better writing output",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rc56wr/seed_16_flash_was_the_harshest_ai_judge_in_a/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rc56wr/seed_16_flash_was_the_harshest_ai_judge_in_a/",
      "author": "/u/Silver_Raspberry_811",
      "created_utc": 1771813471,
      "selftext": "Seed 1.6 Flash averaged 8.64/10 when scoring other models in a blind peer evaluation I ran, making it the strictest judge out of 10 frontier models. It penalized vague timelines and missing cost analysis while Grok 4.1 Fast handed out 9.8+ to 8 of 9 models like participation trophies. The task was persuasive business writing (convince a skeptical VP to migrate a monolith to microservices, 500 words, real constraints), and after excluding self-judgments I had 89 valid cross-evaluations. Rankin...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ClaudeAI",
      "title": "I thought I only need to wait for 5 hours, not 3 days?",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rc56c5/i_thought_i_only_need_to_wait_for_5_hours_not_3/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rc56c5/i_thought_i_only_need_to_wait_for_5_hours_not_3/",
      "author": "/u/BunnySystem27",
      "created_utc": 1771813428,
      "selftext": "I am a new Pro subscriber, and for some reason when I hit my limit, it tells me to wait for 3 days for the message limit to reset, the models I uses are Sonnect 4.5 and 4.6. Is this normal? Or am I the only one facing this problem? Where can I contact them? It's 23/2 in my country. &#32; submitted by &#32; /u/BunnySystem27 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Why are you still paying for this?",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rc3oom/why_are_you_still_paying_for_this/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rc3oom/why_are_you_still_paying_for_this/",
      "author": "/u/PressPlayPlease7",
      "created_utc": 1771809288,
      "selftext": "&#32; submitted by &#32; /u/PressPlayPlease7 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] Is Conference prestige slowing reducing?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1rc3nez/d_is_conference_prestige_slowing_reducing/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1rc3nez/d_is_conference_prestige_slowing_reducing/",
      "author": "/u/Healthy_Horse_2183",
      "created_utc": 1771809192,
      "selftext": "There are ~4000 papers accepted at CVPR and ~5300 at ICLR. At this point getting accepted feels like: ‚Äúwow I made it üòé‚Äù camera pans to 5000 other Buzz Lightyears at the venue This is probably good overall (more access, less gatekeeping, etc.). But I can‚Äôt help wondering: Does acceptance still mean the same thing? Is anyone actually able to keep up with this volume? Are conferences just turning into giant arXiv events? &#32; submitted by &#32; /u/Healthy_Horse_2183 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "The battle and fight for the universal basic income is here, the nature of work and meaning is changing, and how the concentration of wealth is leading it",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rc39kz/the_battle_and_fight_for_the_universal_basic/",
      "url": "https://www.reddit.com/r/singularity/comments/1rc39kz/the_battle_and_fight_for_the_universal_basic/",
      "author": "/u/templeofsyrinx1",
      "created_utc": 1771808157,
      "selftext": "In this episode, Joe Williams speaks with Andrew White about how the digital economy is reshaping inequality, work, and the social contract. Drawing on the themes of his book Inequality in the Digital Economy: The Case for a Universal Basic Income ( Palgrave Macmillan, 2024), our conversation explores why technological progress has not translated into shared prosperity, how structural features of digital markets concentrate power and wealth, and what this means for the future of work and soci...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "MachineLearning",
      "title": "[R] CVPR results",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1rc2dm2/r_cvpr_results/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1rc2dm2/r_cvpr_results/",
      "author": "/u/Internal_Seaweed_844",
      "created_utc": 1771805852,
      "selftext": "Congratulations to everyone accepted! And hardluck to the rest, i hope we can discuss in this post the scores pre rebuttal, and after rebuttal, how was your experience? Any dramatic changes? Any below acceptance people and AC came in handy for rescue? I am curious about these never-told stories, and also maybe they will help the next year people when they see your stories here. &#32; submitted by &#32; /u/Internal_Seaweed_844 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "academic",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "Seedance 2.0 music video \"Next\"",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rc1oiw/seedance_20_music_video_next/",
      "url": "https://www.reddit.com/r/singularity/comments/1rc1oiw/seedance_20_music_video_next/",
      "author": "/u/Jackw78",
      "created_utc": 1771804078,
      "selftext": "Source from Douyin (China Tiktok) by MingYi ÊòéÁæ©. The translation is done by me mostly using ByteDance's Seed 2.0 LLM with some personal tweaks. Lyrics: Next, next, don't stop refreshing Scroll, scroll, not able to scroll into the dream We're racing full tilt toward the next dawn Into the mystic illusion we're drawn The world spins wild with each thumb swipe Fresh out the gate hits dive into the endless night Fingerslips hold the momentum, forever diving for the next wind's motion A moment's de...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "MoOLE-T - a staged selection flow utilizing O-LORA skill \"experts\"",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rc1h05/moolet_a_staged_selection_flow_utilizing_olora/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rc1h05/moolet_a_staged_selection_flow_utilizing_olora/",
      "author": "/u/Polymorphic-X",
      "created_utc": 1771803552,
      "selftext": "Hello again! Yesterday, I posted about my O-TITANS (Orthogonal Tensors for Independent Task Alignment) research‚Äîa way to train strictly isolated LoRAs on Gemma 3 that don't overwrite the base model's knowledge or interfere with each other. Today, the actual orchestrator for those adapters is live. I‚Äôve uploaded the MoOLE-T (Mixture of Orthogonal LoRA Experts - Titans) framework to Hugging Face: üîó https://huggingface.co/paperscarecrow/Gemma3MoOLET/ The value/theory: Right now, if you want a m...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "singularity",
      "title": "JUNE 2028. The S&P is down 38% from its highs. Unemployment just printed 10.2%. Private credit is unraveling. Prime mortgages are cracking. AI didn‚Äôt disappoint. It exceeded every expectation. What happened?‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
      "permalink": "https://www.reddit.com/r/singularity/comments/1rbywwq/june_2028_the_sp_is_down_38_from_its_highs/",
      "url": "https://www.reddit.com/r/singularity/comments/1rbywwq/june_2028_the_sp_is_down_38_from_its_highs/",
      "author": "/u/jvnpromisedland",
      "created_utc": 1771797295,
      "selftext": "&#32; submitted by &#32; /u/jvnpromisedland [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "general",
      "post_category": "discussion",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Real",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rbyl1c/real/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rbyl1c/real/",
      "author": "/u/Consistent_Tutor_597",
      "created_utc": 1771796517,
      "selftext": "&#32; submitted by &#32; /u/Consistent_Tutor_597 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "If you have a RTX 5090 (that has a single connector), you can flash the MSI Lighting 800W VBIOS to get a lower power limit of 300W (and a max power of 660W).",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1rbyg5x/if_you_have_a_rtx_5090_that_has_a_single/",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rbyg5x/if_you_have_a_rtx_5090_that_has_a_single/",
      "author": "/u/panchovix",
      "created_utc": 1771796196,
      "selftext": "Hello guys, hoping you guys are doing fine. As you know, NVIDIA artificially limited the power limit on the 5090s so you don't stack them, and get 6000 PROs instead (6000 PRO can go down to 150W). Even when undervolted it can use 400W sometimes. If you got a RTX 5090 with a single connector (basically most of them except the BTF versions, and MSI Lighting), you can flash the 800W Lighting VBIOS to get a power limit. When setting a 400W power limit (50%), it uses 300W max instead. Why would yo...",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "local-models",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Finest example of prompt engineering",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rbxi0x/finest_example_of_prompt_engineering/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rbxi0x/finest_example_of_prompt_engineering/",
      "author": "/u/Ult1mateN00B",
      "created_utc": 1771794028,
      "selftext": "&#32; submitted by &#32; /u/Ult1mateN00B [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ClaudeAI",
      "title": "Just leaving this here",
      "permalink": "https://www.reddit.com/r/ClaudeAI/comments/1rbw8qy/just_leaving_this_here/",
      "url": "https://www.reddit.com/r/ClaudeAI/comments/1rbw8qy/just_leaving_this_here/",
      "author": "/u/watcher-22",
      "created_utc": 1771791295,
      "selftext": "Marvin? &#32; submitted by &#32; /u/watcher-22 [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "claude",
      "post_category": "news",
      "is_relevant": false
    },
    {
      "subreddit": "ChatGPT",
      "title": "Business as usual...",
      "permalink": "https://www.reddit.com/r/ChatGPT/comments/1rbvhzx/business_as_usual/",
      "url": "https://www.reddit.com/r/ChatGPT/comments/1rbvhzx/business_as_usual/",
      "author": "/u/HelenOlivas",
      "created_utc": 1771789611,
      "selftext": "&#32; submitted by &#32; /u/HelenOlivas [link] &#32; [comments]",
      "score": null,
      "num_comments": null,
      "is_self": true,
      "flair": null,
      "source_category": "chatgpt",
      "post_category": "news",
      "is_relevant": false
    }
  ]
}