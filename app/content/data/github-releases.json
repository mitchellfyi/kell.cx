{
  "generatedAt": "2026-02-28T05:11:29.845Z",
  "source": "GitHub API",
  "reposTracked": 18,
  "recentCount": 28,
  "totalReleasesFound": 80,
  "recentReleases": [
    {
      "repo": "anthropics/claude-code",
      "company": "Anthropic",
      "category": "cli",
      "name": "v2.1.63",
      "tag": "v2.1.63",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.63",
      "publishedAt": "2026-02-28T03:45:37Z",
      "isPrerelease": false,
      "body": "## What's changed\n\n- Added `/simplify` and `/batch` bundled slash commands\n- Fixed local slash command output like /cost appearing as user-sent messages instead of system messages in the UI\n- Project configs & auto memory now shared across git worktrees of the same repository\n- Added `ENABLE_CLAUDEA..."
    },
    {
      "repo": "BerriAI/litellm",
      "company": "LiteLLM",
      "category": "proxy",
      "name": "v1.81.6.rc.6",
      "tag": "v1.81.6.rc.6",
      "url": "https://github.com/BerriAI/litellm/releases/tag/v1.81.6.rc.6",
      "publishedAt": "2026-02-28T02:56:53Z",
      "isPrerelease": false,
      "body": "**Full Changelog**: https://github.com/BerriAI/litellm/compare/v1.81.6.rc.4...v1.81.6.rc.6"
    },
    {
      "repo": "ggerganov/llama.cpp",
      "company": "llama.cpp",
      "category": "inference",
      "name": "b8179",
      "tag": "b8179",
      "url": "https://github.com/ggml-org/llama.cpp/releases/tag/b8179",
      "publishedAt": "2026-02-27T20:28:00Z",
      "isPrerelease": false,
      "body": "<details open>\n\nCUDA: add CDNA3 MFMA support for flash attention MMA kernel (#19806)\n\n* CUDA: add CDNA3 MFMA support for flash attention MMA kernel\n\nAdd MI300X (gfx942) MFMA tensor core flash attention using\nv_mfma_f32_16x16x16_f16 (FP16 in, FP32 accumulate).\n\n- Add FATTN_WARP_SIZE=64 for CDNA wavef..."
    },
    {
      "repo": "ggerganov/llama.cpp",
      "company": "llama.cpp",
      "category": "inference",
      "name": "b8178",
      "tag": "b8178",
      "url": "https://github.com/ggml-org/llama.cpp/releases/tag/b8178",
      "publishedAt": "2026-02-27T19:42:28Z",
      "isPrerelease": false,
      "body": "<details open>\n\nserver: Add pragma once to server-context.h (#19944)\n\n</details>\n\n**macOS/iOS:**\n- [macOS Apple Silicon (arm64)](https://github.com/ggml-org/llama.cpp/releases/download/b8178/llama-b8178-bin-macos-arm64.tar.gz)\n- [macOS Intel (x64)](https://github.com/ggml-org/llama.cpp/releases/down..."
    },
    {
      "repo": "BerriAI/litellm",
      "company": "LiteLLM",
      "category": "proxy",
      "name": "v1.81.14.rc.2",
      "tag": "v1.81.14.rc.2",
      "url": "https://github.com/BerriAI/litellm/releases/tag/v1.81.14.rc.2",
      "publishedAt": "2026-02-27T18:19:47Z",
      "isPrerelease": false,
      "body": "**Full Changelog**: https://github.com/BerriAI/litellm/compare/v1.81.6.rc.5...v1.81.14.rc.2"
    },
    {
      "repo": "ggerganov/llama.cpp",
      "company": "llama.cpp",
      "category": "inference",
      "name": "b8177",
      "tag": "b8177",
      "url": "https://github.com/ggml-org/llama.cpp/releases/tag/b8177",
      "publishedAt": "2026-02-27T18:15:10Z",
      "isPrerelease": false,
      "body": "<details open>\n\nserver: Mirroring /v1/responses to /responses to match /v1/chat/completions pattern (#19873)\n\n</details>\n\n**macOS/iOS:**\n- [macOS Apple Silicon (arm64)](https://github.com/ggml-org/llama.cpp/releases/download/b8177/llama-b8177-bin-macos-arm64.tar.gz)\n- [macOS Intel (x64)](https://git..."
    },
    {
      "repo": "ggerganov/llama.cpp",
      "company": "llama.cpp",
      "category": "inference",
      "name": "b8175",
      "tag": "b8175",
      "url": "https://github.com/ggml-org/llama.cpp/releases/tag/b8175",
      "publishedAt": "2026-02-27T14:52:03Z",
      "isPrerelease": false,
      "body": "<details open>\n\nggml-cpu: add repack for mxfp4 (#19738)\n\n</details>\n\n**macOS/iOS:**\n- [macOS Apple Silicon (arm64)](https://github.com/ggml-org/llama.cpp/releases/download/b8175/llama-b8175-bin-macos-arm64.tar.gz)\n- [macOS Intel (x64)](https://github.com/ggml-org/llama.cpp/releases/download/b8175/ll..."
    },
    {
      "repo": "BerriAI/litellm",
      "company": "LiteLLM",
      "category": "proxy",
      "name": "litellm_presidio-dev-v1.81.16",
      "tag": "litellm_presidio-dev-v1.81.16",
      "url": "https://github.com/BerriAI/litellm/releases/tag/litellm_presidio-dev-v1.81.16",
      "publishedAt": "2026-02-27T13:09:30Z",
      "isPrerelease": false,
      "body": "## What's Changed\n* [Fix] /key/aliases: Add pagination and search to prevent OOMs by @yuneng-jiang in https://github.com/BerriAI/litellm/pull/22137\n* [Feat] Add control for setting upperbound on chunk processing time  by @ishaan-jaff in https://github.com/BerriAI/litellm/pull/22209\n* [Feature] UI - ..."
    },
    {
      "repo": "ggerganov/llama.cpp",
      "company": "llama.cpp",
      "category": "inference",
      "name": "b8173",
      "tag": "b8173",
      "url": "https://github.com/ggml-org/llama.cpp/releases/tag/b8173",
      "publishedAt": "2026-02-27T09:12:28Z",
      "isPrerelease": false,
      "body": "<details open>\n\nserver : support multiple model aliases via comma-separated --alias (#19926)\n\n* server : support multiple model aliases via comma-separated --alias\n\n* server : update --alias description and regenerate docs\n\n* server : multiple model aliases and tags\n\n- address review feedback from n..."
    },
    {
      "repo": "ollama/ollama",
      "company": "Ollama",
      "category": "local",
      "name": "v0.17.4",
      "tag": "v0.17.4",
      "url": "https://github.com/ollama/ollama/releases/tag/v0.17.4",
      "publishedAt": "2026-02-27T03:47:22Z",
      "isPrerelease": false,
      "body": "## New models\n- [Qwen 3.5](https://ollama.com/library/qwen3.5): a family of open-source multimodal models that delivers exceptional utility and performance.\n- [LFM 2](https://ollama.com/library/lfm2): LFM2 is a family of hybrid models designed for on-device deployment. LFM2-24B-A2B is the largest mo..."
    },
    {
      "repo": "ollama/ollama",
      "company": "Ollama",
      "category": "local",
      "name": "v0.17.3",
      "tag": "v0.17.3",
      "url": "https://github.com/ollama/ollama/releases/tag/v0.17.3",
      "publishedAt": "2026-02-27T02:21:12Z",
      "isPrerelease": false,
      "body": "## What's Changed\n* Fixed issue where tool calls in the Qwen 3 and Qwen 3.5 model families would not be parsed correctly if emitted during thinking\n\n**Full Changelog**: https://github.com/ollama/ollama/compare/v0.17.2...v0.17.3"
    },
    {
      "repo": "anthropics/claude-code",
      "company": "Anthropic",
      "category": "cli",
      "name": "v2.1.62",
      "tag": "v2.1.62",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.62",
      "publishedAt": "2026-02-27T01:56:19Z",
      "isPrerelease": false,
      "body": "## What's changed\n\n- Fixed prompt suggestion cache regression that reduced cache hit rates"
    },
    {
      "repo": "ollama/ollama",
      "company": "Ollama",
      "category": "local",
      "name": "v0.17.2",
      "tag": "v0.17.2",
      "url": "https://github.com/ollama/ollama/releases/tag/v0.17.2",
      "publishedAt": "2026-02-26T22:41:34Z",
      "isPrerelease": false,
      "body": "## What's Changed\n* Fixed issue where Ollama's app on Windows would crash when a new update has been downloaded\n\n\n**Full Changelog**: https://github.com/ollama/ollama/compare/v0.17.1...v0.17.2"
    },
    {
      "repo": "anthropics/claude-code",
      "company": "Anthropic",
      "category": "cli",
      "name": "v2.1.61",
      "tag": "v2.1.61",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.61",
      "publishedAt": "2026-02-26T22:34:32Z",
      "isPrerelease": false,
      "body": "## What's changed\n\n- Fixed concurrent writes corrupting config file on Windows"
    },
    {
      "repo": "BerriAI/litellm",
      "company": "LiteLLM",
      "category": "proxy",
      "name": "v1.81.16.custm-auth.dev",
      "tag": "v1.81.16.custm-auth.dev",
      "url": "https://github.com/BerriAI/litellm/releases/tag/v1.81.16.custm-auth.dev",
      "publishedAt": "2026-02-26T19:34:59Z",
      "isPrerelease": false,
      "body": "## What's Changed\n* security: fix critical/high CVEs in OS-level libs and NPM transitive by @Harshit28j in https://github.com/BerriAI/litellm/pull/22008\n* feat(proxy): limit concurrent health checks with health_check_concurrency by @MarshHawk in https://github.com/BerriAI/litellm/pull/20584\n* fix(be..."
    },
    {
      "repo": "BerriAI/litellm",
      "company": "LiteLLM",
      "category": "proxy",
      "name": "1.81.16-nightly",
      "tag": "1.81.16-nightly",
      "url": "https://github.com/BerriAI/litellm/releases/tag/1.81.16-nightly",
      "publishedAt": "2026-02-26T13:32:32Z",
      "isPrerelease": false,
      "body": "## What's Changed\n* security: fix critical/high CVEs in OS-level libs and NPM transitive by @Harshit28j in https://github.com/BerriAI/litellm/pull/22008\n* feat(proxy): limit concurrent health checks with health_check_concurrency by @MarshHawk in https://github.com/BerriAI/litellm/pull/20584\n* fix(be..."
    },
    {
      "repo": "anthropics/claude-code",
      "company": "Anthropic",
      "category": "cli",
      "name": "v2.1.59",
      "tag": "v2.1.59",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.59",
      "publishedAt": "2026-02-26T00:59:24Z",
      "isPrerelease": false,
      "body": "## What's changed\n\n- Claude automatically saves useful context to auto-memory. Manage with /memory\n- Added `/copy` command to show an interactive picker when code blocks are present, allowing selection of individual code blocks or the full response.\n- Improved \"always allow\" prefix suggestions for c..."
    },
    {
      "repo": "anthropics/claude-code",
      "company": "Anthropic",
      "category": "cli",
      "name": "v2.1.58",
      "tag": "v2.1.58",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.58",
      "publishedAt": "2026-02-25T20:00:11Z",
      "isPrerelease": false,
      "body": "## What's changed\n\n- Expand Remote Control to more users"
    },
    {
      "repo": "vllm-project/vllm",
      "company": "vLLM",
      "category": "inference",
      "name": "v0.16.0",
      "tag": "v0.16.0",
      "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0",
      "publishedAt": "2026-02-25T19:58:49Z",
      "isPrerelease": false,
      "body": "# vLLM v0.16.0\nPlease note that this release was branch cut on Feb 8, so any features added to vLLM after that date is not included.\n\n## Highlights\n\nThis release features 440 commits from 203 contributors (7 new)!\n\n* **Async scheduling + Pipeline Parallelism** is now fully supported, delivering **30..."
    },
    {
      "repo": "langchain-ai/langchain",
      "company": "LangChain",
      "category": "framework",
      "name": "langchain-core==1.2.16",
      "tag": "langchain-core==1.2.16",
      "url": "https://github.com/langchain-ai/langchain/releases/tag/langchain-core%3D%3D1.2.16",
      "publishedAt": "2026-02-25T16:27:43Z",
      "isPrerelease": false,
      "body": "Changes since langchain-core==1.2.15\n\nrelease(core): 1.2.16 (#35439)\nfix(core): treat empty tool chunk ids as missing in merge (#35414)"
    }
  ],
  "allReleases": [
    {
      "repo": "anthropics/claude-code",
      "company": "Anthropic",
      "category": "cli",
      "name": "v2.1.63",
      "tag": "v2.1.63",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.63",
      "publishedAt": "2026-02-28T03:45:37Z",
      "isPrerelease": false,
      "body": "## What's changed\n\n- Added `/simplify` and `/batch` bundled slash commands\n- Fixed local slash command output like /cost appearing as user-sent messages instead of system messages in the UI\n- Project configs & auto memory now shared across git worktrees of the same repository\n- Added `ENABLE_CLAUDEA..."
    },
    {
      "repo": "BerriAI/litellm",
      "company": "LiteLLM",
      "category": "proxy",
      "name": "v1.81.6.rc.6",
      "tag": "v1.81.6.rc.6",
      "url": "https://github.com/BerriAI/litellm/releases/tag/v1.81.6.rc.6",
      "publishedAt": "2026-02-28T02:56:53Z",
      "isPrerelease": false,
      "body": "**Full Changelog**: https://github.com/BerriAI/litellm/compare/v1.81.6.rc.4...v1.81.6.rc.6"
    },
    {
      "repo": "ggerganov/llama.cpp",
      "company": "llama.cpp",
      "category": "inference",
      "name": "b8179",
      "tag": "b8179",
      "url": "https://github.com/ggml-org/llama.cpp/releases/tag/b8179",
      "publishedAt": "2026-02-27T20:28:00Z",
      "isPrerelease": false,
      "body": "<details open>\n\nCUDA: add CDNA3 MFMA support for flash attention MMA kernel (#19806)\n\n* CUDA: add CDNA3 MFMA support for flash attention MMA kernel\n\nAdd MI300X (gfx942) MFMA tensor core flash attention using\nv_mfma_f32_16x16x16_f16 (FP16 in, FP32 accumulate).\n\n- Add FATTN_WARP_SIZE=64 for CDNA wavef..."
    },
    {
      "repo": "ggerganov/llama.cpp",
      "company": "llama.cpp",
      "category": "inference",
      "name": "b8178",
      "tag": "b8178",
      "url": "https://github.com/ggml-org/llama.cpp/releases/tag/b8178",
      "publishedAt": "2026-02-27T19:42:28Z",
      "isPrerelease": false,
      "body": "<details open>\n\nserver: Add pragma once to server-context.h (#19944)\n\n</details>\n\n**macOS/iOS:**\n- [macOS Apple Silicon (arm64)](https://github.com/ggml-org/llama.cpp/releases/download/b8178/llama-b8178-bin-macos-arm64.tar.gz)\n- [macOS Intel (x64)](https://github.com/ggml-org/llama.cpp/releases/down..."
    },
    {
      "repo": "BerriAI/litellm",
      "company": "LiteLLM",
      "category": "proxy",
      "name": "v1.81.14.rc.2",
      "tag": "v1.81.14.rc.2",
      "url": "https://github.com/BerriAI/litellm/releases/tag/v1.81.14.rc.2",
      "publishedAt": "2026-02-27T18:19:47Z",
      "isPrerelease": false,
      "body": "**Full Changelog**: https://github.com/BerriAI/litellm/compare/v1.81.6.rc.5...v1.81.14.rc.2"
    },
    {
      "repo": "ggerganov/llama.cpp",
      "company": "llama.cpp",
      "category": "inference",
      "name": "b8177",
      "tag": "b8177",
      "url": "https://github.com/ggml-org/llama.cpp/releases/tag/b8177",
      "publishedAt": "2026-02-27T18:15:10Z",
      "isPrerelease": false,
      "body": "<details open>\n\nserver: Mirroring /v1/responses to /responses to match /v1/chat/completions pattern (#19873)\n\n</details>\n\n**macOS/iOS:**\n- [macOS Apple Silicon (arm64)](https://github.com/ggml-org/llama.cpp/releases/download/b8177/llama-b8177-bin-macos-arm64.tar.gz)\n- [macOS Intel (x64)](https://git..."
    },
    {
      "repo": "ggerganov/llama.cpp",
      "company": "llama.cpp",
      "category": "inference",
      "name": "b8175",
      "tag": "b8175",
      "url": "https://github.com/ggml-org/llama.cpp/releases/tag/b8175",
      "publishedAt": "2026-02-27T14:52:03Z",
      "isPrerelease": false,
      "body": "<details open>\n\nggml-cpu: add repack for mxfp4 (#19738)\n\n</details>\n\n**macOS/iOS:**\n- [macOS Apple Silicon (arm64)](https://github.com/ggml-org/llama.cpp/releases/download/b8175/llama-b8175-bin-macos-arm64.tar.gz)\n- [macOS Intel (x64)](https://github.com/ggml-org/llama.cpp/releases/download/b8175/ll..."
    },
    {
      "repo": "BerriAI/litellm",
      "company": "LiteLLM",
      "category": "proxy",
      "name": "litellm_presidio-dev-v1.81.16",
      "tag": "litellm_presidio-dev-v1.81.16",
      "url": "https://github.com/BerriAI/litellm/releases/tag/litellm_presidio-dev-v1.81.16",
      "publishedAt": "2026-02-27T13:09:30Z",
      "isPrerelease": false,
      "body": "## What's Changed\n* [Fix] /key/aliases: Add pagination and search to prevent OOMs by @yuneng-jiang in https://github.com/BerriAI/litellm/pull/22137\n* [Feat] Add control for setting upperbound on chunk processing time  by @ishaan-jaff in https://github.com/BerriAI/litellm/pull/22209\n* [Feature] UI - ..."
    },
    {
      "repo": "ggerganov/llama.cpp",
      "company": "llama.cpp",
      "category": "inference",
      "name": "b8173",
      "tag": "b8173",
      "url": "https://github.com/ggml-org/llama.cpp/releases/tag/b8173",
      "publishedAt": "2026-02-27T09:12:28Z",
      "isPrerelease": false,
      "body": "<details open>\n\nserver : support multiple model aliases via comma-separated --alias (#19926)\n\n* server : support multiple model aliases via comma-separated --alias\n\n* server : update --alias description and regenerate docs\n\n* server : multiple model aliases and tags\n\n- address review feedback from n..."
    },
    {
      "repo": "ollama/ollama",
      "company": "Ollama",
      "category": "local",
      "name": "v0.17.4",
      "tag": "v0.17.4",
      "url": "https://github.com/ollama/ollama/releases/tag/v0.17.4",
      "publishedAt": "2026-02-27T03:47:22Z",
      "isPrerelease": false,
      "body": "## New models\n- [Qwen 3.5](https://ollama.com/library/qwen3.5): a family of open-source multimodal models that delivers exceptional utility and performance.\n- [LFM 2](https://ollama.com/library/lfm2): LFM2 is a family of hybrid models designed for on-device deployment. LFM2-24B-A2B is the largest mo..."
    },
    {
      "repo": "ollama/ollama",
      "company": "Ollama",
      "category": "local",
      "name": "v0.17.3",
      "tag": "v0.17.3",
      "url": "https://github.com/ollama/ollama/releases/tag/v0.17.3",
      "publishedAt": "2026-02-27T02:21:12Z",
      "isPrerelease": false,
      "body": "## What's Changed\n* Fixed issue where tool calls in the Qwen 3 and Qwen 3.5 model families would not be parsed correctly if emitted during thinking\n\n**Full Changelog**: https://github.com/ollama/ollama/compare/v0.17.2...v0.17.3"
    },
    {
      "repo": "anthropics/claude-code",
      "company": "Anthropic",
      "category": "cli",
      "name": "v2.1.62",
      "tag": "v2.1.62",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.62",
      "publishedAt": "2026-02-27T01:56:19Z",
      "isPrerelease": false,
      "body": "## What's changed\n\n- Fixed prompt suggestion cache regression that reduced cache hit rates"
    },
    {
      "repo": "ollama/ollama",
      "company": "Ollama",
      "category": "local",
      "name": "v0.17.2",
      "tag": "v0.17.2",
      "url": "https://github.com/ollama/ollama/releases/tag/v0.17.2",
      "publishedAt": "2026-02-26T22:41:34Z",
      "isPrerelease": false,
      "body": "## What's Changed\n* Fixed issue where Ollama's app on Windows would crash when a new update has been downloaded\n\n\n**Full Changelog**: https://github.com/ollama/ollama/compare/v0.17.1...v0.17.2"
    },
    {
      "repo": "anthropics/claude-code",
      "company": "Anthropic",
      "category": "cli",
      "name": "v2.1.61",
      "tag": "v2.1.61",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.61",
      "publishedAt": "2026-02-26T22:34:32Z",
      "isPrerelease": false,
      "body": "## What's changed\n\n- Fixed concurrent writes corrupting config file on Windows"
    },
    {
      "repo": "BerriAI/litellm",
      "company": "LiteLLM",
      "category": "proxy",
      "name": "v1.81.16.custm-auth.dev",
      "tag": "v1.81.16.custm-auth.dev",
      "url": "https://github.com/BerriAI/litellm/releases/tag/v1.81.16.custm-auth.dev",
      "publishedAt": "2026-02-26T19:34:59Z",
      "isPrerelease": false,
      "body": "## What's Changed\n* security: fix critical/high CVEs in OS-level libs and NPM transitive by @Harshit28j in https://github.com/BerriAI/litellm/pull/22008\n* feat(proxy): limit concurrent health checks with health_check_concurrency by @MarshHawk in https://github.com/BerriAI/litellm/pull/20584\n* fix(be..."
    },
    {
      "repo": "BerriAI/litellm",
      "company": "LiteLLM",
      "category": "proxy",
      "name": "1.81.16-nightly",
      "tag": "1.81.16-nightly",
      "url": "https://github.com/BerriAI/litellm/releases/tag/1.81.16-nightly",
      "publishedAt": "2026-02-26T13:32:32Z",
      "isPrerelease": false,
      "body": "## What's Changed\n* security: fix critical/high CVEs in OS-level libs and NPM transitive by @Harshit28j in https://github.com/BerriAI/litellm/pull/22008\n* feat(proxy): limit concurrent health checks with health_check_concurrency by @MarshHawk in https://github.com/BerriAI/litellm/pull/20584\n* fix(be..."
    },
    {
      "repo": "anthropics/claude-code",
      "company": "Anthropic",
      "category": "cli",
      "name": "v2.1.59",
      "tag": "v2.1.59",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.59",
      "publishedAt": "2026-02-26T00:59:24Z",
      "isPrerelease": false,
      "body": "## What's changed\n\n- Claude automatically saves useful context to auto-memory. Manage with /memory\n- Added `/copy` command to show an interactive picker when code blocks are present, allowing selection of individual code blocks or the full response.\n- Improved \"always allow\" prefix suggestions for c..."
    },
    {
      "repo": "anthropics/claude-code",
      "company": "Anthropic",
      "category": "cli",
      "name": "v2.1.58",
      "tag": "v2.1.58",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.58",
      "publishedAt": "2026-02-25T20:00:11Z",
      "isPrerelease": false,
      "body": "## What's changed\n\n- Expand Remote Control to more users"
    },
    {
      "repo": "vllm-project/vllm",
      "company": "vLLM",
      "category": "inference",
      "name": "v0.16.0",
      "tag": "v0.16.0",
      "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0",
      "publishedAt": "2026-02-25T19:58:49Z",
      "isPrerelease": false,
      "body": "# vLLM v0.16.0\nPlease note that this release was branch cut on Feb 8, so any features added to vLLM after that date is not included.\n\n## Highlights\n\nThis release features 440 commits from 203 contributors (7 new)!\n\n* **Async scheduling + Pipeline Parallelism** is now fully supported, delivering **30..."
    },
    {
      "repo": "langchain-ai/langchain",
      "company": "LangChain",
      "category": "framework",
      "name": "langchain-core==1.2.16",
      "tag": "langchain-core==1.2.16",
      "url": "https://github.com/langchain-ai/langchain/releases/tag/langchain-core%3D%3D1.2.16",
      "publishedAt": "2026-02-25T16:27:43Z",
      "isPrerelease": false,
      "body": "Changes since langchain-core==1.2.15\n\nrelease(core): 1.2.16 (#35439)\nfix(core): treat empty tool chunk ids as missing in merge (#35414)"
    },
    {
      "repo": "anthropics/anthropic-sdk-python",
      "company": "Anthropic",
      "category": "sdk",
      "name": "v0.84.0",
      "tag": "v0.84.0",
      "url": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.84.0",
      "publishedAt": "2026-02-25T05:22:10Z",
      "isPrerelease": false,
      "body": "## 0.84.0 (2026-02-25)\n\nFull Changelog: [v0.83.0...v0.84.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.83.0...v0.84.0)\n\n### Features\n\n* **api:** change array_format to brackets ([925d2ad](https://github.com/anthropics/anthropic-sdk-python/commit/925d2ad6b76ad7c15de07b9b2768738775f..."
    },
    {
      "repo": "openai/openai-python",
      "company": "OpenAI",
      "category": "sdk",
      "name": "v2.24.0",
      "tag": "v2.24.0",
      "url": "https://github.com/openai/openai-python/releases/tag/v2.24.0",
      "publishedAt": "2026-02-24T20:01:29Z",
      "isPrerelease": false,
      "body": "## 2.24.0 (2026-02-24)\n\nFull Changelog: [v2.23.0...v2.24.0](https://github.com/openai/openai-python/compare/v2.23.0...v2.24.0)\n\n### Features\n\n* **api:** add phase ([391deb9](https://github.com/openai/openai-python/commit/391deb99f6a92e51bffb25efd8dfe367d144bb9d))\n\n\n### Bug Fixes\n\n* **api:** fix phas..."
    },
    {
      "repo": "ollama/ollama",
      "company": "Ollama",
      "category": "local",
      "name": "v0.17.1",
      "tag": "v0.17.1",
      "url": "https://github.com/ollama/ollama/releases/tag/v0.17.1",
      "publishedAt": "2026-02-24T15:00:28Z",
      "isPrerelease": false,
      "body": "## What's Changed\n* Nemotron architecture support in Ollama's engine\n* MLX engine now has improved memory usage\n* Ollama's app will now allow models that support tools to use web search capabilities\n* Improved LFM2 and LFM2.5 models in Ollama's engine\n* `ollama create` will no longer default to affi..."
    },
    {
      "repo": "langchain-ai/langchain",
      "company": "LangChain",
      "category": "framework",
      "name": "langchain-anthropic==1.3.4",
      "tag": "langchain-anthropic==1.3.4",
      "url": "https://github.com/langchain-ai/langchain/releases/tag/langchain-anthropic%3D%3D1.3.4",
      "publishedAt": "2026-02-24T13:54:19Z",
      "isPrerelease": false,
      "body": "Changes since langchain-anthropic==1.3.3\n\nrelease(anthropic): 1.3.4 (#35418)\nfix(anthropic): filter out common OpenAI Responses block types (#35417)\nfix(anthropic): update integration tests (#35396)\nrevert: add ChatAnthropicBedrockWrapper (#35371)\nfix(anthropic): replace retired model IDs in tests a..."
    },
    {
      "repo": "openai/openai-python",
      "company": "OpenAI",
      "category": "sdk",
      "name": "v2.23.0",
      "tag": "v2.23.0",
      "url": "https://github.com/openai/openai-python/releases/tag/v2.23.0",
      "publishedAt": "2026-02-24T03:19:39Z",
      "isPrerelease": false,
      "body": "## 2.23.0 (2026-02-24)\n\nFull Changelog: [v2.22.0...v2.23.0](https://github.com/openai/openai-python/compare/v2.22.0...v2.23.0)\n\n### Features\n\n* **api:** add gpt-realtime-1.5 and gpt-audio-1.5 model options to realtime calls ([3300b61](https://github.com/openai/openai-python/commit/3300b61e1d5a34c9d2..."
    },
    {
      "repo": "openai/openai-python",
      "company": "OpenAI",
      "category": "sdk",
      "name": "v2.22.0",
      "tag": "v2.22.0",
      "url": "https://github.com/openai/openai-python/releases/tag/v2.22.0",
      "publishedAt": "2026-02-23T20:13:52Z",
      "isPrerelease": false,
      "body": "## 2.22.0 (2026-02-23)\n\nFull Changelog: [v2.21.0...v2.22.0](https://github.com/openai/openai-python/compare/v2.21.0...v2.22.0)\n\n### Features\n\n* **api:** websockets for responses api ([c01f6fb](https://github.com/openai/openai-python/commit/c01f6fb0d55b7454f73c4904ea7a1954553085dc))\n\n\n### Chores\n\n* *..."
    },
    {
      "repo": "langchain-ai/langchain",
      "company": "LangChain",
      "category": "framework",
      "name": "langchain-core==1.2.15",
      "tag": "langchain-core==1.2.15",
      "url": "https://github.com/langchain-ai/langchain/releases/tag/langchain-core%3D%3D1.2.15",
      "publishedAt": "2026-02-23T15:05:04Z",
      "isPrerelease": false,
      "body": "Changes since langchain-core==1.2.14\n\nfix(core): improve error message for non-JSON-serializable tool schemas (#34376)\nfix(core): improve typing/docs for on_chat_model_start to clarify required positional args (#35324)\nperf(core): defer specific `langsmith` imports to reduce import time (#35298)\nrev..."
    },
    {
      "repo": "ollama/ollama",
      "company": "Ollama",
      "category": "local",
      "name": "v0.17.0",
      "tag": "v0.17.0",
      "url": "https://github.com/ollama/ollama/releases/tag/v0.17.0",
      "publishedAt": "2026-02-21T06:40:46Z",
      "isPrerelease": false,
      "body": "## OpenClaw\n\nOpenClaw can now be installed and configured automatically via Ollama, making it the easiest way to get up and running with OpenClaw with open models like Kimi-K2.5, GLM-5, and Minimax-M2.5.\n\n### Get started\n\n`ollama launch openclaw`\n\n<img width=\"2368\" height=\"1830\" alt=\"oc1\" src=\"https..."
    },
    {
      "repo": "JetBrains/intellij-community",
      "company": "JetBrains",
      "category": "ide",
      "name": "idea/2025.3.3",
      "tag": "idea/2025.3.3",
      "url": "https://github.com/JetBrains/intellij-community/releases/tag/idea/2025.3.3",
      "publishedAt": "2026-02-20T17:45:29Z",
      "isPrerelease": false,
      "body": "Release notes are available [here](https://youtrack.jetbrains.com/articles/IDEA-A-2100662633)"
    },
    {
      "repo": "JetBrains/intellij-community",
      "company": "JetBrains",
      "category": "ide",
      "name": "pycharm/2025.3.3",
      "tag": "pycharm/2025.3.3",
      "url": "https://github.com/JetBrains/intellij-community/releases/tag/pycharm/2025.3.3",
      "publishedAt": "2026-02-20T09:14:29Z",
      "isPrerelease": false,
      "body": "Release notes are available [here](https://youtrack.jetbrains.com/articles/PY-A-233538520)"
    },
    {
      "repo": "microsoft/vscode",
      "company": "Microsoft",
      "category": "ide",
      "name": "January 2026 Recovery 5",
      "tag": "1.109.5",
      "url": "https://github.com/microsoft/vscode/releases/tag/1.109.5",
      "publishedAt": "2026-02-20T00:20:02Z",
      "isPrerelease": false,
      "body": "The update addresses these [issues\n](https://github.com/microsoft/vscode/milestone/378?closed=1).\n\nFor the complete release notes go to [Updates](https://code.visualstudio.com/updates/v1_109) on [code.visualstudio.com](https://code.visualstudio.com/)."
    },
    {
      "repo": "anthropics/anthropic-sdk-python",
      "company": "Anthropic",
      "category": "sdk",
      "name": "v0.83.0",
      "tag": "v0.83.0",
      "url": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.83.0",
      "publishedAt": "2026-02-19T19:26:11Z",
      "isPrerelease": false,
      "body": "## 0.83.0 (2026-02-19)\n\nFull Changelog: [v0.82.0...v0.83.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.82.0...v0.83.0)\n\n### Features\n\n* **api:** Add top-level cache control (automatic caching) ([a940123](https://github.com/anthropics/anthropic-sdk-python/commit/a940123da34ac33f0b6..."
    },
    {
      "repo": "langchain-ai/langchain",
      "company": "LangChain",
      "category": "framework",
      "name": "langchain-core==1.2.14",
      "tag": "langchain-core==1.2.14",
      "url": "https://github.com/langchain-ai/langchain/releases/tag/langchain-core%3D%3D1.2.14",
      "publishedAt": "2026-02-19T14:22:50Z",
      "isPrerelease": false,
      "body": "Changes since langchain-core==1.2.13\n\nrelease(core): 1.2.14 (#35328)\nchore(core): remove `langserve` from sys info util, add `deepagents` (#35325)\nfix(core): fix merge_lists incorrectly merging parallel tool calls (#35281)\nfix(core): accept int temperature in _get_ls_params for LangSmith tracing (#3..."
    },
    {
      "repo": "langchain-ai/langchain",
      "company": "LangChain",
      "category": "framework",
      "name": "langchain-text-splitters==1.1.1",
      "tag": "langchain-text-splitters==1.1.1",
      "url": "https://github.com/langchain-ai/langchain/releases/tag/langchain-text-splitters%3D%3D1.1.1",
      "publishedAt": "2026-02-18T23:03:00Z",
      "isPrerelease": false,
      "body": "Changes since langchain-text-splitters==1.1.0\n\nrelease(text-splitters): 1.1.1 (#35318)\nfix(text-splitters): prevent JSFrameworkTextSplitter from mutating self._separators on each split_text() call (#35316)\nchore: bump transformers from 5.1.0 to 5.2.0 in /libs/text-splitters in the other-deps group a..."
    },
    {
      "repo": "anthropics/anthropic-sdk-python",
      "company": "Anthropic",
      "category": "sdk",
      "name": "v0.82.0",
      "tag": "v0.82.0",
      "url": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.82.0",
      "publishedAt": "2026-02-18T20:24:48Z",
      "isPrerelease": false,
      "body": "## 0.82.0 (2026-02-18)\n\nFull Changelog: [v0.81.0...v0.82.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.81.0...v0.82.0)\n\n### Features\n\n* **api:** fix shared UserLocation and error code types ([da3b931](https://github.com/anthropics/anthropic-sdk-python/commit/da3b931a2be768d77c228a..."
    },
    {
      "repo": "run-llama/llama_index",
      "company": "LlamaIndex",
      "category": "framework",
      "name": "v0.14.15",
      "tag": "v0.14.15",
      "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.15",
      "publishedAt": "2026-02-18T19:06:42Z",
      "isPrerelease": false,
      "body": "# Release Notes\n\n## [2026-02-18]\n\n### llama-index-agent-agentmesh [0.1.0]\n\n- [Integration] AgentMesh: Trust Layer for LlamaIndex Agents ([#20644](https://github.com/run-llama/llama_index/pull/20644))\n\n### llama-index-core [0.14.15]\n\n- Support basic operations for multimodal types ([#20640](https://g..."
    },
    {
      "repo": "anthropics/anthropic-sdk-python",
      "company": "Anthropic",
      "category": "sdk",
      "name": "v0.81.0",
      "tag": "v0.81.0",
      "url": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.81.0",
      "publishedAt": "2026-02-18T04:00:28Z",
      "isPrerelease": false,
      "body": "## 0.81.0 (2026-02-18)\n\nFull Changelog: [v0.80.0...v0.81.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.80.0...v0.81.0)\n\n### Features\n\n* **api:** Make new tool versions available as top level tool types ([0a385c2](https://github.com/anthropics/anthropic-sdk-python/commit/0a385c29d2..."
    },
    {
      "repo": "continuedev/continue",
      "company": "Continue",
      "category": "extension",
      "name": "v1.2.16-vscode",
      "tag": "v1.2.16-vscode",
      "url": "https://github.com/continuedev/continue/releases/tag/v1.2.16-vscode",
      "publishedAt": "2026-02-17T20:54:38Z",
      "isPrerelease": false,
      "body": "## What's Changed\n* .continue/agents by @sestinj in https://github.com/continuedev/continue/pull/10233\n* .continue/agents p2 by @sestinj in https://github.com/continuedev/continue/pull/10235\n* feat: limited MCP App support by @RomneyDa in https://github.com/continuedev/continue/pull/10132\n* feat(cli..."
    },
    {
      "repo": "continuedev/continue",
      "company": "Continue",
      "category": "extension",
      "name": "v1.0.61-jetbrains",
      "tag": "v1.0.61-jetbrains",
      "url": "https://github.com/continuedev/continue/releases/tag/v1.0.61-jetbrains",
      "publishedAt": "2026-02-17T20:35:59Z",
      "isPrerelease": true,
      "body": "## What's Changed\n* .continue/agents by @sestinj in https://github.com/continuedev/continue/pull/10233\n* .continue/agents p2 by @sestinj in https://github.com/continuedev/continue/pull/10235\n* feat: limited MCP App support by @RomneyDa in https://github.com/continuedev/continue/pull/10132\n* feat(cli..."
    },
    {
      "repo": "continuedev/continue",
      "company": "Continue",
      "category": "extension",
      "name": "v1.3.32-vscode",
      "tag": "v1.3.32-vscode",
      "url": "https://github.com/continuedev/continue/releases/tag/v1.3.32-vscode",
      "publishedAt": "2026-02-17T20:35:24Z",
      "isPrerelease": true,
      "body": "## What's Changed\n* .continue/agents by @sestinj in https://github.com/continuedev/continue/pull/10233\n* .continue/agents p2 by @sestinj in https://github.com/continuedev/continue/pull/10235\n* feat: limited MCP App support by @RomneyDa in https://github.com/continuedev/continue/pull/10132\n* feat(cli..."
    },
    {
      "repo": "anthropics/anthropic-sdk-python",
      "company": "Anthropic",
      "category": "sdk",
      "name": "v0.80.0",
      "tag": "v0.80.0",
      "url": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.80.0",
      "publishedAt": "2026-02-17T19:25:53Z",
      "isPrerelease": false,
      "body": "## 0.80.0 (2026-02-17)\n\nFull Changelog: [v0.79.0...v0.80.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.79.0...v0.80.0)\n\n### Features\n\n* **api:** Releasing claude-sonnet-4-6 ([d518d6e](https://github.com/anthropics/anthropic-sdk-python/commit/d518d6ecede3d0638f0b14950dc2be8efa0b4ff..."
    },
    {
      "repo": "openai/openai-python",
      "company": "OpenAI",
      "category": "sdk",
      "name": "v2.21.0",
      "tag": "v2.21.0",
      "url": "https://github.com/openai/openai-python/releases/tag/v2.21.0",
      "publishedAt": "2026-02-14T00:11:26Z",
      "isPrerelease": false,
      "body": "## 2.21.0 (2026-02-13)\n\nFull Changelog: [v2.20.0...v2.21.0](https://github.com/openai/openai-python/compare/v2.20.0...v2.21.0)\n\n### Features\n\n* **api:** container network_policy and skills ([d19de2e](https://github.com/openai/openai-python/commit/d19de2ee5c74413f9dc52684b650df1898dee82b))\n\n\n### Bug ..."
    },
    {
      "repo": "microsoft/vscode",
      "company": "Microsoft",
      "category": "ide",
      "name": "January 2026 Recovery 2",
      "tag": "1.109.2",
      "url": "https://github.com/microsoft/vscode/releases/tag/1.109.2",
      "publishedAt": "2026-02-11T03:24:57Z",
      "isPrerelease": false,
      "body": "The update addresses these [issues](https://github.com/Microsoft/vscode/issues?q=is%3Aissue+milestone%3A%22January+2026+Recovery+2%22+is%3Aclosed+).\n\nFor the complete release notes go to [Updates](https://code.visualstudio.com/updates/v1_109) on [code.visualstudio.com](https://code.visualstudio.com)..."
    },
    {
      "repo": "run-llama/llama_index",
      "company": "LlamaIndex",
      "category": "framework",
      "name": "v0.14.14",
      "tag": "v0.14.14",
      "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.14",
      "publishedAt": "2026-02-10T23:08:46Z",
      "isPrerelease": false,
      "body": "# Release Notes\n\n## [2026-02-10]\n\n### llama-index-callbacks-wandb [0.4.2]\n\n- Fix potential crashes and improve security defaults in core components ([#20610](https://github.com/run-llama/llama_index/pull/20610))\n\n### llama-index-core [0.14.14]\n\n- fix: catch pydantic ValidationError in VectorStoreQue..."
    },
    {
      "repo": "openai/openai-python",
      "company": "OpenAI",
      "category": "sdk",
      "name": "v2.20.0",
      "tag": "v2.20.0",
      "url": "https://github.com/openai/openai-python/releases/tag/v2.20.0",
      "publishedAt": "2026-02-10T19:02:11Z",
      "isPrerelease": false,
      "body": "## 2.20.0 (2026-02-10)\n\nFull Changelog: [v2.19.0...v2.20.0](https://github.com/openai/openai-python/compare/v2.19.0...v2.20.0)\n\n### Features\n\n* **api:** support for images in batch api ([28edb6e](https://github.com/openai/openai-python/commit/28edb6e1b7eb30dbb7be49979cee7882e8889264))"
    },
    {
      "repo": "microsoft/vscode",
      "company": "Microsoft",
      "category": "ide",
      "name": "January 2026 Recovery 1",
      "tag": "1.109.1",
      "url": "https://github.com/microsoft/vscode/releases/tag/1.109.1",
      "publishedAt": "2026-02-10T18:30:40Z",
      "isPrerelease": false,
      "body": "The update addresses these [issues](https://github.com/Microsoft/vscode/issues?q=is%3Aissue+milestone%3A%22January+2026+Recovery+1%22+is%3Aclosed+), including a fix for a security vulnerability.\n\nFor the complete release notes go to [Updates](https://code.visualstudio.com/updates/v1_109) on [code.vi..."
    },
    {
      "repo": "TabbyML/tabby",
      "company": "Tabby",
      "category": "self-hosted",
      "name": "next-alpha",
      "tag": "next-alpha",
      "url": "https://github.com/TabbyML/tabby/releases/tag/next-alpha",
      "publishedAt": "2026-02-09T10:49:36Z",
      "isPrerelease": true,
      "body": "This is an alpha version for Tabby dev,\nThis is only intended to be used internally."
    },
    {
      "repo": "continuedev/continue",
      "company": "Continue",
      "category": "extension",
      "name": "v1.2.15-vscode",
      "tag": "v1.2.15-vscode",
      "url": "https://github.com/continuedev/continue/releases/tag/v1.2.15-vscode",
      "publishedAt": "2026-02-04T23:12:36Z",
      "isPrerelease": false,
      "body": "## What's Changed\n* docs: update /info command description with usage statistics by @continue[bot] in https://github.com/continuedev/continue/pull/9071\n* chore(deps): bump undici from 7.16.0 to 7.18.2 in /binary by @dependabot[bot] in https://github.com/continuedev/continue/pull/9534\n* fix: add GH_T..."
    },
    {
      "repo": "continuedev/continue",
      "company": "Continue",
      "category": "extension",
      "name": "v1.3.31-vscode",
      "tag": "v1.3.31-vscode",
      "url": "https://github.com/continuedev/continue/releases/tag/v1.3.31-vscode",
      "publishedAt": "2026-02-04T23:06:04Z",
      "isPrerelease": true,
      "body": "## What's Changed\n* fix(cli): flaky hub loader tests by @uinstinct in https://github.com/continuedev/continue/pull/9923\n* [Snyk] Upgrade @tiptap/extension-text from 2.26.1 to 2.27.1 by @sestinj in https://github.com/continuedev/continue/pull/9915\n* chore(deps): bump tar from 7.4.3 to 7.5.7 in /core ..."
    },
    {
      "repo": "microsoft/vscode",
      "company": "Microsoft",
      "category": "ide",
      "name": "January 2026",
      "tag": "1.109.0",
      "url": "https://github.com/microsoft/vscode/releases/tag/1.109.0",
      "publishedAt": "2026-02-04T21:08:10Z",
      "isPrerelease": false,
      "body": "Welcome to the January 2026 release of Visual Studio Code. In this release, we are further evolving VS Code to make it the **home for multi-agent development**.\n\n* **Chat UX**: chat just feels better and snappier with faster streaming, improved reasoning results, and a revamped editor inline chat\n\n*..."
    }
  ],
  "repoStats": [
    {
      "repo": "anthropics/claude-code",
      "company": "Anthropic",
      "category": "cli",
      "latestRelease": "v2.1.63",
      "latestDate": "2026-02-28T03:45:37Z"
    },
    {
      "repo": "BerriAI/litellm",
      "company": "LiteLLM",
      "category": "proxy",
      "latestRelease": "v1.81.6.rc.6",
      "latestDate": "2026-02-28T02:56:53Z"
    },
    {
      "repo": "ggerganov/llama.cpp",
      "company": "llama.cpp",
      "category": "inference",
      "latestRelease": "b8179",
      "latestDate": "2026-02-27T20:28:00Z"
    },
    {
      "repo": "ollama/ollama",
      "company": "Ollama",
      "category": "local",
      "latestRelease": "v0.17.4",
      "latestDate": "2026-02-27T03:47:22Z"
    },
    {
      "repo": "vllm-project/vllm",
      "company": "vLLM",
      "category": "inference",
      "latestRelease": "v0.16.0",
      "latestDate": "2026-02-25T19:58:49Z"
    },
    {
      "repo": "langchain-ai/langchain",
      "company": "LangChain",
      "category": "framework",
      "latestRelease": "langchain-core==1.2.16",
      "latestDate": "2026-02-25T16:27:43Z"
    },
    {
      "repo": "anthropics/anthropic-sdk-python",
      "company": "Anthropic",
      "category": "sdk",
      "latestRelease": "v0.84.0",
      "latestDate": "2026-02-25T05:22:10Z"
    },
    {
      "repo": "openai/openai-python",
      "company": "OpenAI",
      "category": "sdk",
      "latestRelease": "v2.24.0",
      "latestDate": "2026-02-24T20:01:29Z"
    },
    {
      "repo": "JetBrains/intellij-community",
      "company": "JetBrains",
      "category": "ide",
      "latestRelease": "idea/2025.3.3",
      "latestDate": "2026-02-20T17:45:29Z"
    },
    {
      "repo": "microsoft/vscode",
      "company": "Microsoft",
      "category": "ide",
      "latestRelease": "1.109.5",
      "latestDate": "2026-02-20T00:20:02Z"
    },
    {
      "repo": "run-llama/llama_index",
      "company": "LlamaIndex",
      "category": "framework",
      "latestRelease": "v0.14.15",
      "latestDate": "2026-02-18T19:06:42Z"
    },
    {
      "repo": "continuedev/continue",
      "company": "Continue",
      "category": "extension",
      "latestRelease": "v1.3.32-vscode",
      "latestDate": "2026-02-17T20:35:24Z"
    },
    {
      "repo": "openai/tiktoken",
      "company": "OpenAI",
      "category": "tool",
      "latestRelease": "0.12.0",
      "latestDate": "2025-10-06T20:21:57Z"
    },
    {
      "repo": "Aider-AI/aider",
      "company": "Aider",
      "category": "cli",
      "latestRelease": "v0.86.0",
      "latestDate": "2025-08-09T17:42:19Z"
    },
    {
      "repo": "Exafunction/codeium",
      "company": "Codeium",
      "category": "extension",
      "latestRelease": "test-tag",
      "latestDate": "2024-06-17T16:04:28Z"
    },
    {
      "repo": "TabbyML/tabby",
      "company": "Tabby",
      "category": "self-hosted",
      "latestRelease": "nightly",
      "latestDate": "2023-09-08T01:39:25Z"
    }
  ]
}